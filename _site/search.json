[
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Overview of Naive Bayes Classification:\nNaive Bayes classification is a popular machine learning algorithm used for various tasks, such as text classification, spam detection, and sentiment analysis. At its core, Naive Bayes is based on probability theory and Bayes’ theorem. It’s a simple yet effective method for categorizing data into predefined classes or categories.\nProbabilistic Nature and Bayes’ Theorem Foundation:\nNaive Bayes is grounded in Bayes’ theorem, which is a fundamental concept in probability theory. It uses conditional probability to make predictions. In essence, Naive Bayes calculates the probability that a data point belongs to a specific class based on the observed features or attributes associated with that data point.\nThe “naive” part of Naive Bayes comes from the assumption that the features used in the classification are conditionally independent, which means that the presence or absence of one feature does not affect the presence or absence of another feature. While this assumption simplifies the model, it may not always hold true in real-world scenarios.\nObjectives of Naive Bayes Classification:\nThe primary objective of Naive Bayes classification is to assign a class label to a data point based on its features. This is often used for tasks like spam email detection, sentiment analysis, or categorizing documents.\nAchievements Through Naive Bayes Classification:\nThrough Naive Bayes classification, we aim to achieve the following:\n\nEfficient Text Classification: Naive Bayes is particularly effective for text classification tasks, such as classifying emails as spam or not spam.\nQuick and Simple Predictions: Naive Bayes is computationally efficient and works well with high-dimensional data. It can make predictions quickly, making it suitable for real-time applications.\nGood Performance with Small Datasets: Naive Bayes can perform well even when you have limited data, making it a valuable choice for scenarios with smaller datasets.\n\nVariants of Naive Bayes and When to Use Each:\nThere are several variants of Naive Bayes classification, and the choice of which one to use depends on the nature of your data:\n\nGaussian Naive Bayes: Use this when your features are continuous and have a Gaussian (normal) distribution. It’s suitable for data like measurements, where the values follow a bell-shaped curve.\nMultinomial Naive Bayes: This variant is commonly used for text classification tasks, where the features represent the frequency of words or tokens in documents. It’s suitable for discrete data like word counts.\nBernoulli Naive Bayes: Use Bernoulli Naive Bayes when your features are binary, representing presence or absence of certain attributes. It’s often used for tasks like spam detection, where the focus is on whether a word is present or not in a document.\n\nIn summary, Naive Bayes classification is a probabilistic approach based on Bayes’ theorem that aims to assign class labels to data points. It’s a versatile algorithm with different variants suited for various types of data, making it a valuable tool in machine learning and data analysis."
  },
  {
    "objectID": "naive_bayes.html#introduction",
    "href": "naive_bayes.html#introduction",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Overview of Naive Bayes Classification:\nNaive Bayes classification is a popular machine learning algorithm used for various tasks, such as text classification, spam detection, and sentiment analysis. At its core, Naive Bayes is based on probability theory and Bayes’ theorem. It’s a simple yet effective method for categorizing data into predefined classes or categories.\nProbabilistic Nature and Bayes’ Theorem Foundation:\nNaive Bayes is grounded in Bayes’ theorem, which is a fundamental concept in probability theory. It uses conditional probability to make predictions. In essence, Naive Bayes calculates the probability that a data point belongs to a specific class based on the observed features or attributes associated with that data point.\nThe “naive” part of Naive Bayes comes from the assumption that the features used in the classification are conditionally independent, which means that the presence or absence of one feature does not affect the presence or absence of another feature. While this assumption simplifies the model, it may not always hold true in real-world scenarios.\nObjectives of Naive Bayes Classification:\nThe primary objective of Naive Bayes classification is to assign a class label to a data point based on its features. This is often used for tasks like spam email detection, sentiment analysis, or categorizing documents.\nAchievements Through Naive Bayes Classification:\nThrough Naive Bayes classification, we aim to achieve the following:\n\nEfficient Text Classification: Naive Bayes is particularly effective for text classification tasks, such as classifying emails as spam or not spam.\nQuick and Simple Predictions: Naive Bayes is computationally efficient and works well with high-dimensional data. It can make predictions quickly, making it suitable for real-time applications.\nGood Performance with Small Datasets: Naive Bayes can perform well even when you have limited data, making it a valuable choice for scenarios with smaller datasets.\n\nVariants of Naive Bayes and When to Use Each:\nThere are several variants of Naive Bayes classification, and the choice of which one to use depends on the nature of your data:\n\nGaussian Naive Bayes: Use this when your features are continuous and have a Gaussian (normal) distribution. It’s suitable for data like measurements, where the values follow a bell-shaped curve.\nMultinomial Naive Bayes: This variant is commonly used for text classification tasks, where the features represent the frequency of words or tokens in documents. It’s suitable for discrete data like word counts.\nBernoulli Naive Bayes: Use Bernoulli Naive Bayes when your features are binary, representing presence or absence of certain attributes. It’s often used for tasks like spam detection, where the focus is on whether a word is present or not in a document.\n\nIn summary, Naive Bayes classification is a probabilistic approach based on Bayes’ theorem that aims to assign class labels to data points. It’s a versatile algorithm with different variants suited for various types of data, making it a valuable tool in machine learning and data analysis."
  },
  {
    "objectID": "naive_bayes.html#spotify-google-play-store-reviews",
    "href": "naive_bayes.html#spotify-google-play-store-reviews",
    "title": "Naïve Bayes",
    "section": "Spotify Google Play Store Reviews",
    "text": "Spotify Google Play Store Reviews\n\nFeature Selection for Text Data\nObjective: The primary objective of the Feature Selection component in this project is to identify and choose the most relevant and informative features (variables or attributes) from the dataset, for the given task. Effective feature selection can improve the model’s performance, reduce overfitting, and enhance the interpretability of the results.\nUsing the Spotify Reviews dataset, which contains the columns ‘time_submitted’, ‘review’, ‘rating’, ‘processed_review’ and ‘sentiment’, I have selected ‘processed_review’ (which is the ‘review’ column cleaned and pre-processed for analysis) as my input feature, and ‘sentiment’ (positve or negative) as my target feature. I plan to use a Naive Bayes classification model to classify the inputted review as either positive or negative.\n\n\nNaive Bayes Classification Model\nIn Python, after importing my dataset, I complete the following steps:\n\nTrain-test split\n\nI split the data into training and test sets, where 80% of the data is used for training, and the remaining 20% for testing.\n\nFeature extraction (Use TF-IDF vectorization to convert text data into numerical features)\n\nTF-IDF stands for “Term Frequency-Inverse Document Frequency.” It is a numerical statistic that reflects the importance of a word (term) within a document or a collection of documents (corpus). TF-IDF is commonly used in information retrieval and text mining to determine the importance of words in a document relative to their frequency in the entire corpus.\n\nTrain a Naive Bayes classifier\n\nI create and train a Multinomial Naive Bayes classifier, a commonly used algorithm for text classification tasks.\n\nEvaluate the model\n\nI use the trained model to make predictions on the test set, assigning sentiment labels to the reviews.\n\nCalculate accuracy and generate a classification report\n\nI assess the performance of the model using key metrics, including:\n\nAccuracy: A measure of the model’s overall correctness in predicting sentiments.\nPrecision: The ratio of true positive predictions to the total positive predictions.\nRecall: The ratio of true positive predictions to the total actual positives in the data.\nF1-score: A metric that combines precision and recall for a balanced evaluation. The F1 score reaches its best value at 1 (perfect precision and recall) and its worst at 0 (either precision or recall is 0).\n\nThe classification report provides detailed information on precision, recall, and F1-score for both positive and negative sentiments, along with support values indicating the number of instances for each class.\n\n\nBelow you can see the code used for the model as well as the summary report output and visualizations.\n\n\nCode\n\n# Import the necessary libraries and load the dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset containing Spotify reviews and their ratings\ndf = pd.read_csv('../../data/clean_data/reviews/spotify_reviews_sentiment.csv')\n\n# Train-test split\n# Split the data into training and testing sets\nX = df['processed_review']  # Text data\ny = df['sentiment']  # Binary sentiment label\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature extraction\n# Use TF-IDF vectorization to convert text data into numerical features\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.transform(X_test)\n\n# Train a Naive Bayes classifier\n# Create and train a Multinomial Naive Bayes classifier\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\n\n# Evaluate the model\n# Use the trained model to make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Calculate accuracy and generate a classification report\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\n# Print the results\nprint(f\"Accuracy: {accuracy}\")\nprint(report)\n\n\n\nAccuracy: 0.8477149119246692\n              precision    recall  f1-score   support\n\n    negative       0.82      0.91      0.86      6321\n    positive       0.89      0.79      0.83      5998\n\n    accuracy                           0.85     12319\n   macro avg       0.85      0.85      0.85     12319\nweighted avg       0.85      0.85      0.85     12319\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Class labels for binary classification\nclass_labels = ['Positive', 'Negative']\n\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))  # Adjust the figure size\n\n# Define labels for the confusion matrix cells\ngroup_names = ['True Pos', 'False Neg', 'False Pos', 'True Neg']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in conf_matrix.flatten()]\n\n# Calculate percentages relative to true instances (actual positives and actual negatives)\ntp, fn, fp, tn = conf_matrix.ravel()\ntotal_positives = tp + fn\ntotal_negatives = fp + tn\ntp_percentage = tp / total_positives\nfn_percentage = fn / total_positives\nfp_percentage = fp / total_negatives\ntn_percentage = tn / total_negatives\ngroup_percentages = [f\"{tp_percentage:.2%}\", f\"{fn_percentage:.2%}\", f\"{fp_percentage:.2%}\", f\"{tn_percentage:.2%}\"]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2, 2)\n\n# Create a heatmap and annotate the cells with labels\nsns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Calculate the sentiment distribution\nsentiment_distribution = df['sentiment'].value_counts()\n\n# Calculate the percentages\npercentages = (sentiment_distribution / sentiment_distribution.sum()) * 100\n\n# Create the bar plot\nax = sentiment_distribution.plot(kind='bar', color=['red', 'green'])\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.title('Distribution of Sentiments')\n\n# Annotate the bars with percentages\nfor i, count in enumerate(sentiment_distribution):\n    ax.text(i, count, f'{count} ({percentages[i]:.2f}%)', ha='center', va='bottom')\n\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\nCode\n# Learning curve function\nfrom sklearn.model_selection import learning_curve\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Accuracy\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n# Create a learning curve\nplot_learning_curve(clf, 'Naive Bayes Learning Curve', X_train, y_train, cv=5)\nplt.show()\n\n\n\n\n\n\n\nModel Results Interpretation\n\nClassification Report Output Analysis\nLet’s analyze the classification report output from above:\n\nAccuracy: The accuracy of the model is approximately 84.77%, which indicates that the model correctly predicted the sentiment for about 84.77% of the reviews in the test dataset. This is a measure of overall correctness.\nPrecision:\n\nFor the ‘negative’ class, the precision is 82%. This means that when the model predicts a review as ‘negative,’ it is correct 82% of the time.\nFor the ‘positive’ class, the precision is 89%. This indicates that when the model predicts a review as ‘positive,’ it is correct 89% of the time.\n\nRecall:\n\nFor the ‘negative’ class, the recall is 91%. This means that the model correctly identifies 91% of the actual ‘negative’ reviews.\nFor the ‘positive’ class, the recall is 79%. The model captures 79% of the actual ‘positive’ reviews.\n\nF1-Score:\n\nThe F1-score is a balance between precision and recall. For the ‘negative’ class, it is 0.86, indicating a good balance between precision and recall.\nFor the ‘positive’ class, the F1-score is 0.83, also indicating a good balance.\n\nSupport:\n\nThe ‘support’ column shows the number of instances for each class in the test dataset. There are 6,321 instances of ‘negative’ reviews and 5,998 instances of ‘positive’ reviews.\n\nMacro Avg:\n\nThe macro average F1-score is 0.85, which is the average of the F1-scores for both classes. It provides an overall evaluation of the model’s performance, giving equal weight to both classes.\n\nWeighted Avg:\n\nThe weighted average F1-score is also 0.85. It is similar to the macro average but takes into account the class imbalance in the dataset. It is weighted based on the number of instances in each class.\n\n\nAnalysis:\n\nThe accuracy of 84.77% suggests that the model is performing reasonably well in classifying sentiment.\nPrecision and recall values indicate that the model is better at predicting ‘positive’ sentiment than ‘negative’ sentiment.\nThe F1-scores are relatively balanced for both classes, which is a good sign.\nThe macro and weighted average F1-scores are also around 0.85, indicating consistent performance across classes and accounting for class imbalances.\n\nIn summary, the model appears to perform well in predicting sentiments, but there might be room for improvement in correctly identifying ‘negative’ sentiments. The F1-scores suggest a reasonable balance between precision and recall. The performance metrics are well-balanced and provide a comprehensive view of the model’s effectiveness in classifying sentiment in the dataset.\n\n\nConfusion Matrix Analysis\nLet’s analyze the confusion matrix from above:\nThe confusion matrix for the binary classification task is as follows:\n\nTrue Positives (TP): 5725\nFalse Negatives (FN): 596\nFalse Positives (FP): 1280\nTrue Negatives (TN): 4718\n\nAnd the percentages, relative to the actual instances, are:\n\nTrue Positive Rate (Recall for ‘Positive’ class): 90.61%\n\nThis indicates that out of all the actual positive cases, the model correctly predicted approximately 90.61% of them as positive.\n\nFalse Negative Rate: 9.39%\n\nThis represents the cases where the model incorrectly predicted negative when the actual class was positive.\n\nFalse Positive Rate: 21.34%\n\nThis shows the cases where the model incorrectly predicted positive when the actual class was negative.\n\nTrue Negative Rate (Specificity for ‘Negative’ class): 78.66%\n\nThis indicates that out of all the actual negative cases, the model correctly predicted approximately 78.66% of them as negative.\n\n\nAnalysis:\n\nThe model’s ability to correctly predict positive cases (True Positives) is quite high at 90.61%, indicating that it’s effective at identifying positive reviews.\nThere is a relatively low rate of False Negatives (9.39%), meaning that the model misses only a small portion of positive reviews.\nThe model’s ability to correctly predict negative cases (True Negatives) is also good at 78.66%, showing it’s effective at identifying negative reviews.\nThe False Positive Rate is 21.34%, indicating that there is a notable number of false alarms where the model predicts a review as positive when it’s actually negative.\n\nOverall, the model demonstrates a reasonably good performance, especially in terms of correctly identifying positive and negative cases. However, there is room for improvement in reducing false positive predictions. It’s important to consider the specific goals of the classification task when interpreting these results.\n\n\nSentiment Distribution Plot Analysis\nThe provided chart above displays the distribution of sentiments in the dataset, and it’s evident that the data is relatively well balanced between negative and positive sentiments.\nHere’s an analysis of the chart:\n\nTotal Data Points: The chart represents a total of 61,594 data points, which is the sum of negative (31,657) and positive (29,937) sentiments.\nBalanced Distribution: The dataset is quite balanced between negative and positive sentiments, with approximately 51.40% of the reviews classified as negative and 48.6% as positive. This balance is important for training machine learning models as it prevents biases that could arise from an imbalanced dataset.\nVisual Representation: The chart provides a clear visual representation of the sentiment distribution, making it easy to understand the relative proportion of positive and negative reviews.\n\nIn summary, this analysis highlights that the dataset is well balanced between positive and negative sentiments, which is favorable for training and evaluating machine learning models for sentiment analysis.\n\n\nLearning Curve Analysis\nLet’s analyze the learning curve from above:\n\nCross-Validation Accuracy Line: The accuracy of the model on the cross-validation data is represented by the green line. It starts at approximately 0.83 when the number of training examples is minimal and gradually increases as the training set size grows. This indicates that the model benefits from more data, as it’s able to generalize better with larger training sets. At around 40,000 training examples, the cross-validation accuracy reaches approximately 0.85.\nTraining Accuracy Line: The red line represents the accuracy on the training data. It starts high, around 0.88, but as the number of training examples increases, it slightly decreases to around 0.87. This decrease in training accuracy as more examples are added indicates that the model might be suffering from overfitting. Overfitting occurs when a model becomes too specialized on the training data and doesn’t generalize well to unseen data.\n\nHere’s the key takeaway:\n\nIncreasing the size of the training dataset benefits the model’s performance on unseen data (cross-validation accuracy). This suggests that the model could benefit from even more training data if available.\nThe small decline in training accuracy as more data is added may indicate some degree of overfitting. Regularization techniques or model complexity reduction may help address this issue.\n\nIn summary, the learning curve provides valuable insights into how the model’s performance changes with more training examples, highlighting the trade-off between model bias and variance. The model seems to be improving with more data but might require additional adjustments to mitigate overfitting.\n\n\n\nConclusion\nIn summary, the model demonstrates a reasonable level of accuracy and balanced performance across different metrics. While it excels at identifying ‘positive’ sentiment, there is room for improvement in correctly identifying ‘negative’ sentiment. The sentiment distribution plot confirms a balanced dataset, and the learning curve highlights the potential benefits of additional training data while addressing overfitting concerns. Overall, the model shows promise in sentiment classification, but ongoing refinement may further enhance its performance and generalization capabilities."
  },
  {
    "objectID": "naive_bayes.html#edm-subgenres",
    "href": "naive_bayes.html#edm-subgenres",
    "title": "Naïve Bayes",
    "section": "EDM Subgenres",
    "text": "EDM Subgenres\n\nFeature Selection for Record Data\nObjective: The primary objective of the Feature Selection component in this project is to identify and choose the most relevant and informative features (variables or attributes) from the dataset, for the given task. Effective feature selection can improve the model’s performance, reduce overfitting, and enhance the interpretability of the results.\nUsing the EDM subgenres dataset, the columns ‘danceability’, ‘energy’, ‘loudness’, ‘speechiness’, ‘instrumentalness’, ‘liveness’, ‘valence’, and ‘tempo’ are the input features and ‘genre’ is my target feature. I plan to use a Gaussian Naive Bayes classification model to classify the inputted features as one of the 5 edm subgenres.\n\n\nNaive Bayes Classification Model\nIn Python, after importing my dataset, I completed the following steps:\n\nTrain-Test Split:\n\nI divided the dataset into two parts: a training set, which constitutes 80% of the data, and a test set, comprising the remaining 20%. This division allows us to train the model on one part and evaluate its performance on the other.\n\nFeature Selection:\n\nFor this EDM subgenre prediction model, I selected specific audio features as input, including ‘danceability,’ ‘energy,’ ‘loudness_linear,’ ‘speechiness,’ ‘acousticness,’ ‘instrumentalness,’ ‘liveness,’ ‘valence,’ and ‘tempo.’ These features will be used to make predictions about the subgenres of EDM music.\n\nTraining a Gaussian Naive Bayes Classifier:\n\nI employed a Gaussian Naive Bayes classifier, which is well-suited for handling continuous, numeric features. This model assumes that the features are normally distributed (Gaussian) and learns to predict EDM subgenres based on the provided input features.\n\nMaking Predictions:\n\nUsing the trained Gaussian Naive Bayes model, I made predictions on the test data. These predictions assign specific EDM subgenre labels to the test data based on the model’s learned patterns.\n\nModel Evaluation:\n\nTo assess the model’s performance, I compared the predicted subgenre labels with the true subgenre labels present in the test data. This evaluation step allows us to determine how well the model can classify EDM subgenres.\n\nCalculating Accuracy and Generating a Classification Report:\n\nI calculated the model’s accuracy, which provides an overall measure of its correctness in predicting EDM subgenres. Additionally, I generated a classification report, which includes detailed metrics such as precision, recall, and F1-score for each subgenre class. The report provides valuable insights into the model’s performance, including its ability to distinguish between different EDM subgenres.\n\n\nBelow you can see the code used for the model as well as the summary report output and visualizations.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\n\n# Choose your input features\nfeatures = ['danceability', 'energy', 'loudness_linear', 'speechiness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n\n# Split data into training and testing sets\nX = df[features]\ny = df['genre_encoded']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Gaussian Naive Bayes classifier\ngnb_classifier = GaussianNB()\ngnb_classifier.fit(X_train, y_train)\n\n\n# Make predictions on the test data\ny_pred = gnb_classifier.predict(X_test)\n# Evaluate the classifier\naccuracy = gnb_classifier.score(X_test, y_test)\n# Generate the classification report\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\n\nprint(report)\nprint(\"Accuracy:\", accuracy)\n\n\n               precision    recall  f1-score   support\n\ndrum and bass       0.70      0.56      0.62       779\n      dubstep       0.62      0.59      0.61       733\n   tech house       0.60      0.63      0.62       767\n       techno       0.61      0.66      0.63       763\n       trance       0.53      0.59      0.56       744\n\n     accuracy                           0.61      3786\n    macro avg       0.61      0.61      0.61      3786\n weighted avg       0.61      0.61      0.61      3786\n\nAccuracy: 0.6075013206550449\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make predictions on the test data\ny_pred = gnb_classifier.predict(X_test)\n\n# Generate the confusion matrix\nconfusion = confusion_matrix(y_test, y_pred)\n\n# Calculate percentages for each cell\nconfusion_percentages = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis] * 100\n\n# Define class labels for the confusion matrix\nclass_labels = label_encoder.classes_\n\n# Create a heatmap of the confusion matrix with percentages\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_percentages, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix (with Percentages)')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# Define the training sizes you want to include in the learning curve\ntrain_sizes, train_scores, test_scores = learning_curve(GaussianNB(), X, y, train_sizes=np.linspace(0.1, 1.0, 5), cv=5, scoring='accuracy')\n\n# Calculate the mean and standard deviation of training and test scores\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\n# Create the learning curve plot\nplt.figure(figsize=(8, 6))\nplt.plot(train_sizes, train_scores_mean, label='Training Accuracy')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1)\nplt.plot(train_sizes, test_scores_mean, label='Validation Accuracy')\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1)\nplt.xlabel('Training Examples')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')\nplt.title('Learning Curve for Gaussian Naive Bayes')\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom sklearn.inspection import permutation_importance\n\n# Fit the Gaussian Naive Bayes classifier\ngnb_classifier.fit(X_train, y_train)\n\n# Calculate permutation importances\nperm_importance = permutation_importance(gnb_classifier, X_test, y_test, n_repeats=30, random_state=42)\n\n# Get feature importances\nimportances = perm_importance.importances_mean\n\n# Create a bar chart to visualize feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(features, importances, color='skyblue')\nplt.title('Permutation Feature Importance')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.show()\n\n\n\n\n\n\n\nModel Results Interpretation\n\nClassification Report Output Analysis\nLet’s analyze the classification report output from above:\n\nClasses: The report is based on a classification task with five classes: “drum and bass,” “dubstep,” “tech house,” “techno,” and “trance.”\nPrecision: Precision measures how many of the predicted positive instances were actually correct. Here’s the precision analysis for each class:\n\nFor “drum and bass,” precision is 0.70, meaning that 70% of the instances predicted as “drum and bass” were correct.\nFor “dubstep,” precision is 0.62, indicating that 62% of the instances predicted as “dubstep” were correct.\nFor “tech house,” precision is 0.60, indicating that 60% of the instances predicted as “tech house” were correct.\nFor “techno,” precision is 0.61, meaning that 61% of the instances predicted as “techno” were correct.\nFor “trance,” precision is 0.53, indicating that 53% of the instances predicted as “trance” were correct.\n\nRecall: Recall measures how many of the actual positive instances were correctly predicted. Here’s the recall analysis for each class:\n\nFor “drum and bass,” recall is 0.56, meaning that 56% of the actual “drum and bass” instances were correctly predicted.\nFor “dubstep,” recall is 0.59, indicating that 59% of the actual “dubstep” instances were correctly predicted.\nFor “tech house,” recall is 0.63, indicating that 63% of the actual “tech house” instances were correctly predicted.\nFor “techno,” recall is 0.66, meaning that 66% of the actual “techno” instances were correctly predicted.\nFor “trance,” recall is 0.59, indicating that 59% of the actual “trance” instances were correctly predicted.\n\nF1-Score: The F1-score is the harmonic mean of precision and recall. It provides a balance between the two metrics. Here’s the F1-score analysis for each class:\n\nFor “drum and bass,” the F1-score is 0.62, indicating a balanced performance between precision and recall.\nFor “dubstep,” the F1-score is 0.61, also showing a balance between precision and recall.\nFor “tech house,” the F1-score is 0.62, indicating a balanced performance.\nFor “techno,” the F1-score is 0.63, showing a good balance between precision and recall.\nFor “trance,” the F1-score is 0.56, indicating a slightly lower balance.\n\nSupport: Support represents the number of instances in each class.\nAccuracy: The overall accuracy of the model is 0.6075, indicating that the model correctly classifies 60.75% of all instances.\nMacro Average: The macro average provides the average precision, recall, and F1-score across all classes. In this case, the macro average is approximately 0.61.\nWeighted Average: The weighted average considers class imbalance and provides an average that gives more weight to classes with more instances. In this case, the weighted average is approximately 0.61.\n\nIn summary, the classification report shows that the model has varying performance across different classes. While it achieves a reasonably balanced F1-score for most classes, there might be room for improvement in terms of precision and recall, particularly for the “trance” class. Additionally, the overall accuracy is around 60.75%, indicating that the model’s performance is decent, but there may be potential for improvement.\n\n\nConfusion Matrix Analysis\nLet’s analyze the confusion matrix from above:\nThe confusion matrix with percentages provides insight into the model’s performance for each class. Each row represents the actual instances for a specific class, and each column represents the predicted class. The values in each row are the percentages of actual instances that were predicted to belong to each class.\n\nDrum and Bass (Actuals):\n\n56.23% of actual “drum and bass” instances were correctly predicted as “drum and bass.”\n23.62% of actual “drum and bass” instances were incorrectly predicted as “dubstep.”\n8.22% were incorrectly predicted as “tech house.”\n3.21% were incorrectly predicted as “techno.”\n8.73% were incorrectly predicted as “trance.”\n\nDubstep (Actuals):\n\n59.35% of actual “dubstep” instances were correctly predicted as “dubstep.”\n17.74% were incorrectly predicted as “drum and bass.”\n6.55% were incorrectly predicted as “tech house.”\n3.14% were incorrectly predicted as “techno.”\n13.23% were incorrectly predicted as “trance.”\n\nTech House (Actuals):\n\n63.36% of actual “tech house” instances were correctly predicted as “tech house.”\n2.22%were incorrectly predicted as “drum and bass.”\n2.35% were incorrectly predicted as “dubstep.”\n18.25% were incorrectly predicted as “techno.”\n13.82% were incorrectly predicted as “trance.”\n\nTechno (Actuals):\n\n66.19% of actual “techno” instances were correctly predicted as “techno.”\n2.49% were incorrectly predicted as “drum and bass.”\n\n1.18% were incorrectly predicted as “dubstep.”\n15.73% were incorrectly predicted as “tech house.”\n14.42% were incorrectly predicted as “trance.”\n\nTrance (Actuals):\n\n58.6% of actual “trance” instances were correctly predicted as “trance.”\n2.96% were incorrectly predicted as “drum and bass.”\n\n7.80% were incorrectly predicted as “dubstep.”\n12.1% were incorrectly predicted as “tech house.”\n18.55% were incorrectly predicted as “techno.”\n\n\nAnalysis:\n\nThe diagonal values represent the true positives, i.e., the percentages of correctly predicted instances for each class.\nThe off-diagonal values represent various types of misclassifications.\nThe model appears to have varying performance across different genres. While some genres like “dubstep” and “techno” are well-predicted, others like “drum and bass” and “trance” have room for improvement.\nThe model has the most significant challenges in distinguishing between “tech house” and “techno,” where misclassifications are relatively high. This makes sense because tech house combines stylistic features of techno with house.\nThe analysis suggests that fine-tuning the model, considering feature importance, or collecting more genre-specific data could lead to improved genre classification accuracy.\n\n\n\nLearning Curve Analysis\nLet’s analyze the above learning curve:\n\nTraining Accuracy (Blue Line):\n\nInitially, with a relatively small training dataset size of around 2,000 samples, the model achieves a perfect training accuracy of 1. This means the model can perfectly fit the smaller dataset, as it essentially memorizes the data.\nAs the training dataset size increases beyond 2,000 samples, the training accuracy starts to decrease gradually. This decrease in training accuracy suggests that the model might be finding it more challenging to fit the larger dataset.\nThe training accuracy eventually stabilizes at around 0.6 (60%) when the training dataset reaches approximately 14,000 samples.\nThe decrease in training accuracy indicates that the model is not overfitting the training data, and it starts to generalize better as the dataset size increases.\n\nValidation Accuracy (Orange Line):\n\nInitially, with a small training dataset size of around 2,000 samples, the validation accuracy is notably lower, at around 0.2 (20%). This lower accuracy indicates that the model’s performance on unseen data is not as strong when trained on a small dataset.\nAs the training dataset size increases from 2,000 to around 14,000 samples, the validation accuracy steadily improves. It rises to approximately 0.6 (60%) at the larger dataset size.\nThe increasing validation accuracy suggests that the model’s ability to generalize to new, unseen data improves as more training data becomes available.\n\n\nAnalysis:\n\nThe initial low training accuracy and validation accuracy with a small dataset size (around 2,000 samples) suggest that the model may struggle to generalize effectively from limited training data.\nThe convergence of the training and validation accuracy lines at around 14,000 training samples is a positive sign. It indicates that the model is neither underfitting nor overfitting but achieving a good balance between bias and variance.\nThe model appears to benefit from the increased dataset size, allowing it to generalize more effectively and improve its accuracy on unseen data.\nThe learning curve suggests that collecting more training data could lead to further improvements in the model’s performance.\nOverall, the learning curve illustrates the importance of dataset size in achieving better generalization and model performance.\n\n\n\nPermutation Feature Importance Analysis\n\nHow does it work?\n“Permutation Feature Importance” is used to assess the importance of each feature in a machine learning model, in this case, a Gaussian Naive Bayes classifier.\n\nPermutation Importance Calculation: The permutation_importance function is used to calculate the importance of each feature. It does this by taking the following steps for each feature:\n\nIt temporarily shuffles (permutes) the values of a single feature in the test data (X_test). This means it disrupts the relationship between the feature and the target variable (genre in this case).\nThen, it measures the impact of this permutation on the model’s performance. If the feature was important, shuffling its values should significantly decrease the model’s accuracy or other performance metric.\nThis process is repeated multiple times (in this case, 30 times) for each feature, and the average impact on model performance is calculated.\n\nFeature Importance Values: After running these permutations, you get a measure of how much each feature contributes to the model’s predictive performance. Features that, when permuted, have a large negative impact on the model’s performance are considered more important.\nBar Chart Visualization: Finally, the script creates a horizontal bar chart to visualize the feature importances. Each feature is shown on the y-axis, and its importance (the impact of permuting it) is shown on the x-axis. Features with higher importance will have higher values on the x-axis.\n\n\n\nAnalysis of Chart\nLet’s analyze the bar chart from above:\n\nTempo: Tempo stands out as the most important feature. When the tempo is permuted (randomly shuffled), it has a significant negative impact on the model’s performance. This suggests that the tempo plays a crucial role in the model’s ability to classify music genres accurately.\nInstrumentalness: Following tempo, instrumentalness is the second most important feature. This means that the proportion of instrumental music in a track significantly influences the model’s predictions. High instrumentalness might be a strong indicator for specific music genres.\nDanceability: Danceability is the third most important feature. It indicates that the level of danceability, or how suitable a track is for dancing, plays a notable role in classifying genres correctly.\nLoudness: Loudness is the fourth most important feature. It suggests that the volume or loudness of a track has a significant impact on the model’s predictions. It might be that certain genres tend to be louder than others.\n\nThis analysis aligns with the idea that musical attributes related to tempo, instrumentalness, danceability, and loudness are strong indicators for genre classification. It provides valuable insights into the characteristics of music that contribute the most to the model’s accuracy.\nThese findings could be useful for several purposes, such as understanding the model’s decision-making process, potentially identifying which musical attributes are distinctive for each genre, and guiding future data collection or feature engineering efforts.\n\n\n\n\nConclusion\nIn summary, the genre classification model shows potential, but it benefits from refinement, particularly for genres with higher misclassification rates. Additionally, the learning curve highlights the importance of dataset size for improving model generalization. Further data collection and model fine-tuning may contribute to more accurate genre predictions."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "data_exploration.html",
    "href": "data_exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In this section, I delve into exploring the datasets I previously gathered and cleaned. Exploratory Data Analysis (EDA) is the crucial first step in the data analysis process, where we unravel the hidden stories and patterns within our datasets. It involves a comprehensive examination of data, employing various statistical and visualization techniques to understand its structure, relationships, and anomalies. EDA not only uncovers valuable insights but also serves as the foundation for informed decision-making in data-driven projects. I will work through each of the 6 datasets I gathered and cleaned from the previous Data Gathering and Data Cleaning tabs, applying different EDA methods using Matplotlib, Seaborn and Pandas.\n\n\nI will walk through each of the below topics for my datasets, being sure to complete thorough and effective EDA.\nData Understanding: To effectively tackle our project’s objectives, we must first immerse ourselves in the dataset. We’ll explore its features, understand their data types, and investigate potential relationships that are crucial for our analysis.\nDescriptive Statistics: Let’s start by quantifying the data’s central tendencies and spread for numerical variables, giving us insights into the dataset’s overall distribution. For categorical variables, we’ll create frequency distributions and visualizations to grasp their distribution patterns.\nData Visualization: We’ll harness the power of data visualization to make sense of complex data. Through histograms, scatter plots, and more, we’ll gain a visual perspective on data distribution, variable relationships, and potential trends.\nCorrelation Analysis: By scrutinizing correlations between variables, we’ll uncover valuable insights. Heatmaps and scatter plots will help us identify positive, negative, or negligible correlations, steering us towards deeper analyses.\nHypothesis Generation: Building on our initial observations, we’ll refine our hypotheses and research questions, ensuring they align with the emerging insights from the data.\nData Grouping and Segmentation: When applicable, we’ll divide the data into meaningful segments or groups. This approach allows us to explore specific subgroups, revealing insights that might not be apparent in the overall dataset.\nIdentifying Outliers: We’ll pay close attention to outliers, which can indicate data quality issues or hold intriguing stories within the dataset. Uncovering these anomalies is crucial to a comprehensive understanding of the data.\n\n\n\nMany of the datasets used contain audio features of songs/artists. Below I have defined what each column means and represents, so you can get a clearer understanding of further data analysis that is to come.\nDuration The length of the song in milliseconds.\nPopularity A 0-to-100 score that ranks how popular an artist/track is relative to other artists/tracks on Spotify.\nAcousticness A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\nDanceability Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\nEnergy Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\nInstrumentalness Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\nKey The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\nLiveness Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\nLoudness The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\nMode Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\nSpeechiness Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\nTempo The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\nValence A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)."
  },
  {
    "objectID": "data_exploration.html#introduction",
    "href": "data_exploration.html#introduction",
    "title": "Data Exploration",
    "section": "",
    "text": "In this section, I delve into exploring the datasets I previously gathered and cleaned. Exploratory Data Analysis (EDA) is the crucial first step in the data analysis process, where we unravel the hidden stories and patterns within our datasets. It involves a comprehensive examination of data, employing various statistical and visualization techniques to understand its structure, relationships, and anomalies. EDA not only uncovers valuable insights but also serves as the foundation for informed decision-making in data-driven projects. I will work through each of the 6 datasets I gathered and cleaned from the previous Data Gathering and Data Cleaning tabs, applying different EDA methods using Matplotlib, Seaborn and Pandas.\n\n\nI will walk through each of the below topics for my datasets, being sure to complete thorough and effective EDA.\nData Understanding: To effectively tackle our project’s objectives, we must first immerse ourselves in the dataset. We’ll explore its features, understand their data types, and investigate potential relationships that are crucial for our analysis.\nDescriptive Statistics: Let’s start by quantifying the data’s central tendencies and spread for numerical variables, giving us insights into the dataset’s overall distribution. For categorical variables, we’ll create frequency distributions and visualizations to grasp their distribution patterns.\nData Visualization: We’ll harness the power of data visualization to make sense of complex data. Through histograms, scatter plots, and more, we’ll gain a visual perspective on data distribution, variable relationships, and potential trends.\nCorrelation Analysis: By scrutinizing correlations between variables, we’ll uncover valuable insights. Heatmaps and scatter plots will help us identify positive, negative, or negligible correlations, steering us towards deeper analyses.\nHypothesis Generation: Building on our initial observations, we’ll refine our hypotheses and research questions, ensuring they align with the emerging insights from the data.\nData Grouping and Segmentation: When applicable, we’ll divide the data into meaningful segments or groups. This approach allows us to explore specific subgroups, revealing insights that might not be apparent in the overall dataset.\nIdentifying Outliers: We’ll pay close attention to outliers, which can indicate data quality issues or hold intriguing stories within the dataset. Uncovering these anomalies is crucial to a comprehensive understanding of the data.\n\n\n\nMany of the datasets used contain audio features of songs/artists. Below I have defined what each column means and represents, so you can get a clearer understanding of further data analysis that is to come.\nDuration The length of the song in milliseconds.\nPopularity A 0-to-100 score that ranks how popular an artist/track is relative to other artists/tracks on Spotify.\nAcousticness A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\nDanceability Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\nEnergy Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\nInstrumentalness Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\nKey The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\nLiveness Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\nLoudness The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\nMode Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\nSpeechiness Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\nTempo The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\nValence A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)."
  },
  {
    "objectID": "data_exploration.html#news-api",
    "href": "data_exploration.html#news-api",
    "title": "Data Exploration",
    "section": "News API",
    "text": "News API\nI’ve collected a rich dataset from the News API, focusing on articles related to Spotify. This dataset is a treasure trove of information about the latest developments, trends, and news surrounding the music streaming platform. To distill the essence of these articles, I decided to create a captivating word cloud. By analyzing the titles and descriptions of each article, this word cloud visually represents the most frequently occurring words, offering a quick and insightful glimpse into the key themes and topics that dominate the Spotify news landscape. It’s a fascinating way to showcase the highlights and trending subjects within this dynamic and ever-evolving industry.\nThe generated Spotify-themed word cloud below provides a visually captivating snapshot of the key themes and topics present in the collected articles. As expected, the word ‘Spotify’ takes center stage, reaffirming its central role in the context of these articles. The prominence of terms like ‘music,’ ‘new,’ and ‘AI’ suggests that the articles likely revolve around discussions regarding innovative developments, trends, and technologies in the music industry. The word ‘week’ may indicate a focus on weekly updates or happenings within Spotify, while ‘Apple’ hints at possible comparisons or interactions between the two major players in the music streaming industry.\nOverall, this word cloud offers a quick and engaging overview of the most significant terms within the dataset, providing a valuable starting point for understanding the core topics and trends in Spotify-related news."
  },
  {
    "objectID": "data_exploration.html#electronic-dance-music-subgenres",
    "href": "data_exploration.html#electronic-dance-music-subgenres",
    "title": "Data Exploration",
    "section": "Electronic Dance Music Subgenres",
    "text": "Electronic Dance Music Subgenres\n\nData Understanding\n\n\nartist_name          object\ndanceability        float64\nenergy              float64\nkey                 float64\nloudness            float64\nmode                float64\nspeechiness         float64\nacousticness        float64\ninstrumentalness    float64\nliveness            float64\nvalence             float64\ntempo               float64\nduration_mins       float64\ntime_signature      float64\ngenre                object\ndtype: object\n\n\n\n\n(20607, 15)\n\n\n\n\nDescriptive Statistics\nBelow are the descriptive statistics of the dataset. As this dataset is a combination of 5 edm subgenres, it would be interesting to compare these statistics across the genres.\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ndanceability\n20607.0\n0.645416\n0.142001\n0.000000\n0.552000\n0.658000\n0.761000\n0.989000\n\n\nenergy\n20607.0\n0.822341\n0.151667\n0.001370\n0.744000\n0.866000\n0.938000\n1.000000\n\n\nkey\n20607.0\n5.569224\n3.637224\n0.000000\n2.000000\n6.000000\n9.000000\n11.000000\n\n\nloudness\n20607.0\n-6.171249\n3.191303\n-38.474000\n-7.951000\n-6.032000\n-4.057500\n2.335000\n\n\nmode\n20607.0\n0.526617\n0.499303\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nspeechiness\n20607.0\n0.093484\n0.090122\n0.000000\n0.045600\n0.060400\n0.096300\n0.959000\n\n\nacousticness\n20607.0\n0.040806\n0.118974\n0.000001\n0.000760\n0.004120\n0.022100\n0.995000\n\n\ninstrumentalness\n20607.0\n0.450730\n0.373322\n0.000000\n0.019550\n0.489000\n0.840000\n0.995000\n\n\nliveness\n20607.0\n0.195688\n0.172525\n0.007020\n0.085600\n0.119000\n0.262000\n0.988000\n\n\nvalence\n20607.0\n0.350837\n0.233250\n0.000000\n0.158000\n0.311000\n0.512000\n0.983000\n\n\ntempo\n20607.0\n132.703367\n21.461315\n0.000000\n124.991000\n128.022000\n139.978500\n248.001000\n\n\nduration_mins\n20607.0\n4.956840\n2.197097\n0.160883\n3.504317\n4.470033\n6.237517\n82.463883\n\n\ntime_signature\n20607.0\n3.975300\n0.241966\n0.000000\n4.000000\n4.000000\n4.000000\n5.000000\n\n\n\n\n\n\n\nI noticed, as seen below, that my data is not balanced by genre. I decided to balance each genre by lowering each value count to match the minority class (‘techno’). After balancing, each genre contains 820 rows of data.\n\n\nBefore balancing:\n\n\ngenre\ntrance           4367\ntech house       4362\ntechno           4234\ndrum and bass    3858\ndubstep          3786\nName: count, dtype: int64\n\n\n\n\nAfter balancing:\n\n\ngenre\ntrance           3786\ntech house       3786\ntechno           3786\ndrum and bass    3786\ndubstep          3786\nName: count, dtype: int64\n\n\n\n\nShape of dataset after balancing genres: \n\n\n(18930, 15)\n\n\n\n\nData Visualizations\nI’ve created several visualizations to examine the differences in the edm subgenres and see what appears to be identifying traits.\nThis graph shows the duration of songs per genre. It is evident there are outliers in the dataset (songs over 12 minutes), that may represent long mixes or full dj sets. Overall, it appears that most of the songs are less than 10 minutes long, with techno having more likelihood for longer songs.\n\n\n[Text(0.5, 1.0, 'Duration of Songs by Genre')]\n\n\n\n\n\nThe below density plots show the ditribution of loudness by genre. Tech-house takes the cake with the highest loudness density.\n\n\n/Users/schenfeldp/opt/anaconda3/lib/python3.8/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\nText(0.5, 0.98, 'Distribution of Loudness by Genre')\n\n\n\n\n\nBelow is a density plot showing the different distributions of tempo for each genre. Most distinctly, drum and bass along with dubstep have the most differing tempo compared to the other genres. Tech-house seems to have a distinct tempo range, with a mean around 125.\n\n\n\n\n\nThe boxplot below showing the distribution of daceability for each genre, demonstrates that the most ‘danceable’ genres are tech-house and techno.\n\n\n\n\n\n\n\nCorrelation Analysis\nThe highest correlation from this matrix seems to be ‘loudness’ and ‘energy’ with a correlation of .59. Additionally, the most negatively correlated columns are ‘energy’ and ‘acousticness’, which would make sense because acoustic versions of songs are typically lower in energy.\n\n\n\n\n\n\n\nHypothesis Generation\n\nCan we predict edm subgenres using machine learning?"
  },
  {
    "objectID": "data_exploration.html#spotify-revenues-expenses-premium-users",
    "href": "data_exploration.html#spotify-revenues-expenses-premium-users",
    "title": "Data Exploration",
    "section": "Spotify Revenues, Expenses & Premium Users",
    "text": "Spotify Revenues, Expenses & Premium Users\n\nData Understanding\nAbout this dataset:\nSpotify Revenue, Expenses and its Premium Users contains the number of premium users, number of Ad-supported users and total monthly active users (MAUs). The data spans from 2017-2023, with quarterly data.\nMAUs include number of premium users as well as number of Ad-supported users.\nNote : Sum of Premium Users and Ad-supported users can have some difference from MAUs.\nNote : All money figures are in Euro millions except ARPU which is in Euro and as it is.\nNote : All users figures are in millions.\nFollowing definitions:\nMAUs : It is defined as the total count of Ad-Supported Users and Premium Subscribers that have consumed content for greater than zero milliseconds in the last thirty days from the period-end indicated.\nPremium MAUs : It is defined as users that have completed registration with Spotify and have activated a payment method for Premium Service.\nAd MAUs : It is defined as the total count of Ad-Supported Users that have consumed content for greater than zero milliseconds in the last thirty days from the period-end indicated.\nPremium ARPU : It is average revenue per user which is monthly measure defined as Premium subscription revenue recognized in the quarter indicated divided by the average daily Premium Subscribers in such quarter, which is then divided by three months.\nCost of Revenue : Expenses done by the company.\n\n\ndate                               datetime64[ns]\ntotal_revenue                             float64\ncost_of_revenue                           float64\ngross_profit                              float64\npremium_revenue                           float64\npremium_cost_revenue                      float64\npremium_gross_profit                      float64\nad_revenue                                float64\nad_cost_of_revenue                        float64\nad_gross_profit                           float64\nmaus                                      float64\npremium_maus                              float64\nad_maus                                   float64\npremium_arpu                              float64\nsales_and_marketing_cost                  float64\nresearch_and_development_cost             float64\ngeneral_and_administrative_cost           float64\ndtype: object\n\n\n\n\nDescriptive Statistics\nBelow is a summary of the statistics of the dataset. This dataset is on the smaller side, with only 25 rows, representing 25 quarters of the year from 2017-2023. Although a small dataset, it would be interesting to see the trends over time. For instance, the lowest revenue recorded was 902 million euros, and the max was 3166 million euros. My instinct would tell me that as Spotify grew over the years, the revenue continued to increase. There are a lot of interesting metrics in this dataset to explore, especially marketing costs. This table gives a broad oversight of our data, which is particularly useful because all of our variables are numerical.\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\ndate\n25\n2020-03-30 21:07:12\n2017-03-31 00:00:00\n2018-09-30 00:00:00\n2020-03-31 00:00:00\n2021-09-30 00:00:00\n2023-03-31 00:00:00\nNaN\n\n\ntotal_revenue\n25.0\n1949.2\n902.0\n1449.0\n1855.0\n2501.0\n3166.0\n688.571468\n\n\ncost_of_revenue\n25.0\n1448.32\n775.0\n1010.0\n1381.0\n1833.0\n2365.0\n511.590634\n\n\ngross_profit\n25.0\n501.28\n105.0\n373.0\n479.0\n668.0\n801.0\n188.493484\n\n\npremium_revenue\n25.0\n1721.4\n828.0\n1210.0\n1700.0\n2178.0\n2717.0\n599.337203\n\n\npremium_cost_revenue\n25.0\n1247.04\n686.0\n894.0\n1219.0\n1545.0\n1939.0\n409.621419\n\n\npremium_gross_profit\n25.0\n474.36\n118.0\n316.0\n481.0\n633.0\n778.0\n191.566977\n\n\nad_revenue\n25.0\n215.76\n74.0\n130.0\n175.0\n282.0\n449.0\n107.578762\n\n\nad_cost_of_revenue\n25.0\n201.68\n87.0\n115.0\n157.0\n286.0\n426.0\n105.019332\n\n\nad_gross_profit\n25.0\n14.08\n-16.0\n4.0\n18.0\n25.0\n42.0\n15.739864\n\n\nmaus\n25.0\n294.72\n131.0\n191.0\n286.0\n381.0\n515.0\n117.212599\n\n\npremium_maus\n25.0\n130.08\n52.0\n87.0\n130.0\n172.0\n210.0\n49.435918\n\n\nad_maus\n25.0\n172.52\n82.0\n109.0\n163.0\n220.0\n317.0\n71.70258\n\n\npremium_arpu\n25.0\n4.6504\n4.12\n4.38\n4.63\n4.86\n5.53\n0.374661\n\n\nsales_and_marketing_cost\n25.0\n243.84\n110.0\n172.0\n236.0\n294.0\n453.0\n95.165505\n\n\nresearch_and_development_cost\n25.0\n203.0\n80.0\n135.0\n173.0\n253.0\n435.0\n101.236522\n\n\ngeneral_and_administrative_cost\n25.0\n102.36\n42.0\n73.0\n102.0\n126.0\n171.0\n34.507101\n\n\n\n\n\n\n\n\n\nData Visualization\nI began by creating a graph of the Premium monthly active users and the Ad monthly active users over time. I was curious to see if there were any trends or periods where MAUs fell. From the below graph, MAUs for both premium and ad-supported users steadily increased from 2017-2023. Premium users seem to be behind in numbers compared to the ad-supported users, this could be an indication of an opportunity to push premium sign-ups.\n\n\n&lt;matplotlib.legend.Legend at 0x7fba12d55880&gt;\n\n\n\n\n\nThis graph below shows the gross profit trends for premium and ad users. It is evident that premium users generate more profit (monthly subscription payments from users), whereas ad-supported accounts are free and generate minimal profit.\n\n\n\n\n\nBelow we can see the breakdown of costs from this dataset, which indicatively shows that ‘cost_of_revenue’ is the most significant cost.\n\n\n\n\n\nThe graph below is very interesting as it plots the number of premium monthly active users over time as well as the premium average revenue per user over time. The number of premium MAUs has steadily increased over time while the premium ARPU has decreased. What attributed to this effect?\n\n\n\n\n\n\n\nCorrelation Analysis\nThe correlation matrix below shows the correlation between the different variables in the dataset. Darker colors represent stronger correlations, while lighter colors represent weaker or no correlations. It appears that most of the variables are almost directly correlated with one another, which would make sense since total_revenue, for example, would be highly correlated with costs and profits.\n\n\n\n\n\n\n\nHypothesis Generation\nBased on my initial observations from the data, I would like to refine my research questions to the following:\n\nBased on historical data, can you create forecasts for future revenue, user engagement, or expenses?\nHow do marketing and development expenses correlate with changes in revenue and user engagement?\nHow do cost components change over time, and are there any cost-saving opportunities?"
  },
  {
    "objectID": "data_exploration.html#spotify-user-behavior",
    "href": "data_exploration.html#spotify-user-behavior",
    "title": "Data Exploration",
    "section": "Spotify User Behavior",
    "text": "Spotify User Behavior\n\nUnderstanding the Data\nBelow are definitions of the columns in this dataset.\n\nAge - Age group of user\nGender - Gender of user\nspotify_usage_period - How long have you been using Spotify?\nspotify_listening_device - Which of the following devices do you primarily use to listen to Spotify?\nspotify_subscription_plan - Which Spotify subscription plan do you currently have?\npremium_sub_willingness - Are you willing to take a premium subscription or willing to continue with premium subscription in future?\npreffered_premium_plan - If premium or willing to take premium, what amount do you pay for the subscription?\npreferred_listening_content - What do you prefer to listen more?\nfav_music_genre - What genre(s) of music do you enjoy the most?\nmusic_time_slot - What is your favourite time slot to listen to music?\nmusic_Influencial_mood - When it comes to listening to music, which of the following moods or situations most strongly influences your choice of music?\nmusic_lis_frequency - When do you listen to music more often?\nmusic_expl_method - How do you discover new music on Spotify?\nmusic_recc_rating - How do you rate the spotify music recommendations from 1-5?\npod_lis_frequency - How often do you listen to Podcast?\nfav_pod_genre - What genre(s) of Podcast do you enjoy the most?\npreffered_pod_format - What podcast format you generally prefer?\npod_host_preference - Are you more inclined to listen to podcasts from unknown personalities, or do you prefer podcasts hosted by well-known individuals?\npreffered_pod_duration - Do you prefer shorter podcast episodes (under 30 minutes) or longer episodes (over 30 minutes)\npod_variety_satisfaction - Are you satisfied with the variety and availability of podcasts on Spotify?\n\n\n\nage                            object\ngender                         object\nspotify_usage_period           object\nspotify_listening_device       object\nspotify_subscription_plan      object\npremium_sub_willingness        object\npreferred_premium_plan         object\npreferred_listening_content    object\nfav_music_genre                object\nmusic_time_slot                object\nmusic_influential_mood         object\nmusic_lis_frequency            object\nmusic_expl_method              object\nmusic_rec_rating                int64\npod_lis_frequency              object\nfav_pod_genre                  object\npreferred_pod_format           object\npod_host_preference            object\npreferred_pod_duration         object\npod_variety_satisfaction       object\ndtype: object\n\n\n\n\nDescriptive Statistics\nWe only have one numerical variable in our dataset so the below table shows the statistics for that column. It appears that the average rating for the Spotify recommendation system is 3.5 out of 5, which I would say is a favorable score overall.\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nmusic_rec_rating\n520.0\n3.503846\n0.979552\n1.0\n3.0\n3.5\n4.0\n5.0\n\n\n\n\n\n\n\n\n\nData Visualizations\nBelow is a graph showing the distribution of gender within the dataset. Within the 520 rows, there are over 350 responses from females, and only around 100 from males. The data is heavily skewed towards women.\n\n\n[Text(0.5, 1.0, 'Count of Gender')]\n\n\n\n\n\nHere we can see that the dataset is mostly comprised of users within the age range of 20-35. It is intuitive that there would be little to no users between the ages of 6-12 and over 60 years old. Although this dataset only contains 520 rows, I would believe it is likely that in general 20-35 is the most active age group on the platform.\n\n\n[Text(0.5, 1.0, 'Distribution of Age Ranges')]\n\n\n\n\n\nThis pie chart shows the breakdown of how long the users in this dataset have been using Spotify. It is a good sign that most users have been on the platform for more than 2 years. This shows great retention rate.\n\n\n\n\n\nFrom the below pie chart, it is evident that there is a lack of podcast listening within these group of listeners. The frequenices of ‘Never’ and ’Rarely make up 63.7% of the data! This could potentially be a good opportunity for Spotify to work on its reach of podcast listeners.\n\n\n\n\n\nThis horizantal bar chart shows the distribution of what the favorite podcast genres are. Comedy and Lifestyle/Health are the top 2 most popular genres. Marketing these podcasts towards users could increase podcast listening rates.\n\n\n\n\n\n\n\nCorrelation Analysis\nDue to not having numeric values in this dataset, I did not create a correlation matrix. However, the graphs below may indicate some correlation between variables.\nThis graph shows the distribution of favorite genres and what time slot they are most preferred for listening. The ‘Melody’ genre is most preferred to be listened at night, as well as the ‘Pop’ genre. The time of day may affect what genre is listened to.\n\n\n[Text(0.5, 1.0, 'Which Genre is Most Preferred at What Time Slot')]\n\n\n\n\n\nThis histogram shows a user’s willigness to change to a premium plan according to their age group. While we already know the age range of 20-35 is most abundant in this dataset, it is also evident they are more likely to switch to premium.\n\n\n[Text(0.5, 1.0, 'Willingness to Change to Premium According to Age Group')]\n\n\n\n\n\n\n\nHypothesis Generation\nBased on my EDA, I would refine my research questions to the following:\n\nWhich Premium plan is preferred by users, and does it vary by age or music preferences?\nHow can Spotify tailor its marketing and content strategy to better cater to the preferences and behavior of different user segments?\nCan personalized music or podcast recommendations be generated based on user preferences and behavior?"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Understanding the Topic: Spotify, since its inception in 2008, has evolved into a revolutionary platform that has reshaped the music industry and beyond. It’s not just about streaming music; it’s about the transformation of media consumption, content recommendation, and business models.\nWhy It Matters: This research topic is of paramount importance in today’s digital landscape. Spotify is a model for innovation in media industries, and understanding its dynamics can shed light on broader implications for technology, entertainment, and user behavior. It’s about the future of how we consume content.\nWhy You Should Continue Reading: If you’re curious about the evolution of media, the power of recommendation algorithms, and the challenges of data-driven platforms, this is a journey you won’t want to miss. Our exploration will unravel the mysteries behind the “Spotify phenomenon” and its implications for the world of content distribution.\nPast Research: Numerous research groups have dabbled in the realm of Spotify, examining everything from its business models to user behavior. Some have explored the economic impacts, while others have delved into the technological innovations that drive this platform.\nDiverse Perspectives in the Literature: The literature surrounding Spotify is rich with diverse viewpoints. Some see it as a disruptor that empowers artists, while others critique its revenue-sharing model. Some praise its personalized recommendations, while others raise concerns about data privacy.\nExploration: In my research, I will aim to navigate through this diverse landscape. I’m not just content with what we know so far. I want to uncover the hidden patterns, the evolving business strategies, and the societal impacts of Spotify.\nQuestions and Goals: What drives the success of Spotify, beyond its music catalog? How have subscription-based models changed media consumption? What are the implications of recommendation algorithms in shaping our choices? Our goal is to answer these questions and provide insights that can inform not only academia but also industry stakeholders and music enthusiasts alike.\nHypothesis: I hypothesize that Spotify’s success goes beyond music; it’s about understanding user behavior and delivering tailored content. I predict that my research will reveal the pivotal role of recommendation systems in shaping modern media consumption.\nIntrigued? Join me on this intellectual journey into the world of Spotify, where music, technology, and culture converge to shape the future of entertainment."
  },
  {
    "objectID": "introduction.html#research-topic-spotify",
    "href": "introduction.html#research-topic-spotify",
    "title": "Introduction",
    "section": "",
    "text": "Understanding the Topic: Spotify, since its inception in 2008, has evolved into a revolutionary platform that has reshaped the music industry and beyond. It’s not just about streaming music; it’s about the transformation of media consumption, content recommendation, and business models.\nWhy It Matters: This research topic is of paramount importance in today’s digital landscape. Spotify is a model for innovation in media industries, and understanding its dynamics can shed light on broader implications for technology, entertainment, and user behavior. It’s about the future of how we consume content.\nWhy You Should Continue Reading: If you’re curious about the evolution of media, the power of recommendation algorithms, and the challenges of data-driven platforms, this is a journey you won’t want to miss. Our exploration will unravel the mysteries behind the “Spotify phenomenon” and its implications for the world of content distribution.\nPast Research: Numerous research groups have dabbled in the realm of Spotify, examining everything from its business models to user behavior. Some have explored the economic impacts, while others have delved into the technological innovations that drive this platform.\nDiverse Perspectives in the Literature: The literature surrounding Spotify is rich with diverse viewpoints. Some see it as a disruptor that empowers artists, while others critique its revenue-sharing model. Some praise its personalized recommendations, while others raise concerns about data privacy.\nExploration: In my research, I will aim to navigate through this diverse landscape. I’m not just content with what we know so far. I want to uncover the hidden patterns, the evolving business strategies, and the societal impacts of Spotify.\nQuestions and Goals: What drives the success of Spotify, beyond its music catalog? How have subscription-based models changed media consumption? What are the implications of recommendation algorithms in shaping our choices? Our goal is to answer these questions and provide insights that can inform not only academia but also industry stakeholders and music enthusiasts alike.\nHypothesis: I hypothesize that Spotify’s success goes beyond music; it’s about understanding user behavior and delivering tailored content. I predict that my research will reveal the pivotal role of recommendation systems in shaping modern media consumption.\nIntrigued? Join me on this intellectual journey into the world of Spotify, where music, technology, and culture converge to shape the future of entertainment."
  },
  {
    "objectID": "introduction.html#relevant-publications",
    "href": "introduction.html#relevant-publications",
    "title": "Introduction",
    "section": "Relevant Publications",
    "text": "Relevant Publications\n\n“Universal Spotification? The shifting meanings of “Spotify” as a model for the media industries”\nBy Rasmus Fleischer (Fleischer 2021)\n\nSummary\nSince its inception in 2008, Spotify has been hailed as a transformative model for media industries. Numerous tech startups aimed to replicate this “Spotify for X” concept in various domains like books, movies, journalism, and art, but most of these attempts failed. This article, through an analysis of Swedish and US news articles from 2008 to 2018, reveals that the metaphor of “Spotify” evolved in diverse ways. The initial idea of relying on advertising to offer “free but legal” consumption was abandoned in favor of subscription-based models. Additionally, streaming services, including Spotify, witnessed a shift toward curation and algorithmic recommendation systems, which introduced new dimensions to the “Spotify for X” metaphor.\n\n\n\n“#Nowplaying on #Spotify: Leveraging Spotify Information on Twitter for Artist Recommendations”\nBy Martin Pichl, Eva Zangerie, and Günther Specht (Pichl, Zangerle, and Specht 2015)\n\nSummary\nThe advent of the internet has opened up new avenues for product distribution, such as online stores and streaming platforms, offering a wide array of products. Recommender systems play a crucial role in helping customers discover products that align with their preferences on these platforms. However, the existing literature highlights the challenge of insufficient research-appropriate data for recommender system development.\nTo address this data scarcity issue, this article introduces a music recommendation system that utilizes a dataset containing the listening habits of users who share their current music choices on the microblogging platform Twitter. Since this dataset receives daily updates, the article proposes the use of a genetic algorithm that enables the recommender system to adapt its input parameters to the continuously expanding dataset.\nIn the evaluation phase, the article compares the performance of the newly introduced recommender system to two baseline approaches. The results indicate that the proposed recommender system shows promising performance and significantly outperforms the baseline methods."
  },
  {
    "objectID": "introduction.html#data-driven-questions-regarding-spotify",
    "href": "introduction.html#data-driven-questions-regarding-spotify",
    "title": "Introduction",
    "section": "Data-Driven Questions Regarding Spotify",
    "text": "Data-Driven Questions Regarding Spotify\n\nHow has the total number of Spotify Premium subscribers changed over the past year, and what factors influence subscription growth or decline?\nWhich genres of music are currently trending on Spotify, and how have their popularity levels evolved over the past decade? Are there genres that have dramatically declined/inclined over the past decade?\nHow does the length of a Spotify playlist affect user engagement and retention, and is there an ideal playlist length for maximizing user satisfaction?\nAre there any correlations between the number of Spotify followers an artist has and their streaming success, and how does this vary across different musical genres?\nWhat are the demographics (age, gender, location) of the most active Spotify users, and how do these demographics vary by region?\nWhat is the economic impact of Spotify on the music industry, including its effect on album sales, concert ticket sales, and artist revenue distribution?\nHow does user-generated content, such as Spotify playlists created by listeners, influence the discoverability and popularity of songs and artists on the platform?\nWhat audio features (e.g., tempo, key, valence) of songs predict their potential for becoming “hit” songs on Spotify, taking into account historical hit data?\nWhat is the correlation between the release day of a song (e.g., Friday) and its streaming performance on Spotify?\nWhat is the average number of skips per song on Spotify, and how does this vary by genre, artist, or time of day?"
  },
  {
    "objectID": "data_gathering.html",
    "href": "data_gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "In the world of data-driven decision-making, the process of data gathering serves as the foundation of any meaningful analysis. I collected data from diverse sources, including APIs and databases. These sources collectively provide a comprehensive dataset that forms the cornerstone of insightful analysis."
  },
  {
    "objectID": "data_gathering.html#introduction",
    "href": "data_gathering.html#introduction",
    "title": "Data Gathering",
    "section": "",
    "text": "In the world of data-driven decision-making, the process of data gathering serves as the foundation of any meaningful analysis. I collected data from diverse sources, including APIs and databases. These sources collectively provide a comprehensive dataset that forms the cornerstone of insightful analysis."
  },
  {
    "objectID": "data_gathering.html#news-api",
    "href": "data_gathering.html#news-api",
    "title": "Data Gathering",
    "section": "News API",
    "text": "News API\nI harnessed the power of the News API to curate a collection of articles centered around Spotify. By collecting article titles and descriptions, I compiled a diverse dataset that encapsulates the latest insights and developments within the world of music and technology."
  },
  {
    "objectID": "data_gathering.html#electronic-dance-music-subgenres",
    "href": "data_gathering.html#electronic-dance-music-subgenres",
    "title": "Data Gathering",
    "section": "Electronic Dance Music Subgenres",
    "text": "Electronic Dance Music Subgenres\nUsing Spotify’s API in R, I collected 10 of the msot popular artists for each of the main subgenres of EDM: tech-house, techno, trance, dubstep, and drum and bass (dnb). I then requested the audio features of those artists of each genre to then store in a new dataset. Each genre has it’s own table containing information about the music defining that genre.\nBelow are examples of a song from each of the 5 genres. Take a listen to discern the differences between the genres!\nTech House\n\n\nTechno\n\n\nTrance\n\n\nDubstep\n\n\nDrum and Bass"
  },
  {
    "objectID": "data_gathering.html#top-5-djs",
    "href": "data_gathering.html#top-5-djs",
    "title": "Data Gathering",
    "section": "2022 Top 5 DJs",
    "text": "2022 Top 5 DJs"
  },
  {
    "objectID": "data_gathering.html#spotify-revenue-expenses-premium-users",
    "href": "data_gathering.html#spotify-revenue-expenses-premium-users",
    "title": "Data Gathering",
    "section": "Spotify Revenue, Expenses, & Premium Users",
    "text": "Spotify Revenue, Expenses, & Premium Users\nThis dataset consists of Spotify’s financial and user-related data, including information on revenue and expenses. It also provides key user metrics, such as the number of premium users, the number of ad-supported users, and the total count of monthly active users (MAUs). This dataset allows for analysis of Spotify’s financial performance and user engagement over time, providing valuable insights into the company’s growth and profitability."
  },
  {
    "objectID": "data_gathering.html#spotify-user-behavior",
    "href": "data_gathering.html#spotify-user-behavior",
    "title": "Data Gathering",
    "section": "Spotify User Behavior",
    "text": "Spotify User Behavior\nThe Spotify User Behavior Dataset from Kaggle is a repository of anonymized data, designed to provide extensive insights into the behavior and preferences of Spotify users. This dataset is purpose-built for in-depth analysis of user interactions, music consumption behaviors, and engagement metrics within the Spotify music streaming platform"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "In the world of data science, ‘Data Cleaning’ is the unsung hero of the data journey. It’s the process where the raw data, often messy and imperfect, transforms into a reliable and trustworthy foundation for analysis. Data cleaning involves identifying and rectifying inconsistencies, errors, and missing values within datasets. Without this critical step, the insights drawn from data can be inaccurate, leading to misguided decisions. I understand the significance of data cleaning in ensuring the integrity and reliability of datasets, and I apply meticulous attention to detail to uncover hidden patterns and meaningful insights buried within the data. I will walk through the pre-processing steps I took for the data mentioned in the Data Gathering tab."
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "Data Cleaning",
    "section": "",
    "text": "In the world of data science, ‘Data Cleaning’ is the unsung hero of the data journey. It’s the process where the raw data, often messy and imperfect, transforms into a reliable and trustworthy foundation for analysis. Data cleaning involves identifying and rectifying inconsistencies, errors, and missing values within datasets. Without this critical step, the insights drawn from data can be inaccurate, leading to misguided decisions. I understand the significance of data cleaning in ensuring the integrity and reliability of datasets, and I apply meticulous attention to detail to uncover hidden patterns and meaningful insights buried within the data. I will walk through the pre-processing steps I took for the data mentioned in the Data Gathering tab."
  },
  {
    "objectID": "data_cleaning.html#news-api",
    "href": "data_cleaning.html#news-api",
    "title": "Data Cleaning",
    "section": "News API",
    "text": "News API\n\nRaw Data\nI requested data from the News API using Python to request English articles that relate to Spotify. The response data was collected to a JSON file for further analysis and processing. Below is a screenshot of what the raw data looks like. You can see it contains each article along with its attributes such as author, title, description, etc. For access to the raw data, click here.\n\n\n\nCleaning the Data\nI defined a function, ‘string_cleaner’ that takes an input string, applies a series of cleaning operations to remove unwanted characters, punctuation, and extra spaces, and then converts the text to lowercase. This function is useful for preparing text data for analysis or natural language processing tasks by ensuring that the text is standardized and cleaned of noise. Below is a screenshot of what the Spotify article text looks like after cleaning. For access to the clean data file, click here."
  },
  {
    "objectID": "data_cleaning.html#electronic-dance-music-subgenres",
    "href": "data_cleaning.html#electronic-dance-music-subgenres",
    "title": "Data Cleaning",
    "section": "Electronic Dance Music Subgenres",
    "text": "Electronic Dance Music Subgenres\n\nRaw Data\nEach genre has a separate table containing 10 of the most popular DJs for that genre, along with those artists’ audio features. These features include danceability, energy, key, loudness, etc. I used the Spotify API in R in order to create this dataset. For access to the raw data files, click here.\n\n\n\nCleaning the Data\nI performed the same cleaning steps across all 5 subgenre datasets. I first took note of the shape of each dataset, which can be seen below.\n\nimport pandas as pd\n\ntrance = pd.read_csv('../../data/raw_data/edm_subgenres/trance_audio_features.csv')\ntechno = pd.read_csv('../../data/raw_data/edm_subgenres/techno_audio_features_raw.csv')\ntech_house = pd.read_csv('../../data/raw_data/edm_subgenres/tech_house_audio_features_raw.csv')\ndnb = pd.read_csv('../../data/raw_data/edm_subgenres/dnb_audio_features_raw.csv')\ndubstep = pd.read_csv('../../data/raw_data/edm_subgenres/dubstep_audio_features_raw.csv')\n\ntrance.shape\n\n(4367, 15)\n\n\n\ntechno.shape\n\n(4234, 14)\n\n\n\ntech_house.shape\n\n(4362, 14)\n\n\n\ndnb.shape\n\n(3858, 14)\n\n\n\ndubstep.shape\n\n(3790, 14)\n\n\nLuckily, for all of the datasets, the datatypes were proper and the column names were sufficient. There were no missing values in trance, techno, tech_house, or dnb datasets. However, in the dubstep dataframe, there were a handful of missing values which I needed to further investigate. After looking at the total null values for each column, I then filtered the dataframe to show only rows with null values, this way I coud get a sense of what was missing.\n\ndubstep.isnull().sum()\n\nartist_name         0\ndanceability        4\nenergy              4\nkey                 4\nloudness            4\nmode                4\nspeechiness         4\nacousticness        4\ninstrumentalness    4\nliveness            4\nvalence             4\ntempo               4\nduration_ms         4\ntime_signature      4\ndtype: int64\n\n\n\n# Let's look at the missing values in the dataaet\nnull_data = dubstep[dubstep.isnull().any(axis=1)]\nnull_data\n\n# It appears all missing values are only for the artist Excision.\n\n\n\n\n\n\n\n\nartist_name\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\ntime_signature\n\n\n\n\n10\nExcision\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n16\nExcision\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n36\nExcision\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n46\nExcision\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nI noticed that there were 4 rows in total that have complete null values for all columns (besides artist name). All 4 rows are for the artist Excision, so I further wanted to look at how many records there are for Excision as a whole, before I decided what to do with the missing values. I filtered the dataset to show only records for the artist Excision.\n\n# Let's look at all rows for Excision\ndubstep.loc[dubstep['artist_name'] == 'Excision']\n\n# We have 116 rows for Excision, and 4 of those rows have all missing values. If we drop these 4 rows that would be 3.45% of Excision data, but only \n# .32% of the entire dataset. In this case, we can drop the rows as it does not pose the threat of losing a lot of data.\n\n\n\n\n\n\n\n\nartist_name\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\ntime_signature\n\n\n\n\n0\nExcision\n0.379\n0.875\n7.0\n-2.201\n0.0\n0.0648\n0.006250\n0.00000\n0.182\n0.171\n149.960\n196233.0\n4.0\n\n\n1\nExcision\n0.576\n0.981\n10.0\n0.381\n0.0\n0.6060\n0.007790\n0.06590\n0.197\n0.139\n149.723\n243200.0\n4.0\n\n\n2\nExcision\n0.295\n0.967\n2.0\n-2.940\n1.0\n0.1740\n0.000213\n0.82100\n0.705\n0.148\n149.776\n203224.0\n4.0\n\n\n3\nExcision\n0.368\n0.976\n8.0\n-0.904\n1.0\n0.1750\n0.004530\n0.24100\n0.141\n0.174\n74.415\n189342.0\n4.0\n\n\n4\nExcision\n0.319\n0.983\n1.0\n-0.480\n1.0\n0.3800\n0.007900\n0.00144\n0.100\n0.280\n75.373\n228800.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n112\nExcision\n0.875\n0.438\n11.0\n-7.420\n1.0\n0.0769\n0.000553\n0.40900\n0.102\n0.545\n139.959\n308529.0\n4.0\n\n\n113\nExcision\n0.781\n0.958\n11.0\n-9.255\n0.0\n0.0996\n0.003450\n0.81200\n0.219\n0.364\n144.003\n279200.0\n4.0\n\n\n114\nExcision\n0.763\n0.987\n0.0\n-10.080\n1.0\n0.0607\n0.001350\n0.93900\n0.187\n0.714\n139.997\n344666.0\n4.0\n\n\n115\nExcision\n0.577\n0.770\n2.0\n-7.818\n1.0\n0.0760\n0.063000\n0.90800\n0.364\n0.435\n139.866\n287997.0\n4.0\n\n\n116\nExcision\n0.534\n0.840\n7.0\n-9.990\n1.0\n0.3440\n0.004740\n0.68900\n0.182\n0.293\n134.455\n399488.0\n4.0\n\n\n\n\n117 rows × 14 columns\n\n\n\nBased off of the output, there were 116 records for Excision, so if I decided to drop these 4 rows that would mean losing 3.45% of Excision data, or .32% of the total dataset. Based off of these numbers, I decided it was safe to drop these rows. The final shape of the dataset is shown below (1245 rows & 14 columns to begin, 1241 rows & 14 columns to end).\n\ndubstep = dubstep.dropna()\ndubstep.shape\n\n(3786, 14)\n\n\nI then combined all the genre datasets into one large dataset.\nFor all clean subgenre data files, click here."
  },
  {
    "objectID": "data_cleaning.html#top-5-djs",
    "href": "data_cleaning.html#top-5-djs",
    "title": "Data Cleaning",
    "section": "2022 Top 5 DJs",
    "text": "2022 Top 5 DJs\n\nRaw Data\nI created a dataframe for each of the 5 artists which contains audio features including ‘danceability’, ‘energy’, ‘valence’, ‘tempo’, etc. The screenshot below shows an example of the raw data pulled from Spotify for the number 1 DJ in 2022, Martin Garrix. For access to the raw data, click here.\n\n\n\nCleaning the Data\nAfter importing each of the 5 datasets for each artist, I checked the shape of each dataset.\n\nimport pandas as pd\n\nalok = pd.read_csv('../../data/raw_data/top_edm_artists/alok_audio_features.csv')\narmin = pd.read_csv('../../data/raw_data/top_edm_artists/armin_van_buuren_audio_features.csv')\ndavid_guetta = pd.read_csv('../../data/raw_data/top_edm_artists/david_guetta_audio_features.csv')\ndimitri_vegas = pd.read_csv('../../data/raw_data/top_edm_artists/dimitri_vegas_like_mike_audio_features.csv')\ngarrix = pd.read_csv('../../data/raw_data/top_edm_artists/martin_garrix_audio_features.csv')\n\n\nalok.shape\n\n(245, 36)\n\n\n\narmin.shape\n\n(1134, 36)\n\n\n\ndavid_guetta.shape\n\n(742, 36)\n\n\n\ndimitri_vegas.shape\n\n(265, 36)\n\n\n\ngarrix.shape\n\n(214, 36)\n\n\nI then removed any unnecessary columns, and combined all the datasets into one, with the shape of 2600 rows and 28 columns.\n\n# Drop unnecessary columns\nalok = alok.drop(columns=['artist_id', 'album_id', 'track_id', 'analysis_url', 'track_href', 'track_preview_url', 'track_uri', 'external_urls.spotify'])\narmin = armin.drop(columns=['artist_id', 'album_id', 'track_id', 'analysis_url', 'track_href', 'track_preview_url', 'track_uri', 'external_urls.spotify'])\ndavid_guetta = david_guetta.drop(columns=['artist_id', 'album_id', 'track_id', 'analysis_url', 'track_href', 'track_preview_url', 'track_uri', 'external_urls.spotify'])\ndimitri_vegas = dimitri_vegas.drop(columns=['artist_id', 'album_id', 'track_id', 'analysis_url', 'track_href', 'track_preview_url', 'track_uri', 'external_urls.spotify'])\ngarrix = garrix.drop(columns=['artist_id', 'album_id', 'track_id', 'analysis_url', 'track_href', 'track_preview_url', 'track_uri', 'external_urls.spotify'])\n\n# Combine datasets to one\ntop_5_djs = pd.concat([alok, armin, david_guetta, dimitri_vegas, garrix], ignore_index=True)\n\nI checked for any null values; there were none.\n\ntop_5_djs.isnull().sum()\n\nartist_name                     0\nalbum_type                      0\nalbum_release_date              0\nalbum_release_year              0\nalbum_release_date_precision    0\ndanceability                    0\nenergy                          0\nkey                             0\nloudness                        0\nmode                            0\nspeechiness                     0\nacousticness                    0\ninstrumentalness                0\nliveness                        0\nvalence                         0\ntempo                           0\ntime_signature                  0\ndisc_number                     0\nduration_ms                     0\nexplicit                        0\nis_local                        0\ntrack_name                      0\ntrack_number                    0\ntype                            0\nalbum_name                      0\nkey_name                        0\nmode_name                       0\nkey_mode                        0\ndtype: int64\n\n\nFor the cleaned file, click here."
  },
  {
    "objectID": "data_cleaning.html#spotify-revenue-expenses-premium-users",
    "href": "data_cleaning.html#spotify-revenue-expenses-premium-users",
    "title": "Data Cleaning",
    "section": "Spotify Revenue, Expenses, & Premium Users",
    "text": "Spotify Revenue, Expenses, & Premium Users\n\nRaw Data\nThis dataset was taken from Kaggle and contains data pertaining to each quarter of the year with total revenue, cost of revenue, premium revenue and more. Below is a screenshot of what the raw data looks like. Click here for the raw data file.\n\n\nCleaning the Data\nAfter loading the csv file into Python, I checked the shape of the initial raw data, which was 26 rows and 17 columns. I then began by taking a look at the top (head) and bottom (tail) of the data. When looking at the bottom, I noticed that the last row had all missing (NaN) values except for one column. I kept this in mind for later when dealing with missing values.\n\nimport pandas as pd\ndf = pd.read_csv('../../data/raw_data/revenue_expenses_premium_users/spotify_quarterly.csv')\ndf.shape\n\n(26, 17)\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nDate\nTotal Revenue\nCost of Revenue\nGross Profit\nPremium Revenue\nPremium Cost Revenue\nPremium Gross Profit\nAd Revenue\nAd Cost of revenue\nAd gross Profit\nMAUs\nPremium MAUs\nAd MAUs\nPremium ARPU\nSales and Marketing Cost\nResearch and Development Cost\nGenreal and Adminstraive Cost\n\n\n\n\n21\n31-12-2017\n1449.0\n867.0\n582.0\n1018.0\n761.0\n257.0\n130.0\n106.0\n24.0\n160.0\n71.0\n93.0\n5.24\n173.0\n123.0\n73.0\n\n\n22\n30-09-2017\n1032.0\n802.0\n230.0\n923.0\n711.0\n212.0\n109.0\n91.0\n18.0\n150.0\n62.0\n91.0\n5.06\n138.0\n98.0\n67.0\n\n\n23\n30-06-2017\n1007.0\n775.0\n232.0\n904.0\n686.0\n218.0\n103.0\n89.0\n14.0\n138.0\n59.0\n83.0\n5.53\n146.0\n95.0\n70.0\n\n\n24\n31-03-2017\n902.0\n797.0\n105.0\n828.0\n710.0\n118.0\n74.0\n87.0\n-13.0\n131.0\n52.0\n82.0\n5.46\n110.0\n80.0\n54.0\n\n\n25\n31-12-2016\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n6.00\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNext, I wanted to clean up the column names, so I removed any spaces and replaced them with underscores as well as made all column names lowercase.\n\n# Let's change the column names to have no spaces and all lowercase\ndef add_underscores(df):\n    df.columns = df.columns.str.replace(' ', '_')\n    return df\n\ndf = add_underscores(df)\ndf.columns = df.columns.str.lower()\ndf.columns\n\nIndex(['date', 'total_revenue', 'cost_of_revenue', 'gross_profit',\n       'premium_revenue', 'premium_cost_revenue', 'premium_gross_profit',\n       'ad_revenue', 'ad_cost_of_revenue', 'ad_gross_profit', 'maus',\n       'premium_maus', 'ad_maus', 'premium_arpu', 'sales_and_marketing_cost',\n       'research_and_development_cost', 'genreal_and_adminstraive_cost'],\n      dtype='object')\n\n\nThen, I renamed any column names that had spelling mistakes.\n\n# Now, lets fix any spelling mistakes in the column names\ndf = df.rename(columns={'genreal_and_adminstraive_cost': 'general_and_administrative_cost'}) \ndf.columns\n\nIndex(['date', 'total_revenue', 'cost_of_revenue', 'gross_profit',\n       'premium_revenue', 'premium_cost_revenue', 'premium_gross_profit',\n       'ad_revenue', 'ad_cost_of_revenue', 'ad_gross_profit', 'maus',\n       'premium_maus', 'ad_maus', 'premium_arpu', 'sales_and_marketing_cost',\n       'research_and_development_cost', 'general_and_administrative_cost'],\n      dtype='object')\n\n\nI summed up all of the missing values in the dataset. I kept in mind from earlier that the bottom row of the dataset was missing all values except for one column. Looking at the sum of null values, I knew that this was the only row missing any values and therefore it could be removed from the dataframe.\n\ndf.isnull().sum()\n\ndate                               0\ntotal_revenue                      1\ncost_of_revenue                    1\ngross_profit                       1\npremium_revenue                    1\npremium_cost_revenue               1\npremium_gross_profit               1\nad_revenue                         1\nad_cost_of_revenue                 1\nad_gross_profit                    1\nmaus                               1\npremium_maus                       1\nad_maus                            1\npremium_arpu                       0\nsales_and_marketing_cost           1\nresearch_and_development_cost      1\ngeneral_and_administrative_cost    1\ndtype: int64\n\n\n\ndf = df.dropna()\ndf.shape\n\n(25, 17)\n\n\nI saved the cleaned data to a csv, which you can find here."
  },
  {
    "objectID": "data_cleaning.html#spotify-user-behavior",
    "href": "data_cleaning.html#spotify-user-behavior",
    "title": "Data Cleaning",
    "section": "Spotify User Behavior",
    "text": "Spotify User Behavior\n\nRaw Data\nThis dataset encompasses a wide range of user information, including demographics like age and gender, as well as details related to Spotify usage, such as preferred listening devices and subscription plans. It also delves into user preferences, including favorite music genres and podcast habits, offering valuable insights into Spotify’s user base and their behaviors. Below is a screenshot of the raw data. For access to the file, click here.\n\n\n\nCleaning the Data\nAfter loading the csv file into Python, I checked the shape of the initial raw data, which was 520 rows and 20 columns.\n\nimport pandas as pd\ndf = pd.read_csv('../../data/raw_data/spotify_user_behavior/spotify_user_behavior.csv', keep_default_na=False)\ndf.shape\n\n(520, 20)\n\n\nI then looked at the columns and made them all lowercase and fixed any spelling errors.\n\ndf.columns = df.columns.str.lower()\ndf = df.rename(columns={'preffered_premium_plan': 'preferred_premium_plan', \n'music_influencial_mood': 'music_influential_mood', 'music_recc_rating': 'music_rec_rating', \n'preffered_pod_format': 'preferred_pod_format', 'preffered_pod_duration': 'preferred_pod_duration'})\ndf.columns \n\nIndex(['age', 'gender', 'spotify_usage_period', 'spotify_listening_device',\n       'spotify_subscription_plan', 'premium_sub_willingness',\n       'preferred_premium_plan', 'preferred_listening_content',\n       'fav_music_genre', 'music_time_slot', 'music_influential_mood',\n       'music_lis_frequency', 'music_expl_method', 'music_rec_rating',\n       'pod_lis_frequency', 'fav_pod_genre', 'preferred_pod_format',\n       'pod_host_preference', 'preferred_pod_duration',\n       'pod_variety_satisfaction'],\n      dtype='object')\n\n\nI finally checked the data types of the columns to make sure they were all the correct type, as well as checked for any missing values in the dataset. Luckily, no data types needed to be changed and there were no missing values.\n\ndf.dtypes\n\nage                            object\ngender                         object\nspotify_usage_period           object\nspotify_listening_device       object\nspotify_subscription_plan      object\npremium_sub_willingness        object\npreferred_premium_plan         object\npreferred_listening_content    object\nfav_music_genre                object\nmusic_time_slot                object\nmusic_influential_mood         object\nmusic_lis_frequency            object\nmusic_expl_method              object\nmusic_rec_rating                int64\npod_lis_frequency              object\nfav_pod_genre                  object\npreferred_pod_format           object\npod_host_preference            object\npreferred_pod_duration         object\npod_variety_satisfaction       object\ndtype: object\n\n\n\ndf.isnull().sum()\n\nage                            0\ngender                         0\nspotify_usage_period           0\nspotify_listening_device       0\nspotify_subscription_plan      0\npremium_sub_willingness        0\npreferred_premium_plan         0\npreferred_listening_content    0\nfav_music_genre                0\nmusic_time_slot                0\nmusic_influential_mood         0\nmusic_lis_frequency            0\nmusic_expl_method              0\nmusic_rec_rating               0\npod_lis_frequency              0\nfav_pod_genre                  0\npreferred_pod_format           0\npod_host_preference            0\npreferred_pod_duration         0\npod_variety_satisfaction       0\ndtype: int64\n\n\nI saved the cleaned data to a csv, which you can find here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Understanding the Topic: Spotify, since its inception in 2008, has evolved into a revolutionary platform that has reshaped the music industry and beyond. It’s not just about streaming music; it’s about the transformation of media consumption, content recommendation, and business models.\nWhy It Matters: This research topic is of paramount importance in today’s digital landscape. Spotify is a model for innovation in media industries, and understanding its dynamics can shed light on broader implications for technology, entertainment, and user behavior. It’s about the future of how we consume content.\nWhy You Should Continue Reading: If you’re curious about the evolution of media, the power of recommendation algorithms, and the challenges of data-driven platforms, this is a journey you won’t want to miss. Our exploration will unravel the mysteries behind the “Spotify phenomenon” and its implications for the world of content distribution.\nPast Research: Numerous research groups have dabbled in the realm of Spotify, examining everything from its business models to user behavior. Some have explored the economic impacts, while others have delved into the technological innovations that drive this platform.\nDiverse Perspectives in the Literature: The literature surrounding Spotify is rich with diverse viewpoints. Some see it as a disruptor that empowers artists, while others critique its revenue-sharing model. Some praise its personalized recommendations, while others raise concerns about data privacy.\nExploration: In my research, I will aim to navigate through this diverse landscape. I’m not just content with what we know so far. I want to uncover the hidden patterns, the evolving business strategies, and the societal impacts of Spotify.\nQuestions and Goals: What drives the success of Spotify, beyond its music catalog? How have subscription-based models changed media consumption? What are the implications of recommendation algorithms in shaping our choices? Our goal is to answer these questions and provide insights that can inform not only academia but also industry stakeholders and music enthusiasts alike.\nHypothesis: I hypothesize that Spotify’s success goes beyond music; it’s about understanding user behavior and delivering tailored content. I predict that my research will reveal the pivotal role of recommendation systems in shaping modern media consumption.\nIntrigued? Join me on this intellectual journey into the world of Spotify, where music, technology, and culture converge to shape the future of entertainment."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Patricia Schenfeld",
    "section": "Education",
    "text": "Education\n\nGeorgetown University, MS Data Science & Analytics\nWashington, DC | Expected May 2025\n\n\n\n\n\nMerit scholarship\nGPA: 4.0\n\n\n\n\n\nUniversity of Delaware, BS Management Information Systems\nNewark, DE | May 2021\n\n\n\n\n\nMagna Cum Laude (top 5%)\nHonors: Panel of Distinguished MIS Seniors, Dean’s List (all 8 semesters), GPA: 3.9, University of Delaware Presidential Scholarship (merit)\nStudy abroad: Australia and Thailand (Winter 2020), Switzerland and Italy (Summer 2019)"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Patricia Schenfeld",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nPython\nR\nSQL\nTableau/PowerBI\nExcel\nGoogle Data Analytics Certificate (via Coursera)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "About Patricia Schenfeld",
    "section": "Experience",
    "text": "Experience\n\nRave Me Away, Finance & Business Management Intern\nWashington, DC | August 2023 - Present\n\n\n\n\n\nTech startup addressing safety at music festivals, affiliated with Georgetown’s Startup Internships\nGenerated expense report and expense category analysis, and optimized investor pitch deck\nSuccessfully pitched and secured a featured article in EDM.com showcasing the company’s mission, resulting in increased brand visibility and industry recognition\n\n\n\n\n\nKPMG, GRC Advisory Associate\nStamford, CT | July 2021 - August 2023\n\n\n\n\n\nConducted external IT Audit for a leading home/personal products company and regional banks\n\nPerformed IT Application Control testing\nManaged offshore team; lead meetings, assigned and reviewed audit deliverables\n\nEstablished, executed, and communicated risk and compliance objectives for a major hardware firm\n\nAssessed design and implementation of IT controls against internal policies and best practices to develop Risk Control Matrices to strengthen internal control environments\nCollaborated on recommendations for attaining future state objectives\n\nAssumed additional responsibilities when no senior staff were on engagements\nTeam winner of Power BI and Alteryx Data Driven Project\n\n\n\n\n\nKPMG, Technology Assurance Intern\nNew York, NY | Summer 2020\n\n\n\n\n\nConverted to virtual program due to COVID-19"
  },
  {
    "objectID": "index.html#activities-and-volunteering",
    "href": "index.html#activities-and-volunteering",
    "title": "About Patricia Schenfeld",
    "section": "Activities and Volunteering",
    "text": "Activities and Volunteering\n\nGeorgetown University Graduate Women in Business Club\nGeorgetown University Entertainment and Media Alliance Club\nGeorgetown University Hoyas Playing Tennis Graduate Club\nKPMG, Governance, Risk and Compliance People Connectivity Committee                                           \nPhi Chi Theta Professional Business Fraternity, Founding Member of UD Chapter\nUniversity of Delaware Club Tennis Team                                                                                            \nGirl Scouts, Ambassador Level - Silver Award: Created/taught a music program for underserved children"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In this clustering analysis, we will explore a dataset containing various music attributes for songs within the Electronic Dance Music (EDM) subgenres. These attributes include loudness, acousticness, danceability, tempo, energy, and more. The dataset is sourced from Spotify and provides valuable insights into the characteristics of songs in different EDM subgenres. While the dataset originally includes labeled data associating songs with their respective subgenres, for the purpose of clustering analysis, we will remove these labels to allow the algorithms to identify patterns and group songs based on their inherent characteristics.\nOur primary goal in this analysis is to employ clustering techniques, including k-means, DBSCAN, and Hierarchical clustering, to group similar songs within the EDM subgenres. By doing so, we aim to uncover underlying patterns and structure within the EDM music landscape. Ideally, this analysis will enable us to segregate songs into clusters that reflect their commonalities in terms of musical features. Ultimately, our objective is to gain a deeper understanding of the subgenres within EDM and how songs can be grouped based on their intrinsic musical attributes. This insight can be valuable for music recommendation systems, genre classification, and understanding the diversity within EDM."
  },
  {
    "objectID": "clustering.html#introduction",
    "href": "clustering.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "In this clustering analysis, we will explore a dataset containing various music attributes for songs within the Electronic Dance Music (EDM) subgenres. These attributes include loudness, acousticness, danceability, tempo, energy, and more. The dataset is sourced from Spotify and provides valuable insights into the characteristics of songs in different EDM subgenres. While the dataset originally includes labeled data associating songs with their respective subgenres, for the purpose of clustering analysis, we will remove these labels to allow the algorithms to identify patterns and group songs based on their inherent characteristics.\nOur primary goal in this analysis is to employ clustering techniques, including k-means, DBSCAN, and Hierarchical clustering, to group similar songs within the EDM subgenres. By doing so, we aim to uncover underlying patterns and structure within the EDM music landscape. Ideally, this analysis will enable us to segregate songs into clusters that reflect their commonalities in terms of musical features. Ultimately, our objective is to gain a deeper understanding of the subgenres within EDM and how songs can be grouped based on their intrinsic musical attributes. This insight can be valuable for music recommendation systems, genre classification, and understanding the diversity within EDM."
  },
  {
    "objectID": "clustering.html#theory",
    "href": "clustering.html#theory",
    "title": "Clustering",
    "section": "Theory",
    "text": "Theory\n\nK-means Clustering:\nK-means clustering is a popular method for partitioning data into distinct clusters. The big idea behind K-means is to group data points based on their similarity. It does this by first randomly placing a set of centroids (representative points) in the data space. Then, it assigns each data point to the nearest centroid, forming initial clusters. After that, it recalculates the centroids as the mean of all data points in each cluster. This process iteratively continues until the centroids no longer change significantly, which signifies convergence.\nTo determine the optimal number of clusters (k) in K-means, model selection methods like the “elbow method” are often used. The elbow method involves plotting the cost or inertia (sum of squared distances between data points and their assigned centroids) against different values of k. The point where the cost starts to level off, forming an “elbow” in the plot, is considered an optimal value for k.\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise):\nDBSCAN is a density-based clustering method that identifies clusters as areas of high data point density separated by areas of lower density. It works by defining two parameters: a radius (ε) and a minimum number of data points (MinPts). The algorithm starts with an arbitrary data point and checks if there are at least MinPts data points within a distance of ε from it. If yes, it forms a cluster around that point and expands it by finding more dense data points in its neighborhood. This process continues until no more points can be added to the cluster.\nDBSCAN doesn’t require specifying the number of clusters in advance, making it well-suited for datasets where the number of clusters is not known. Model selection methods like the silhouette score can be used to assess the quality of clustering. The silhouette score measures the cohesion within clusters and separation between clusters. A higher silhouette score indicates better clustering.\n\n\nHierarchical Clustering:\nHierarchical clustering builds a hierarchy of clusters, where clusters can be nested within one another. There are two main approaches to hierarchical clustering: agglomerative (bottom-up) and divisive (top-down). In the agglomerative method, each data point starts as its own cluster, and at each step, the two closest clusters are merged into a single cluster. This process continues until all data points belong to a single cluster. In the divisive method, all data points initially belong to one cluster, and at each step, the cluster is split into two smaller clusters.\nTo determine the number of clusters in hierarchical clustering, a dendrogram is often used. A dendrogram is a tree-like structure that displays the sequence of merges or splits. The optimal number of clusters can be chosen by cutting the dendrogram at a point that makes sense in the context of the data.\nThese clustering methods offer different ways to group data points based on their similarities and differences, and model selection methods help in identifying the most appropriate clustering structure for a given dataset."
  },
  {
    "objectID": "clustering.html#methods",
    "href": "clustering.html#methods",
    "title": "Clustering",
    "section": "Methods",
    "text": "Methods\n\nData Selection\nBelow I have imported my EDM subgenre dataset, and then selected only numeric values to be included so it can be used for clustering analyses (this way the labeled column of ‘genre’ is not included).\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv('../clean_data/edm_subgenres/edm_subgenres_bal.csv')\n# Convert 'loudness' from decibel to linear scale\ndf['loudness_linear'] = 10 ** (df['loudness'] / 20.0)\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Select numeric features\nfeatures = df.select_dtypes(include='number')\n\n# Standardize the data\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(features)\n\nfeatures\n\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_mins\ntime_signature\nloudness_linear\n\n\n\n\n0\n0.452\n0.927\n9.0\n-6.496\n0.0\n0.0330\n0.009460\n0.353000\n0.1180\n0.0372\n131.984\n3.523433\n4.0\n0.473369\n\n\n1\n0.521\n0.861\n1.0\n-7.398\n0.0\n0.0298\n0.000998\n0.845000\n0.2990\n0.0600\n127.995\n3.843733\n4.0\n0.426678\n\n\n2\n0.711\n0.743\n11.0\n-8.672\n0.0\n0.0487\n0.061200\n0.009020\n0.0839\n0.0922\n124.020\n5.165317\n4.0\n0.368468\n\n\n3\n0.466\n0.874\n7.0\n-7.243\n1.0\n0.0437\n0.000264\n0.878000\n0.1340\n0.2420\n130.001\n6.923067\n4.0\n0.434360\n\n\n4\n0.572\n0.859\n2.0\n-5.539\n1.0\n0.0406\n0.000799\n0.733000\n0.0688\n0.4730\n118.988\n4.138083\n4.0\n0.528506\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18925\n0.513\n0.726\n9.0\n-6.402\n1.0\n0.0362\n0.000064\n0.344000\n0.0698\n0.0485\n129.924\n3.446150\n4.0\n0.478520\n\n\n18926\n0.758\n0.911\n0.0\n-3.023\n1.0\n0.3440\n0.107000\n0.005460\n0.3070\n0.4430\n139.902\n3.115600\n4.0\n0.706074\n\n\n18927\n0.669\n0.959\n11.0\n-5.192\n0.0\n0.1930\n0.007450\n0.855000\n0.1550\n0.3230\n140.018\n3.657133\n4.0\n0.550047\n\n\n18928\n0.615\n0.966\n8.0\n-3.048\n1.0\n0.0896\n0.003130\n0.012400\n0.0705\n0.1370\n153.024\n3.555550\n4.0\n0.704044\n\n\n18929\n0.452\n0.465\n4.0\n-10.006\n0.0\n0.0344\n0.058900\n0.000524\n0.0950\n0.0832\n149.586\n4.267367\n4.0\n0.316009\n\n\n\n\n18930 rows × 14 columns\n\n\n\n\n\nHyper Parameter Tuning\n\nK-Means\nFor my K-means analysis of the EDM subgenre dataset, I used the elbow and silohuette methods to help determine the optimal value for k. In the elbow graph shown below, typically the optimal value will be where the graph starts to plateau. However, in this graph there is not a clear indication of convergence, so I will also look at the silohuette graph. In the silohuette graph, the optimal value for k will be where the graph has the highest silohuette score. In this case, that would be 3.\n\n\nCode\n# Elbow method\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndistortions = []\nK = range(1, 50)  # Try different values of k\n\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(standardized_data)\n    distortions.append(kmeanModel.inertia_)\n\n\n\n\nCode\n# Plot the elbow\nplt.figure(figsize=(8, 6))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k (Number of Clusters)')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores = []\nK = range(2, 11)  # Try different values of k\n\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    labels = kmeans.fit_predict(standardized_data)\n    silhouette_avg = silhouette_score(standardized_data, labels)\n    silhouette_scores.append(silhouette_avg)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nplt.plot(K, silhouette_scores, 'bo-')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Analysis for K-means Clustering')\nplt.show()\n\n\n\n\n\n\n\n\nDBSCAN\nFor DBSCAN, I used the Silhouette method to find the optimal parameter for the clustering. The results of the below graph indicate the optimal parameter is 7, although we can also argue that we could use 5 as the silhouette score is nearly the same for 5 as it is for 7 (and we know that are true cluster groups is 5, so we will use 5).\n\n\nCode\nimport sklearn.cluster\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) \n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X) \n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.5*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n\n        if(i_print): print(param,sil_scores[-1])\n        \n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\n# DBSCAN\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nopt_labels=maximize_silhouette(standardized_data,algo=\"dbscan\",nmax=15, i_plot=True)\nplot(standardized_data,opt_labels)\n\n\nOPTIMAL PARAMETER = 7.0\n\n\n\n\n\nNameError: name 'plot' is not defined\n\n\n\n\nHierarchical Clustering\nIn the below graph, the optimal parameter for hierarchical clustering is shown to be 11. However, since at the hyperparameter of 6 the silhouette score has a negligible difference from hyperparameter 11, we can also use 6, which is closer to our actual number of clusters of 5.\n\n\nCode\nimport sklearn.cluster\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH)\n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"ag\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.25*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue\n\n        if(i_print): print(param,sil_scores[-1])\n\n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\nopt_labels=maximize_silhouette(standardized_data,algo=\"ag\",nmax=15, i_plot=True)\nplot(standardized_data,opt_labels)\n\n\nOPTIMAL PARAMETER = 11\n\n\n\n\n\nNameError: name 'plot' is not defined"
  },
  {
    "objectID": "clustering.html#final-results",
    "href": "clustering.html#final-results",
    "title": "Clustering",
    "section": "Final Results",
    "text": "Final Results\n\nK-Means\nUsing the optimal value for k from the silohuette method, 3, the graph below shows the final results of the k-means clustering. Using a scatterplot between ‘tempo’ and ‘energy’, the 3 different clusters are denoted by the different colors. It is noticeable that those with higher energy are grouped together, and lower energy grouped together. Overall, we know that the dataset has 5 actual targets (the 5 different genres), however based on the silohuette method, the optimal value for k was only 3. We may need a larger dataset in order to truly find the distinct differences between the genres.\n\n\nCode\n# Choose the optimal k\noptimal_k = 3 \n\n# Apply K-means with the chosen k\nkmeans = KMeans(n_clusters=optimal_k, random_state=0)\nclusters = kmeans.fit_predict(standardized_data)\n\n# Add the cluster labels back to your DataFrame\ndf['Cluster'] = clusters\n\n\n\n\nCode\nimport seaborn as sns\n\nsns.scatterplot(x='tempo', y= 'energy', hue='Cluster', data=df, palette='Set1')\nplt.title('K-means Clustering of EDM Subgenres')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nsns.pairplot(df, hue='Cluster', palette='Set1')\nplt.suptitle('Pair Plot of K-means Clustering of EDM Subgenres', y=1.02)\nplt.show()\n\n\n/Users/schenfeldp/opt/anaconda3/lib/python3.8/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nDBSCAN\nFrom the results of the DBSCAN as shown in the pairplot, this algorithm did not work well on my data. This could be due to the data quality as the different genres are too similar and I would benefit from a larger dataset. Testing out these unsupervised learning methods is more of an exercise as in this case we already know the labels of the data.\n\n\nCode\nimport sklearn.cluster\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) \n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"dbscan\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X) \n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.5*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n\n        if(i_print): print(param,sil_scores[-1])\n        \n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\nfrom sklearn.cluster import DBSCAN\n\n# Create a DBSCAN instance with the optimal number of clusters (5)\ndbscan = DBSCAN(eps=0.5, min_samples=5, n_clusters=5)  \n\n# Fit the DBSCAN model to your standardized_data\ndbscan.fit(standardized_data)\n\n# Get the labels assigned by DBSCAN to each data point\nlabels = dbscan.labels_\n\n# The 'labels' array contains cluster assignments (including noise points with a label of -1)\n\n\n\n\nCode\n# Combine the standardized_data and labels into a single DataFrame\ndata_with_labels = pd.DataFrame(data=standardized_data, columns=['danceability', 'energy', 'key', 'mode', 'speechiness', 'acousticness',\n       'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_mins',\n       'time_signature', 'loudness_linear'])  \ndata_with_labels['Cluster'] = labels\n\n# Create a pair plot with color-coded clusters\nsns.pairplot(data=data_with_labels, hue='Cluster', palette='Set1') \n\n# Set the title and display the pair plot\nplt.suptitle('Pair Plot of DBSCAN Clustering of EDM Subgenres', y=1.02)\nplt.show()\n\n\n/Users/schenfeldp/opt/anaconda3/lib/python3.8/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nHierarchical Clustering\nAs you can see in the below dendrogram, there are many many branches, almost seeming infinite as you go down each level. The orange and red chunks are most similar to each other as they branch off from the same starting point. The same goes for the purple and brown branches. Overall, it is difficult to make concrete conclusions based on the dendrogram, and as for many of the other unsupervised methods we used, they are not the best algorithms for this type of data.\n\n\nCode\nfrom scipy.spatial.distance import pdist\n\n# Calculate the pairwise distance matrix based on standardized_data\ndistance_matrix = pdist(standardized_data, metric='euclidean')\n\n\nlinkage_matrix = linkage(standardized_data, method='ward')\n\n\n\n\nCode\nplt.figure(figsize=(12, 6))\ndendrogram(linkage_matrix, labels=opt_labels, orientation='top', color_threshold=None)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Distance')\nplt.show()"
  },
  {
    "objectID": "dimensionality_reduction.html",
    "href": "dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Using the EDM subgenres dataset, I plan to use PCA and t-SNE to reduce the dimensions of my dataset in order to understand and utilize the most important features of the data. The objective is to reduce the number of variables to predict what genre the song belongs to. Which features are the most important in predicting these genres? We start with 13 dimensions in the dataset and will use Python and scikitlearn to perform PCA and t_SNE dimensionality reduction techniques."
  },
  {
    "objectID": "data_gathering.html#spotify-google-play-store-reviews",
    "href": "data_gathering.html#spotify-google-play-store-reviews",
    "title": "Data Gathering",
    "section": "Spotify Google Play Store Reviews",
    "text": "Spotify Google Play Store Reviews\nThis dataset contains over 61 thousand reviews from the Google Play Store of the Spotify app from 1/1/2022 - 7/9/2022."
  },
  {
    "objectID": "dimensionality_reduction.html#principal-component-analysis-pca",
    "href": "dimensionality_reduction.html#principal-component-analysis-pca",
    "title": "Dimensionality Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nWhat is PCA?\nPCA is like a magic tool that helps us understand and simplify complex data. Imagine you have a bunch of data with many measurements or features, like the height, weight, age, and income of people. PCA allows you to find the most important things that explain most of the variation in the data.\nHere’s how it works:\n\nData Simplification: PCA takes all those measurements and figures out which ones are most important. It does this by creating new variables, called “principal components,” that are combinations of the original measurements.\nReduce Dimensionality: These principal components are ordered from the most important to the least important. By using only the top ones, you can simplify your data while preserving most of the important information. It’s like taking a big puzzle and focusing on the pieces that matter the most.\nVisualization: PCA also helps in visualizing data. If you had data in many dimensions (imagine 10 or 20 different measurements), it’s hard to picture. PCA can compress this information into just a few dimensions that you can easily plot on a graph.\nNoise Reduction: By focusing on the most significant aspects of the data, PCA can also help remove some of the noise or irrelevant details, making it easier to spot patterns and relationships.\n\nIn essence, PCA is like finding the essential ingredients in a recipe, allowing you to understand and work with complex data more easily. It’s a handy tool in data analysis and can be used in various fields to uncover hidden insights in data.\n\n\nDetermining Optimal Prinicipal Components\nBelow I used a Scree Plot to determine the optimal number of principal components to keep. The plot shows the explained variance by each principal component. We want to look for the ‘elbow’ of the plot or where the plot levels off in explained variance. Although the explained variance overall is quite low, in the graph, I believe around 5-6 would be the optimal number of principal components to keep. We will use 6 for our purposes, which decreases our dimensions by over half!\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndata = standardized_data\n\n# Create a PCA object\npca = PCA()\n\n# Fit the PCA model to standardized data\npca.fit(data)\n\n# Get the explained variance ratios\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Create a scree plot\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', linestyle='-')\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Explained Variance Ratio\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\nPerforming PCA & Analysis\nIn the below code we perform the PCA and are returned with the explained variance ratios for each of the top 6 principal components. The total variance explained by these 6 principal components is approximately 66.10%. These components collectively account for this proportion of the total variance in the data. I then created a pairplot that shows a scatterplot matrix of each principal component. These graphs show the relationships and patterns between different prinicpal components. In many of the graphs, there does not appear to be any definitive clustering, except in the PC5 and PC6 plot. It is also evident to see the outliers within the data, especially those that are tightly grouped together.\n\n\nCode\n# Import the necessary libraries\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Assuming you have your data in the 'standardized_data' variable\n# Make sure your data is organized as rows (samples) and columns (features).\n\n# Create a PCA object, specifying the number of components you want to keep\nn_components = 6\npca = PCA(n_components=n_components)\n\n# Fit the PCA model to your standardized data\npca.fit(standardized_data)\n\n# Transform your data to the first 'n_components' principal components\ntransformed_data = pca.transform(standardized_data)\n\n# 'transformed_data' now contains your data in the reduced dimensionality of 'n_components'\n\n# You can also access the explained variance ratio to see how much variance\n# is explained by each principal component.\nexplained_variance = pca.explained_variance_ratio_\n\n# Print the explained variance\nprint(\"Explained Variance Ratios:\", explained_variance)\n\n\nExplained Variance Ratios: [0.18106648 0.12288109 0.10812735 0.09744782 0.07609769 0.07436597]\n\n\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create a DataFrame from the transformed data\ndata_df = pd.DataFrame(transformed_data, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"])\n\n# Create a scatterplot matrix\nsns.pairplot(data_df)\nplt.show()\n\n\n/Users/schenfeldp/opt/anaconda3/lib/python3.8/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)"
  },
  {
    "objectID": "dimensionality_reduction.html#project-proposal",
    "href": "dimensionality_reduction.html#project-proposal",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Using the EDM subgenres dataset, I plan to use PCA and t-SNE to reduce the dimensions of my dataset in order to understand and utilize the most important features of the data. The objective is to reduce the number of variables to predict what genre the song belongs to. Which features are the most important in predicting these genres? We start with 13 dimensions in the dataset and will use Python and scikitlearn to perform PCA and t_SNE dimensionality reduction techniques."
  },
  {
    "objectID": "dimensionality_reduction.html#determining",
    "href": "dimensionality_reduction.html#determining",
    "title": "Dimensionality Reduction",
    "section": "Determining",
    "text": "Determining"
  },
  {
    "objectID": "dimensionality_reduction.html#t",
    "href": "dimensionality_reduction.html#t",
    "title": "Dimensionality Reduction",
    "section": "t",
    "text": "t"
  },
  {
    "objectID": "dimensionality_reduction.html#t-distributed-stochastic-neighbor-embedding",
    "href": "dimensionality_reduction.html#t-distributed-stochastic-neighbor-embedding",
    "title": "Dimensionality Reduction",
    "section": "t-Distributed Stochastic Neighbor Embedding",
    "text": "t-Distributed Stochastic Neighbor Embedding"
  },
  {
    "objectID": "dimensionality_reduction.html#t-distributed-stochastic-neighbor-embedding-t-sne",
    "href": "dimensionality_reduction.html#t-distributed-stochastic-neighbor-embedding-t-sne",
    "title": "Dimensionality Reduction",
    "section": "t-Distributed Stochastic Neighbor Embedding (t-SNE)",
    "text": "t-Distributed Stochastic Neighbor Embedding (t-SNE)\n\nWhat is t-SNE?\nt-SNE, or t-Distributed Stochastic Neighbor Embedding, is a data visualization technique that helps you understand and explore high-dimensional data in a more intuitive and lower-dimensional way.\nIn simple terms, t-SNE takes a complex dataset with many features and transforms it into a two- or three-dimensional representation. In this new representation, similar data points are positioned near each other, and dissimilar points are placed farther apart. This makes it easier to see clusters or patterns in your data.\nImagine you have a puzzle with many puzzle pieces, and you want to simplify it by grouping together similar pieces. t-SNE is like a magic tool that takes those pieces and arranges them on a smaller puzzle board so that similar pieces are next to each other, making it easier for you to see the bigger picture and understand how they relate to each other.\nKeep in mind that t-SNE is useful for data visualization and exploration but might not preserve the exact distances or relationships between data points in the original high-dimensional space. It’s a handy tool for uncovering structures and patterns in your data.\n\n\nImplementing t-SNE\nBelow I implemented t-SNE on my EDM subgenre dataset. The visualization shows clusters within the dataset. It seems to be a trend where the clusters for this dataset are not very distinct.\n\n\nCode\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Perform t-SNE on the standardized data with two components\ntsne = TSNE(n_components=2, random_state=44)\ntsne_result = tsne.fit_transform(standardized_data)\n\n# Create a scatter plot to visualize the t-SNE results\nplt.figure(figsize=(8, 6))\nplt.scatter(tsne_result[:, 0], tsne_result[:, 1]) \nplt.title(\"t-SNE Visualization of Standardized Data\")\nplt.show()\n\n\n\n\n\n\n\nExploring Different Perplexities\nBelow we explore varying perplexities within t-SNE. You can see that when the perplexity is very low (5) the graph is more of just one big blob. As the perplexity increases, you can see the clusters start to separate further and further.\n\n\nCode\n# Import the necessary libraries\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Define a range of perplexity values to explore\nperplexities = [5, 10, 20, 30, 40]\n\n# Iterate over the perplexity values\nfor perplexity in perplexities:\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n    tsne_result = tsne.fit_transform(standardized_data)\n    \n    # Create a scatter plot for the current perplexity value\n    plt.figure(figsize=(8, 6))\n    plt.scatter(tsne_result[:, 0], tsne_result[:, 1])\n    plt.title(f\"t-SNE Visualization (Perplexity={perplexity})\")\n    plt.show()"
  },
  {
    "objectID": "dimensionality_reduction.html#evaluation-and-comparison",
    "href": "dimensionality_reduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison",
    "text": "Evaluation and Comparison\nEvaluation and Comparison of PCA and t-SNE:\n\nEffectiveness in Preserving Data Structure and Information:\n\nPCA: PCA is effective at preserving the overall data structure and global patterns in the data. It achieves this by maximizing variance along principal components. However, it may not capture fine-grained local structures and non-linear relationships.\nt-SNE: t-SNE is effective at preserving local structures and capturing non-linear relationships. It’s particularly useful for revealing clusters and patterns that might not be evident in the original high-dimensional space. However, it may not preserve global structures as well as PCA.\n\nVisualization Capabilities:\n\nPCA: PCA provides a clear visualization of global data structures, making it suitable for understanding broad data relationships. It reduces data dimensions in a way that retains as much variance as possible. However, PCA might not reveal fine local structures or clusters.\nt-SNE: t-SNE excels in visualizing local structures and clusters. It’s effective at revealing fine-grained patterns and relationships. It’s especially useful for exploratory data analysis when you want to focus on local details.\n\nTrade-offs and Scenarios:\n\nPCA is advantageous when:\n\nYou want to reduce dimensionality while preserving as much information as possible.\nYou are interested in understanding global patterns and relationships in the data.\nYour data has a linear structure, and you need to remove multicollinearity.\nYou need a simplified representation of data for modeling purposes.\n\nt-SNE is advantageous when:\n\nYou want to explore and visualize local structures, clusters, and non-linear relationships.\nThe high-dimensional data contains complex, non-linear patterns that are not easily visible in the original space.\nYou are focusing on data exploration and fine-grained insights, rather than dimensionality reduction.\nYour dataset is suitable for capturing local structures without losing too much global context.\n\n\n\nIn summary: - PCA and t-SNE serve different purposes and have their own strengths and weaknesses. The choice between them depends on your specific objectives and the nature of your data. - PCA is best suited for dimensionality reduction, preserving global structure, and understanding broad data relationships. - t-SNE is effective for local structure preservation and fine-grained pattern recognition, making it a powerful tool for data exploration and visualization. - In practice, it’s common to use both techniques, depending on the goals of the analysis. PCA can be used for initial dimensionality reduction, followed by t-SNE to explore and visualize local structures and clusters."
  },
  {
    "objectID": "dimensionality_reduction.html#evaluation-and-comparison-of-pca-and-t-sne",
    "href": "dimensionality_reduction.html#evaluation-and-comparison-of-pca-and-t-sne",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison of PCA and t-SNE",
    "text": "Evaluation and Comparison of PCA and t-SNE\n\nEffectiveness in Preserving Data Structure and Information:\n\nPCA: PCA is effective at preserving the overall data structure and global patterns in the data. It achieves this by maximizing variance along principal components. However, it may not capture fine-grained local structures and non-linear relationships.\nt-SNE: t-SNE is effective at preserving local structures and capturing non-linear relationships. It’s particularly useful for revealing clusters and patterns that might not be evident in the original high-dimensional space. However, it may not preserve global structures as well as PCA.\n\nVisualization Capabilities:\n\nPCA: PCA provides a clear visualization of global data structures, making it suitable for understanding broad data relationships. It reduces data dimensions in a way that retains as much variance as possible. However, PCA might not reveal fine local structures or clusters.\nt-SNE: t-SNE excels in visualizing local structures and clusters. It’s effective at revealing fine-grained patterns and relationships. It’s especially useful for exploratory data analysis when you want to focus on local details.\n\nTrade-offs and Scenarios:\n\nPCA is advantageous when:\n\nYou want to reduce dimensionality while preserving as much information as possible.\nYou are interested in understanding global patterns and relationships in the data.\nYour data has a linear structure, and you need to remove multicollinearity.\nYou need a simplified representation of data for modeling purposes.\n\nt-SNE is advantageous when:\n\nYou want to explore and visualize local structures, clusters, and non-linear relationships.\nThe high-dimensional data contains complex, non-linear patterns that are not easily visible in the original space.\nYou are focusing on data exploration and fine-grained insights, rather than dimensionality reduction.\nYour dataset is suitable for capturing local structures without losing too much global context.\n\n\n\nIn summary: - PCA and t-SNE serve different purposes and have their own strengths and weaknesses. The choice between them depends on your specific objectives and the nature of your data. - PCA is best suited for dimensionality reduction, preserving global structure, and understanding broad data relationships. - t-SNE is effective for local structure preservation and fine-grained pattern recognition, making it a powerful tool for data exploration and visualization. - In practice, it’s common to use both techniques, depending on the goals of the analysis. PCA can be used for initial dimensionality reduction, followed by t-SNE to explore and visualize local structures and clusters."
  },
  {
    "objectID": "clustering.html#final-resultsconclusions",
    "href": "clustering.html#final-resultsconclusions",
    "title": "Clustering",
    "section": "Final Results/Conclusions",
    "text": "Final Results/Conclusions\n\nK-Means\nUsing the optimal value for k from the silohuette method, 3, the graph below shows the final results of the k-means clustering. Using a scatterplot between ‘tempo’ and ‘energy’, the 3 different clusters are denoted by the different colors. It is noticeable that those with higher energy are grouped together, and lower energy grouped together. Overall, we know that the dataset has 5 actual targets (the 5 different genres), however based on the silohuette method, the optimal value for k was only 3. We may need a larger dataset in order to truly find the distinct differences between the genres.\n\n\nCode\n# Choose the optimal k\noptimal_k = 3 \n\n# Apply K-means with the chosen k\nkmeans = KMeans(n_clusters=optimal_k, random_state=0)\nclusters = kmeans.fit_predict(standardized_data)\n\n# Add the cluster labels back to your DataFrame\ndf['Cluster'] = clusters\n\n\n\n\nCode\nimport seaborn as sns\n\nsns.scatterplot(x='tempo', y= 'energy', hue='Cluster', data=df, palette='Set1')\nplt.title('K-means Clustering of EDM Subgenres')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nsns.pairplot(df, hue='Cluster', palette='Set1')\nplt.suptitle('Pair Plot of K-means Clustering of EDM Subgenres', y=1.02)\nplt.show()\n\n\n/Users/schenfeldp/opt/anaconda3/lib/python3.8/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nDBSCAN\nFrom the results of the DBSCAN as shown in the pairplot, this algorithm did not work well on my data. This could be due to the data quality as the different genres are too similar and I would benefit from a larger dataset. Testing out these unsupervised learning methods is more of an exercise as in this case we already know the labels of the data.\n\n\nCode\nimport sklearn.cluster\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) \n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"dbscan\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X) \n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.5*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n\n        if(i_print): print(param,sil_scores[-1])\n        \n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\nfrom sklearn.cluster import DBSCAN\n\n# Create a DBSCAN instance with the optimal number of clusters (5)\ndbscan = DBSCAN(eps=0.5, min_samples=5, n_clusters=5)  \n\n# Fit the DBSCAN model to your standardized_data\ndbscan.fit(standardized_data)\n\n# Get the labels assigned by DBSCAN to each data point\nlabels = dbscan.labels_\n\n# The 'labels' array contains cluster assignments (including noise points with a label of -1)\n\n\n\n\nCode\n# Combine the standardized_data and labels into a single DataFrame\ndata_with_labels = pd.DataFrame(data=standardized_data, columns=['danceability', 'energy', 'key', 'mode', 'speechiness', 'acousticness',\n       'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_mins',\n       'time_signature', 'loudness_linear'])  \ndata_with_labels['Cluster'] = labels\n\n# Create a pair plot with color-coded clusters\nsns.pairplot(data=data_with_labels, hue='Cluster', palette='Set1') \n\n# Set the title and display the pair plot\nplt.suptitle('Pair Plot of DBSCAN Clustering of EDM Subgenres', y=1.02)\nplt.show()\n\n\n/Users/schenfeldp/opt/anaconda3/lib/python3.8/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nHierarchical Clustering\nAs you can see in the below dendrogram, there are many many branches, almost seeming infinite as you go down each level. The orange and red chunks are most similar to each other as they branch off from the same starting point. The same goes for the purple and brown branches. Overall, it is difficult to make concrete conclusions based on the dendrogram, and as for many of the other unsupervised methods we used, they are not the best algorithms for this type of data.\n\n\nCode\nfrom scipy.spatial.distance import pdist\n\n# Calculate the pairwise distance matrix based on standardized_data\ndistance_matrix = pdist(standardized_data, metric='euclidean')\n\n\nlinkage_matrix = linkage(standardized_data, method='ward')\n\n\n\n\nCode\nplt.figure(figsize=(12, 6))\ndendrogram(linkage_matrix, labels=opt_labels, orientation='top', color_threshold=None)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Distance')\nplt.show()"
  },
  {
    "objectID": "decision_trees_classification.html",
    "href": "decision_trees_classification.html",
    "title": "Classification Decision Tree",
    "section": "",
    "text": "Decision Tree:\nDecision trees are a powerful tool in machine learning that mimics the way humans make decisions. Imagine you have a complex problem, and you’re trying to solve it by breaking it down into a series of simpler decisions. A decision tree does just that. It starts with a big question at the top (the root) and then branches out into a series of smaller questions based on the answers at each step. These questions are based on the features of your data, and the goal is to ultimately reach a decision or prediction at the leaves of the tree. Each decision is made by evaluating the importance of different features in the data, helping the model to learn patterns and make accurate predictions.\nRandom Forest:\nRandom Forest takes the idea of decision trees a step further by creating a “forest” or a collection of decision trees. Each tree is trained on a different subset of the data and makes its own set of predictions. Instead of relying on just one tree, Random Forest combines the predictions from all the trees to make a more robust and accurate final prediction. The “random” part comes from the fact that each tree is trained on a random subset of features, adding an element of diversity. This helps prevent overfitting, where the model becomes too tailored to the training data and performs poorly on new, unseen data. By leveraging the collective intelligence of multiple trees, Random Forest provides a more reliable and generalizable solution.\nXGBoost:\nXGBoost, or Extreme Gradient Boosting, is a sophisticated algorithm that excels in predictive modeling. At its core, XGBoost is an ensemble method like Random Forest, but it uses a different strategy. Instead of training all the trees independently, XGBoost builds them sequentially, learning from the mistakes of the previous ones. It’s like a team of experts playing a game, with each expert focusing on the aspects where others fell short. XGBoost is particularly good at handling complex relationships in the data, capturing patterns that might be difficult for other algorithms. It also incorporates a regularization term to control the complexity of the model, preventing it from becoming too intricate and overfitting the training data. Overall, XGBoost is a powerful and flexible tool for making accurate predictions in a wide range of applications."
  },
  {
    "objectID": "decision_trees_classification.html#methods",
    "href": "decision_trees_classification.html#methods",
    "title": "Classification Decision Tree",
    "section": "",
    "text": "Decision Tree:\nDecision trees are a powerful tool in machine learning that mimics the way humans make decisions. Imagine you have a complex problem, and you’re trying to solve it by breaking it down into a series of simpler decisions. A decision tree does just that. It starts with a big question at the top (the root) and then branches out into a series of smaller questions based on the answers at each step. These questions are based on the features of your data, and the goal is to ultimately reach a decision or prediction at the leaves of the tree. Each decision is made by evaluating the importance of different features in the data, helping the model to learn patterns and make accurate predictions.\nRandom Forest:\nRandom Forest takes the idea of decision trees a step further by creating a “forest” or a collection of decision trees. Each tree is trained on a different subset of the data and makes its own set of predictions. Instead of relying on just one tree, Random Forest combines the predictions from all the trees to make a more robust and accurate final prediction. The “random” part comes from the fact that each tree is trained on a random subset of features, adding an element of diversity. This helps prevent overfitting, where the model becomes too tailored to the training data and performs poorly on new, unseen data. By leveraging the collective intelligence of multiple trees, Random Forest provides a more reliable and generalizable solution.\nXGBoost:\nXGBoost, or Extreme Gradient Boosting, is a sophisticated algorithm that excels in predictive modeling. At its core, XGBoost is an ensemble method like Random Forest, but it uses a different strategy. Instead of training all the trees independently, XGBoost builds them sequentially, learning from the mistakes of the previous ones. It’s like a team of experts playing a game, with each expert focusing on the aspects where others fell short. XGBoost is particularly good at handling complex relationships in the data, capturing patterns that might be difficult for other algorithms. It also incorporates a regularization term to control the complexity of the model, preventing it from becoming too intricate and overfitting the training data. Overall, XGBoost is a powerful and flexible tool for making accurate predictions in a wide range of applications."
  },
  {
    "objectID": "decision_trees_classification.html#decision-tree",
    "href": "decision_trees_classification.html#decision-tree",
    "title": "Classification Decision Tree",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nClass"
  },
  {
    "objectID": "decision_trees_classification.html#class",
    "href": "decision_trees_classification.html#class",
    "title": "Classification Decision Tree",
    "section": "Class",
    "text": "Class"
  },
  {
    "objectID": "decision_trees_classification.html#class-distribution",
    "href": "decision_trees_classification.html#class-distribution",
    "title": "Classification Decision Tree",
    "section": "Class Distribution",
    "text": "Class Distribution\nIn the below code, I take the EDM subgenre dataset and look at the class distribution of the target labels. As shown in the data exploration tab of the website, I had intentionally balanced the classes in order to have an evenly distributed dataset; each genre has 3786 tracks. Having evenly distributed class labels in a dataset, often referred to as class balance or class distribution, can offer several advantages in the context of machine learning:\n\nImproved Model Performance:\n\nPreventing Bias: Models trained on imbalanced datasets may develop a bias towards the majority class, leading to poorer performance on minority classes. A balanced distribution helps ensure that the model learns to recognize patterns from all classes equally, leading to a more accurate and fair predictive model.\n\nBetter Generalization:\n\nEnhanced Generalization: Models trained on imbalanced data might perform well on the training set but struggle to generalize to new, unseen data. A balanced dataset helps in creating models that are more likely to generalize well, as they are exposed to a representative distribution of examples from each class during training.\n\nMore Robust Evaluation:\n\nAccurate Evaluation Metrics: Imbalanced datasets can mislead the evaluation of a model. For example, accuracy may seem high, but it might be due to the model predicting the majority class most of the time. Balanced datasets provide a more reliable evaluation, allowing for the use of metrics like precision, recall, and F1 score that consider the performance across all classes.\n\nStable Training:\n\nStable Convergence: Training models on imbalanced data can lead to instability in the learning process. An evenly distributed dataset often contributes to a more stable convergence during training, preventing the model from getting stuck in local minima and improving convergence speed.\n\nReduced Sensitivity to Data Changes:\n\nRobustness to Data Drift: In real-world applications, the distribution of data may change over time. Models trained on balanced datasets tend to be more robust to such changes, as they have learned patterns from all classes rather than being overly specialized in the majority class.\n\nEnhanced Feature Importance:\n\nAccurate Feature Importance: In imbalanced datasets, models may assign exaggerated importance to features correlated with the majority class. Balanced datasets allow models to assign importance more accurately, reflecting the actual contribution of features to the prediction of each class.\n\n\nWhile having balanced class labels is often beneficial, it’s essential to note that real-world data is not always perfectly balanced. In cases of significant class imbalance, techniques such as oversampling the minority class, undersampling the majority class, or using specialized algorithms designed for imbalanced data may be employed to address these challenges.\n\n\nCode\ndf.genre.value_counts()\n\n\ngenre\ntrance           3786\ntech house       3786\ntechno           3786\ndrum and bass    3786\ndubstep          3786\nName: count, dtype: int64"
  },
  {
    "objectID": "decision_trees_classification.html#baseline-model-for-comparison",
    "href": "decision_trees_classification.html#baseline-model-for-comparison",
    "title": "Classification Decision Tree",
    "section": "Baseline Model for Comparison",
    "text": "Baseline Model for Comparison\nThe baseline comparison using a random classifier serves as a critical benchmark in evaluating the performance of more sophisticated machine learning models. The purpose of the random classifier is to establish a baseline level of performance that any meaningful model should surpass. This baseline is particularly important when dealing with classification tasks, as it helps assess whether the chosen model is learning patterns from the data or is merely performing at a chance level. By comparing accuracy, precision, recall, and F1-score metrics from the random classifier against those of a trained model, one can gauge the model’s effectiveness in capturing relevant patterns and making informed predictions. This baseline comparison is instrumental in validating the significance of the machine learning model’s contributions to the task at hand, guiding researchers and practitioners in the development and selection of effective classification algorithms.\n\n\nCode\nimport random\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef generate_balanced_data(num_labels, N=10000):\n    # Generates N random labels with balanced load across all labels.\n    weights = [1/num_labels] * num_labels\n    labels = list(range(num_labels))\n    y = random.choices(labels, weights=weights, k=N)\n    return y\n\n# TEST\nnum_labels = 5\ny = generate_balanced_data(num_labels, 10000)\n\ndef random_classifier(y_data):\n    # Implements a random classifier that predicts labels randomly.\n    max_label = np.max(y_data)\n    \n    # Generate random predictions within the range of labels.\n    y_pred = [int(np.floor((max_label + 1) * np.random.uniform(0, 1))) for _ in range(len(y_data))]\n    \n    # Print information about the random classifier's predictions.\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\", Counter(y_pred).values())\n    print(\"probability of prediction:\", np.fromiter(Counter(y_pred).values(), dtype=float) / len(y_data))\n    print(\"accuracy:\", accuracy_score(y_data, y_pred))\n    precision, recall, _, _ = precision_recall_fscore_support(y_data, y_pred, average='macro')\n    print(\"precision:\", precision)\n    print(\"recall:\", recall)\n\n# Testing the random classifier with balanced labels\nprint(\"\\nMULTI-CLASS:  BALANCED LOAD\")\ny = generate_balanced_data(num_labels, 10000)\nrandom_classifier(y)\n\n\n\nMULTI-CLASS:  BALANCED LOAD\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([2062, 2012, 1931, 2044, 1951])\nprobability of prediction: [0.2062 0.2012 0.1931 0.2044 0.1951]\naccuracy: 0.2017\nprecision: 0.20162185181207337\nrecall: 0.20156229528691672"
  },
  {
    "objectID": "decision_trees_classification.html#decision-tree-classifier",
    "href": "decision_trees_classification.html#decision-tree-classifier",
    "title": "Classification Decision Tree",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\nFinal Results\nAfter modeling the decision tree, the below are the results: * Accuracy: 69.18% * The accuracy is the ratio of correctly predicted instances to the total instances. In this case, approximately 69.18% of the test set instances were predicted correctly by the Decision Tree Classifier. * Precision: 69.19% * Precision is a measure of the accuracy of the positive predictions. In this context, it indicates that around 69.19% of the instances predicted as positive by the model were actually positive. * Recall: 69.17% * Recall (or sensitivity) measures the ability of the model to capture all the relevant instances. A recall of 69.17% means that the model identified around 69.17% of the actual positive instances in the dataset.\n\n\nCode\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport matplotlib.pyplot as plt\n\n# Assuming 'genre' is the target column\ntarget_column = 'genre'\n\n# Split the data into features (X) and the target variable (y)\nX = df.select_dtypes(include='number')\ny = df[target_column]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\nprint(\"-----DECISION TREE CLASSIFIER-----\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprecision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\n\n-----DECISION TREE CLASSIFIER-----\nAccuracy: 0.6917591125198098\nPrecision: 0.6919464788765407\nRecall: 0.691736509738276\n\n\n\n\nCode\nimport graphviz\ndef visualize_tree(dtc_object):\n  dot_data = sklearn.tree.export_graphviz(dtc_object, out_file=None,\n                      feature_names=X_train.columns,\n                      class_names=list(genre_map.keys()),\n                      filled=True, rounded=True,\n                      special_characters=True)\n  graph = graphviz.Source(dot_data)\n  graph.render(outfile=\"test.svg\")\n  return graph\nvisualize_tree(dtc)"
  },
  {
    "objectID": "decision_trees_classification.html#random-forest-classifier",
    "href": "decision_trees_classification.html#random-forest-classifier",
    "title": "Classification Decision Tree",
    "section": "Random Forest Classifier",
    "text": "Random Forest Classifier\n\nFinal Results\nAfter performing hyper-parameter tuning on our random forest algorithm, the results very slightly improved: * Accuracy: 77.87% * The accuracy is the ratio of correctly predicted instances to the total instances. In this case, approximately 77.87% of the test set instances were predicted correctly by the Decision Tree Classifier. * Precision: 77.87% * Precision is a measure of the accuracy of the positive predictions. In this context, it indicates that around 77.87% of the instances predicted as positive by the model were actually positive. * Recall: 77.82% * Recall (or sensitivity) measures the ability of the model to capture all the relevant instances. A recall of 77.82% means that the model identified around 77.82% of the actual positive instances in the dataset.\n\n\nCode\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\ntarget_column = 'genre'\n\n# Split the data into features (X) and the target variable (y)\nX = df.select_dtypes(include='number')\ny = df[target_column]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train a Random Forest Classifier\nrf_classifier = RandomForestClassifier(random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = rf_classifier.predict(X_test)\n\n# Evaluate the model\nprint(\"-----RANDOM FOREST CLASSIFIER-----\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\n# Calculate precision and recall\nprecision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\n-----RANDOM FOREST CLASSIFIER-----\nAccuracy: 0.7778658214474379\nPrecision: 0.7787186900592407\nRecall: 0.7781547761093727\n\n\n\n\nHyper-Parameter Tuning\n\n\nCode\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ntarget_column = 'genre'\n\n# Split the data into features (X) and the target variable (y)\nX = df.select_dtypes(include='number')\ny = df[target_column]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Random Forest Classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Define hyperparameters and their possible values for tuning\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Use Grid Search with cross-validation to find the best hyperparameters\ngrid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Train a Random Forest Classifier with the best hyperparameters\nbest_rf_classifier = RandomForestClassifier(random_state=42, **best_params)\nbest_rf_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = best_rf_classifier.predict(X_test)\n\n# Evaluate the model\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\n# Calculate precision and recall\nprecision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\nBest Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\nAccuracy: 0.7786582144743793\nPrecision: 0.7794206959651867\nRecall: 0.7788913235366173"
  },
  {
    "objectID": "decision_trees_classification.html#xgboost-classifier",
    "href": "decision_trees_classification.html#xgboost-classifier",
    "title": "Classification Decision Tree",
    "section": "XGBoost Classifier",
    "text": "XGBoost Classifier\n\nFinal Results\nAfter performing hyper-parameter tuning on our XGBoost algorithm, the results very slightly improved: * Accuracy: 78.79% * The accuracy is the ratio of correctly predicted instances to the total instances. In this case, approximately 78.79% of the test set instances were predicted correctly by the Decision Tree Classifier. * Precision: 78.96% * Precision is a measure of the accuracy of the positive predictions. In this context, it indicates that around 78.96% of the instances predicted as positive by the model were actually positive. * Recall: 78.82% * Recall (or sensitivity) measures the ability of the model to capture all the relevant instances. A recall of 78.82% means that the model identified around 78.82% of the actual positive instances in the dataset.\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\n\ntarget_column = 'genre'\n\n# Create a label encoder and fit it to the target variable\nlabel_encoder = LabelEncoder()\ndf['encoded_genre'] = label_encoder.fit_transform(df[target_column])\n\n# Split the data into features (X) and the target variable (y)\n# Filter for only numeric columns in the feature matrix\nX = df.select_dtypes(include=['number']).drop('encoded_genre', axis=1)\ny = df['encoded_genre']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train an XGBoost classifier\nxgb_classifier = xgb.XGBClassifier(random_state=42)\nxgb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = xgb_classifier.predict(X_test)\n\n# Decode the predicted labels back to the original class labels\ny_pred_original = label_encoder.inverse_transform(y_pred)\n\n# Evaluate the model\nprint(\"-----XGBOOST CLASSIFIER-----\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprecision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\n-----XGBOOST CLASSIFIER-----\nAccuracy: 0.7823560486001057\nPrecision: 0.7838098236528813\nRecall: 0.7824599382154643\n\n\n\n\nHyper-Parameter Tuning\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\n\n# Assuming 'genre' is the target column\ntarget_column = 'genre'\n\n# Create a label encoder and fit it to the target variable\nlabel_encoder = LabelEncoder()\ndf['encoded_genre'] = label_encoder.fit_transform(df[target_column])\n\n# Split the data into features (X) and the target variable (y)\nX = df.select_dtypes(include=['number']).drop('encoded_genre', axis=1)\ny = df['encoded_genre']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an XGBoost classifier\nxgb_classifier = xgb.XGBClassifier(random_state=42)\n\n# Define a parameter grid to search\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.1, 0.01, 0.001]\n}\n\n# Create GridSearchCV object\ngrid_search = GridSearchCV(xgb_classifier, param_grid, cv=5, scoring='accuracy')\n\n# Fit the model\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters from the grid search\nbest_params = grid_search.best_params_\n\n# Use the best parameters to train the final model\nfinal_model = xgb.XGBClassifier(**best_params, random_state=42)\nfinal_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = final_model.predict(X_test)\n\n# Decode the predicted labels back to the original class labels\ny_pred_original = label_encoder.inverse_transform(y_pred)\n\n# Evaluate the model\nprint(\"-----XGBOOST CLASSIFIER-----\")\nprint(\"Best Parameters:\", best_params)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprecision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\n-----XGBOOST CLASSIFIER-----\nBest Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\nAccuracy: 0.7879027997886952\nPrecision: 0.7895878189560148\nRecall: 0.7880435869170253"
  },
  {
    "objectID": "decision_trees_classification.html#conclusions",
    "href": "decision_trees_classification.html#conclusions",
    "title": "Classification Decision Tree",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "decision_trees_classification.html#decision-tree-classifier-1",
    "href": "decision_trees_classification.html#decision-tree-classifier-1",
    "title": "Classification Decision Tree",
    "section": "Decision Tree Classifier:",
    "text": "Decision Tree Classifier:\n\nAccuracy: 69.18%\nPrecision: 69.19%\nRecall: 69.17%\n\nConclusion: The Decision Tree Classifier, while providing a baseline, might face challenges in capturing the intricate relationships inherent in EDM subgenre classification. It may not be sufficient for discerning the nuanced patterns associated with different EDM subgenres."
  },
  {
    "objectID": "decision_trees_classification.html#random-forest-classifier-1",
    "href": "decision_trees_classification.html#random-forest-classifier-1",
    "title": "Classification Decision Tree",
    "section": "Random Forest Classifier:",
    "text": "Random Forest Classifier:\n\nDefault:\n\nAccuracy: 77.79%\nPrecision: 77.87%\nRecall: 77.82%\n\nTuned:\n\nBest Hyperparameters: {‘max_depth’: 20, ‘min_samples_leaf’: 1, ‘min_samples_split’: 5, ‘n_estimators’: 200}\nAccuracy: 77.87%\nPrecision: 77.94%\nRecall: 77.89%\n\n\nConclusion: The Random Forest Classifier, with or without tuning, offers an improvement over the Decision Tree, making it more suitable for capturing the diverse patterns and characteristics associated with different EDM subgenres. It can handle multiple features simultaneously and provides a solid foundation for EDM subgenre classification."
  },
  {
    "objectID": "decision_trees_classification.html#xgboost-classifier-1",
    "href": "decision_trees_classification.html#xgboost-classifier-1",
    "title": "Classification Decision Tree",
    "section": "XGBoost Classifier:",
    "text": "XGBoost Classifier:\n\nAccuracy: 78.79%\nPrecision: 78.96%\nRecall: 78.80%\n\nConclusion: XGBoost, known for its superior performance in various contexts, including structured data, demonstrates competitive accuracy in predicting EDM subgenres. Its ability to handle complex relationships and adapt to sequential learning makes it a strong candidate for EDM subgenre classification tasks."
  },
  {
    "objectID": "decision_trees_classification.html#overall-recommendation",
    "href": "decision_trees_classification.html#overall-recommendation",
    "title": "Classification Decision Tree",
    "section": "Overall Recommendation:",
    "text": "Overall Recommendation:\n\nDecision Tree: While interpretable, it may struggle to capture the nuances of EDM subgenres. Consider it as a starting point or for simple cases.\nRandom Forest: A robust choice, especially after hyperparameter tuning, providing a good balance between interpretability and predictive power for EDM subgenre classification.\nXGBoost: Offers high accuracy and is well-suited for complex patterns in EDM subgenres. Consider it for tasks where precision and recall are crucial."
  },
  {
    "objectID": "decision_trees_classification.html#overall-conclusions",
    "href": "decision_trees_classification.html#overall-conclusions",
    "title": "Classification Decision Tree",
    "section": "Overall Conclusions &",
    "text": "Overall Conclusions &\n\nDecision Tree Classifier:\n\nAccuracy: 69.18%\nPrecision: 69.19%\nRecall: 69.17%\n\nConclusion: The Decision Tree Classifier, while providing a baseline, might face challenges in capturing the intricate relationships inherent in EDM subgenre classification. It may not be sufficient for discerning the nuanced patterns associated with different EDM subgenres.\n\n\nRandom Forest Classifier:\n\nDefault:\n\nAccuracy: 77.79%\nPrecision: 77.87%\nRecall: 77.82%\n\nTuned:\n\nBest Hyperparameters: {‘max_depth’: 20, ‘min_samples_leaf’: 1, ‘min_samples_split’: 5, ‘n_estimators’: 200}\nAccuracy: 77.87%\nPrecision: 77.94%\nRecall: 77.89%\n\n\nConclusion: The Random Forest Classifier, with or without tuning, offers an improvement over the Decision Tree, making it more suitable for capturing the diverse patterns and characteristics associated with different EDM subgenres. It can handle multiple features simultaneously and provides a solid foundation for EDM subgenre classification."
  },
  {
    "objectID": "decision_trees_classification.html#overall-conclusions-summary",
    "href": "decision_trees_classification.html#overall-conclusions-summary",
    "title": "Classification Decision Tree",
    "section": "Overall Conclusions & Summary",
    "text": "Overall Conclusions & Summary\n\nDecision Tree Classifier:\n\nAccuracy: 69.18%\nPrecision: 69.19%\nRecall: 69.17%\n\nConclusion: The Decision Tree Classifier, while providing a baseline, might face challenges in capturing the intricate relationships inherent in EDM subgenre classification. It may not be sufficient for discerning the nuanced patterns associated with different EDM subgenres.\n\n\nRandom Forest Classifier:\n\nAccuracy: 77.87%\nPrecision: 77.94%\nRecall: 77.89%\n\nConclusion: The Random Forest Classifier, with or without tuning, offers an improvement over the Decision Tree, making it more suitable for capturing the diverse patterns and characteristics associated with different EDM subgenres. It can handle multiple features simultaneously and provides a solid foundation for EDM subgenre classification.\n\n\nXGBoost Classifier:\n\nAccuracy: 78.79%\nPrecision: 78.96%\nRecall: 78.80%\n\nConclusion: XGBoost, known for its superior performance in various contexts, including structured data, demonstrates competitive accuracy in predicting EDM subgenres. Its ability to handle complex relationships and adapt to sequential learning makes it a strong candidate for EDM subgenre classification tasks.\n\n\nOverall Recommendation:\n\nDecision Tree: While interpretable, it may struggle to capture the nuances of EDM subgenres. Consider it as a starting point or for simple cases.\nRandom Forest: A robust choice, especially after hyperparameter tuning, providing a good balance between interpretability and predictive power for EDM subgenre classification.\nXGBoost: Offers high accuracy and is well-suited for complex patterns in EDM subgenres. Consider it for tasks where precision and recall are crucial.\n\nIn our exploration of predicting EDM subgenres using machine learning models like Decision Trees, Random Forests, and XGBoost, we uncovered some interesting insights. Imagine these models as detectives trying to understand the distinct characteristics that define different types of electronic dance music. What we found was that the Random Forest and XGBoost detectives were particularly skilled at deciphering the intricate patterns that distinguish one EDM subgenre from another, outperforming the simpler Decision Tree detective.\nThis discovery is important because it opens up new possibilities for music enthusiasts and industry professionals alike. By leveraging these advanced models, we can more accurately categorize and identify EDM subgenres, offering a deeper understanding of the unique elements that make each genre special. Imagine having a virtual music expert that can not only recognize your favorite subgenres but also introduce you to new and exciting ones based on subtle musical features.\nHowever, our journey wasn’t without its challenges. The Decision Tree detective, while straightforward, struggled to grasp the complexity of EDM subgenres. Yet, with the Random Forest and XGBoost detectives, armed with their ability to collaborate and adapt, we were able to overcome these limitations and create a more nuanced picture of the EDM landscape."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusions",
    "section": "",
    "text": "Overall Conclusions: Unlocking Insights Across Spotify’s Ecosystem\nThis comprehensive data science project on Spotify has provided a multifaceted view into the music streaming giant’s landscape, leveraging diverse datasets to extract valuable insights. Each dataset, ranging from news articles to user behavior and EDM subgenres, contributed to a holistic understanding of Spotify’s dynamics.\nNews API Dataset: The exploration of news articles surrounding Spotify illuminated key industry trends and developments. The word cloud analysis visually encapsulated the prevailing themes, with ‘Spotify,’ ‘music,’ and ‘AI’ dominating discussions. This provided a unique lens into the ever-evolving music streaming industry, acting as a compass for understanding the pivotal topics shaping Spotify’s narrative.\nSpotify Revenue, Expenses, and Premium Users Dataset: The financial analysis showcased a steady rise in monthly active users, with a noteworthy divergence between premium and ad-supported user counts. The examination of costs highlighted the significance of ‘cost_of_revenue.’ The intriguing relationship between premium MAUs and average revenue per user raised questions about contributing factors, laying the groundwork for further investigation.\nSpotify User Behavior Dataset: User demographics and engagement patterns emerged from the analysis, revealing a concentration of active users in the 20-35 age group. The positive retention rate, evident from user tenure, suggests a strong user base. However, the underutilization of podcasts among certain listeners presents an opportunity for strategic improvements in Spotify’s podcast offerings.\nEDM Subgenre Dataset: The machine learning models’ performance in predicting EDM subgenres showcased the evolving landscape of genre classification. While the Decision Tree provided a baseline, the Random Forest and XGBoost models demonstrated superior accuracy, hinting at the complexity and nuance inherent in defining electronic dance music subgenres. These models not only enhance our understanding of EDM but also offer exciting possibilities for industry applications.\nKey Insights and Recommendations: 1. User Engagement and Retention: - The concentration of users in the 20-35 age group emphasizes the importance of tailoring features and content to this demographic. - Addressing the lack of podcast engagement presents an opportunity for strategic initiatives to expand Spotify’s podcast listener base.\n\nFinancial Strategy:\n\nThe divergence between premium and ad-supported users suggests untapped potential in promoting premium subscriptions.\nExploring the factors influencing the decrease in premium average revenue per user over time is crucial for sustaining profitability.\n\nEDM Subgenre Classification:\n\nWhile the Decision Tree provides interpretability, the Random Forest and XGBoost models offer enhanced accuracy in classifying EDM subgenres.\nIncorporating advanced models in genre classification can revolutionize content recommendation systems, offering a more personalized and nuanced user experience.\n\n\nConclusion: Navigating the Spotify Soundscape In traversing the multifaceted realms of news, user behavior, and EDM subgenres within Spotify, this project has unearthed valuable insights and posed thought-provoking questions. The intertwining narratives of financial trends, user engagement patterns, and musical categorization form a dynamic tapestry that invites further exploration.\nAs Spotify continues to shape the music streaming landscape, the project’s findings serve as a compass for strategic decision-making. From refining marketing strategies to enhancing user experiences and embracing the nuances of music genres, the road ahead is marked by opportunities for innovation and growth.\nThis project not only sheds light on the current state of Spotify but also sets the stage for ongoing research, fostering a deeper understanding of user preferences, industry dynamics, and the intricate world of music classification.\n\n\n\n Back to top"
  },
  {
    "objectID": "index 2.12.03 AM.html",
    "href": "index 2.12.03 AM.html",
    "title": "About Patricia Schenfeld",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n      \nPatricia Schenfeld is a Masters student in the Data Science and Analytics program at Georgetown University, graduating in May 2025. She earned her Bachelor’s degree from the University of Delaware in 2021 where she majored in Management Information Systems and minored in Business Analytics. After working in consulting at KPMG for 2 years, Patricia is eager to pivot careers into analytics. She aspires to seamlessly blend her passion for electronic dance music with her burgeoning career in data science, using data-driven insights to enhance the experience for music fans worldwide.\nGU NetID: ps1304"
  },
  {
    "objectID": "index 2.12.03 AM.html#education",
    "href": "index 2.12.03 AM.html#education",
    "title": "About Patricia Schenfeld",
    "section": "Education",
    "text": "Education\n\nGeorgetown University, MS Data Science & Analytics\nWashington, DC | Expected May 2025\n\n\n\n\n\nMerit scholarship\nGPA: 4.0\n\n\n\n\n\nUniversity of Delaware, BS Management Information Systems\nNewark, DE | May 2021\n\n\n\n\n\nMagna Cum Laude (top 5%)\nHonors: Panel of Distinguished MIS Seniors, Dean’s List (all 8 semesters), GPA: 3.9, University of Delaware Presidential Scholarship (merit)\nStudy abroad: Australia and Thailand (Winter 2020), Switzerland and Italy (Summer 2019)"
  },
  {
    "objectID": "index 2.12.03 AM.html#technical-skills",
    "href": "index 2.12.03 AM.html#technical-skills",
    "title": "About Patricia Schenfeld",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nPython\nR\nSQL\nTableau/PowerBI\nExcel\nGoogle Data Analytics Certificate (via Coursera)"
  },
  {
    "objectID": "index 2.12.03 AM.html#experience",
    "href": "index 2.12.03 AM.html#experience",
    "title": "About Patricia Schenfeld",
    "section": "Experience",
    "text": "Experience\n\nRave Me Away, Finance & Business Management Intern\nWashington, DC | August 2023 - Present\n\n\n\n\n\nTech startup addressing safety at music festivals, affiliated with Georgetown’s Startup Internships\nGenerated expense report and expense category analysis, and optimized investor pitch deck\nSuccessfully pitched and secured a featured article in EDM.com showcasing the company’s mission, resulting in increased brand visibility and industry recognition\n\n\n\n\n\nKPMG, GRC Advisory Associate\nStamford, CT | July 2021 - August 2023\n\n\n\n\n\nConducted external IT Audit for a leading home/personal products company and regional banks\n\nPerformed IT Application Control testing\nManaged offshore team; lead meetings, assigned and reviewed audit deliverables\n\nEstablished, executed, and communicated risk and compliance objectives for a major hardware firm\n\nAssessed design and implementation of IT controls against internal policies and best practices to develop Risk Control Matrices to strengthen internal control environments\nCollaborated on recommendations for attaining future state objectives\n\nAssumed additional responsibilities when no senior staff were on engagements\nTeam winner of Power BI and Alteryx Data Driven Project\n\n\n\n\n\nKPMG, Technology Assurance Intern\nNew York, NY | Summer 2020\n\n\n\n\n\nConverted to virtual program due to COVID-19"
  },
  {
    "objectID": "index 2.12.03 AM.html#activities-and-volunteering",
    "href": "index 2.12.03 AM.html#activities-and-volunteering",
    "title": "About Patricia Schenfeld",
    "section": "Activities and Volunteering",
    "text": "Activities and Volunteering\n\nGeorgetown University Graduate Women in Business Club\nGeorgetown University Entertainment and Media Alliance Club\nGeorgetown University Hoyas Playing Tennis Graduate Club\nKPMG, Governance, Risk and Compliance People Connectivity Committee                                           \nPhi Chi Theta Professional Business Fraternity, Founding Member of UD Chapter\nUniversity of Delaware Club Tennis Team                                                                                            \nGirl Scouts, Ambassador Level - Silver Award: Created/taught a music program for underserved children"
  },
  {
    "objectID": "index.html#research-topic-spotify",
    "href": "index.html#research-topic-spotify",
    "title": "Introduction",
    "section": "",
    "text": "Understanding the Topic: Spotify, since its inception in 2008, has evolved into a revolutionary platform that has reshaped the music industry and beyond. It’s not just about streaming music; it’s about the transformation of media consumption, content recommendation, and business models.\nWhy It Matters: This research topic is of paramount importance in today’s digital landscape. Spotify is a model for innovation in media industries, and understanding its dynamics can shed light on broader implications for technology, entertainment, and user behavior. It’s about the future of how we consume content.\nWhy You Should Continue Reading: If you’re curious about the evolution of media, the power of recommendation algorithms, and the challenges of data-driven platforms, this is a journey you won’t want to miss. Our exploration will unravel the mysteries behind the “Spotify phenomenon” and its implications for the world of content distribution.\nPast Research: Numerous research groups have dabbled in the realm of Spotify, examining everything from its business models to user behavior. Some have explored the economic impacts, while others have delved into the technological innovations that drive this platform.\nDiverse Perspectives in the Literature: The literature surrounding Spotify is rich with diverse viewpoints. Some see it as a disruptor that empowers artists, while others critique its revenue-sharing model. Some praise its personalized recommendations, while others raise concerns about data privacy.\nExploration: In my research, I will aim to navigate through this diverse landscape. I’m not just content with what we know so far. I want to uncover the hidden patterns, the evolving business strategies, and the societal impacts of Spotify.\nQuestions and Goals: What drives the success of Spotify, beyond its music catalog? How have subscription-based models changed media consumption? What are the implications of recommendation algorithms in shaping our choices? Our goal is to answer these questions and provide insights that can inform not only academia but also industry stakeholders and music enthusiasts alike.\nHypothesis: I hypothesize that Spotify’s success goes beyond music; it’s about understanding user behavior and delivering tailored content. I predict that my research will reveal the pivotal role of recommendation systems in shaping modern media consumption.\nIntrigued? Join me on this intellectual journey into the world of Spotify, where music, technology, and culture converge to shape the future of entertainment."
  },
  {
    "objectID": "index.html#relevant-publications",
    "href": "index.html#relevant-publications",
    "title": "Introduction",
    "section": "Relevant Publications",
    "text": "Relevant Publications\n\n“Universal Spotification? The shifting meanings of “Spotify” as a model for the media industries”\nBy Rasmus Fleischer (Fleischer 2021)\n\nSummary\nSince its inception in 2008, Spotify has been hailed as a transformative model for media industries. Numerous tech startups aimed to replicate this “Spotify for X” concept in various domains like books, movies, journalism, and art, but most of these attempts failed. This article, through an analysis of Swedish and US news articles from 2008 to 2018, reveals that the metaphor of “Spotify” evolved in diverse ways. The initial idea of relying on advertising to offer “free but legal” consumption was abandoned in favor of subscription-based models. Additionally, streaming services, including Spotify, witnessed a shift toward curation and algorithmic recommendation systems, which introduced new dimensions to the “Spotify for X” metaphor.\n\n\n\n“#Nowplaying on #Spotify: Leveraging Spotify Information on Twitter for Artist Recommendations”\nBy Martin Pichl, Eva Zangerie, and Günther Specht (Pichl, Zangerle, and Specht 2015)\n\nSummary\nThe advent of the internet has opened up new avenues for product distribution, such as online stores and streaming platforms, offering a wide array of products. Recommender systems play a crucial role in helping customers discover products that align with their preferences on these platforms. However, the existing literature highlights the challenge of insufficient research-appropriate data for recommender system development.\nTo address this data scarcity issue, this article introduces a music recommendation system that utilizes a dataset containing the listening habits of users who share their current music choices on the microblogging platform Twitter. Since this dataset receives daily updates, the article proposes the use of a genetic algorithm that enables the recommender system to adapt its input parameters to the continuously expanding dataset.\nIn the evaluation phase, the article compares the performance of the newly introduced recommender system to two baseline approaches. The results indicate that the proposed recommender system shows promising performance and significantly outperforms the baseline methods."
  },
  {
    "objectID": "index.html#data-driven-questions-regarding-spotify",
    "href": "index.html#data-driven-questions-regarding-spotify",
    "title": "Introduction",
    "section": "Data-Driven Questions Regarding Spotify",
    "text": "Data-Driven Questions Regarding Spotify\n\nHow has the total number of Spotify Premium subscribers changed over the past year, and what factors influence subscription growth or decline?\nWhich genres of music are currently trending on Spotify, and how have their popularity levels evolved over the past decade? Are there genres that have dramatically declined/inclined over the past decade?\nHow does the length of a Spotify playlist affect user engagement and retention, and is there an ideal playlist length for maximizing user satisfaction?\nAre there any correlations between the number of Spotify followers an artist has and their streaming success, and how does this vary across different musical genres?\nWhat are the demographics (age, gender, location) of the most active Spotify users, and how do these demographics vary by region?\nWhat is the economic impact of Spotify on the music industry, including its effect on album sales, concert ticket sales, and artist revenue distribution?\nHow does user-generated content, such as Spotify playlists created by listeners, influence the discoverability and popularity of songs and artists on the platform?\nWhat audio features (e.g., tempo, key, valence) of songs predict their potential for becoming “hit” songs on Spotify, taking into account historical hit data?\nWhat is the correlation between the release day of a song (e.g., Friday) and its streaming performance on Spotify?\nWhat is the average number of skips per song on Spotify, and how does this vary by genre, artist, or time of day?"
  }
]