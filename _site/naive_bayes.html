<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Patricia Schenfeld - Naïve Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Patricia Schenfeld</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./introduction.html" rel="" target="">
 <span class="menu-text">Introduction</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/anly501/dsan-5000-project-pschenfeld" rel="" target="">
 <span class="menu-text">Code</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Data</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data">    
        <li>
    <a class="dropdown-item" href="./data_gathering.html" rel="" target="">
 <span class="dropdown-text">Data Gathering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./data_cleaning.html" rel="" target="">
 <span class="dropdown-text">Data Cleaning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./data_exploration.html" rel="" target="">
 <span class="dropdown-text">Data Exploration</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./clustering.html" rel="" target="">
 <span class="dropdown-text">Clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./dimensionality_reduction.html" rel="" target="">
 <span class="dropdown-text">Dimensionality Reduction</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link active" href="./naive_bayes.html" rel="" target="" aria-current="page">
 <span class="menu-text">Naïve Bayes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./decision_trees_classification.html" rel="" target="">
 <span class="menu-text">Decision Trees</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./conclusion.html" rel="" target="">
 <span class="menu-text">Conclusions</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#spotify-google-play-store-reviews" id="toc-spotify-google-play-store-reviews" class="nav-link" data-scroll-target="#spotify-google-play-store-reviews">Spotify Google Play Store Reviews</a>
  <ul class="collapse">
  <li><a href="#feature-selection-for-text-data" id="toc-feature-selection-for-text-data" class="nav-link" data-scroll-target="#feature-selection-for-text-data">Feature Selection for Text Data</a></li>
  <li><a href="#naive-bayes-classification-model" id="toc-naive-bayes-classification-model" class="nav-link" data-scroll-target="#naive-bayes-classification-model">Naive Bayes Classification Model</a></li>
  <li><a href="#model-results-interpretation" id="toc-model-results-interpretation" class="nav-link" data-scroll-target="#model-results-interpretation">Model Results Interpretation</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  <li><a href="#edm-subgenres" id="toc-edm-subgenres" class="nav-link" data-scroll-target="#edm-subgenres">EDM Subgenres</a>
  <ul class="collapse">
  <li><a href="#feature-selection-for-record-data" id="toc-feature-selection-for-record-data" class="nav-link" data-scroll-target="#feature-selection-for-record-data">Feature Selection for Record Data</a></li>
  <li><a href="#naive-bayes-classification-model-1" id="toc-naive-bayes-classification-model-1" class="nav-link" data-scroll-target="#naive-bayes-classification-model-1">Naive Bayes Classification Model</a></li>
  <li><a href="#model-results-interpretation-1" id="toc-model-results-interpretation-1" class="nav-link" data-scroll-target="#model-results-interpretation-1">Model Results Interpretation</a></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Naïve Bayes</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p><strong>Overview of Naive Bayes Classification:</strong></p>
<p>Naive Bayes classification is a popular machine learning algorithm used for various tasks, such as text classification, spam detection, and sentiment analysis. At its core, Naive Bayes is based on probability theory and Bayes’ theorem. It’s a simple yet effective method for categorizing data into predefined classes or categories.</p>
<p><strong>Probabilistic Nature and Bayes’ Theorem Foundation:</strong></p>
<p>Naive Bayes is grounded in Bayes’ theorem, which is a fundamental concept in probability theory. It uses conditional probability to make predictions. In essence, Naive Bayes calculates the probability that a data point belongs to a specific class based on the observed features or attributes associated with that data point.</p>
<p>The “naive” part of Naive Bayes comes from the assumption that the features used in the classification are conditionally independent, which means that the presence or absence of one feature does not affect the presence or absence of another feature. While this assumption simplifies the model, it may not always hold true in real-world scenarios.</p>
<p><strong>Objectives of Naive Bayes Classification:</strong></p>
<p>The primary objective of Naive Bayes classification is to assign a class label to a data point based on its features. This is often used for tasks like spam email detection, sentiment analysis, or categorizing documents.</p>
<p><strong>Achievements Through Naive Bayes Classification:</strong></p>
<p>Through Naive Bayes classification, we aim to achieve the following:</p>
<ol type="1">
<li><p><strong>Efficient Text Classification:</strong> Naive Bayes is particularly effective for text classification tasks, such as classifying emails as spam or not spam.</p></li>
<li><p><strong>Quick and Simple Predictions:</strong> Naive Bayes is computationally efficient and works well with high-dimensional data. It can make predictions quickly, making it suitable for real-time applications.</p></li>
<li><p><strong>Good Performance with Small Datasets:</strong> Naive Bayes can perform well even when you have limited data, making it a valuable choice for scenarios with smaller datasets.</p></li>
</ol>
<p><strong>Variants of Naive Bayes and When to Use Each:</strong></p>
<p>There are several variants of Naive Bayes classification, and the choice of which one to use depends on the nature of your data:</p>
<ol type="1">
<li><p><strong>Gaussian Naive Bayes:</strong> Use this when your features are continuous and have a Gaussian (normal) distribution. It’s suitable for data like measurements, where the values follow a bell-shaped curve.</p></li>
<li><p><strong>Multinomial Naive Bayes:</strong> This variant is commonly used for text classification tasks, where the features represent the frequency of words or tokens in documents. It’s suitable for discrete data like word counts.</p></li>
<li><p><strong>Bernoulli Naive Bayes:</strong> Use Bernoulli Naive Bayes when your features are binary, representing presence or absence of certain attributes. It’s often used for tasks like spam detection, where the focus is on whether a word is present or not in a document.</p></li>
</ol>
<p>In summary, Naive Bayes classification is a probabilistic approach based on Bayes’ theorem that aims to assign class labels to data points. It’s a versatile algorithm with different variants suited for various types of data, making it a valuable tool in machine learning and data analysis.</p>
</section>
<section id="spotify-google-play-store-reviews" class="level2">
<h2 class="anchored" data-anchor-id="spotify-google-play-store-reviews">Spotify Google Play Store Reviews</h2>
<section id="feature-selection-for-text-data" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-for-text-data">Feature Selection for Text Data</h3>
<p><strong>Objective:</strong> The primary objective of the Feature Selection component in this project is to identify and choose the most relevant and informative features (variables or attributes) from the dataset, for the given task. Effective feature selection can improve the model’s performance, reduce overfitting, and enhance the interpretability of the results.</p>
<p>Using the Spotify Reviews dataset, which contains the columns ‘time_submitted’, ‘review’, ‘rating’, ‘processed_review’ and ‘sentiment’, I have selected ‘processed_review’ (which is the ‘review’ column cleaned and pre-processed for analysis) as my input feature, and ‘sentiment’ (positve or negative) as my target feature. I plan to use a Naive Bayes classification model to classify the inputted review as either positive or negative.</p>
</section>
<section id="naive-bayes-classification-model" class="level3">
<h3 class="anchored" data-anchor-id="naive-bayes-classification-model">Naive Bayes Classification Model</h3>
<p>In Python, after importing my dataset, I complete the following steps:</p>
<ul>
<li>Train-test split
<ul>
<li>I split the data into training and test sets, where 80% of the data is used for training, and the remaining 20% for testing.</li>
</ul></li>
<li>Feature extraction (Use TF-IDF vectorization to convert text data into numerical features)
<ul>
<li>TF-IDF stands for “Term Frequency-Inverse Document Frequency.” It is a numerical statistic that reflects the importance of a word (term) within a document or a collection of documents (corpus). TF-IDF is commonly used in information retrieval and text mining to determine the importance of words in a document relative to their frequency in the entire corpus.</li>
</ul></li>
<li>Train a Naive Bayes classifier
<ul>
<li>I create and train a Multinomial Naive Bayes classifier, a commonly used algorithm for text classification tasks.</li>
</ul></li>
<li>Evaluate the model
<ul>
<li>I use the trained model to make predictions on the test set, assigning sentiment labels to the reviews.</li>
</ul></li>
<li>Calculate accuracy and generate a classification report
<ul>
<li>I assess the performance of the model using key metrics, including:
<ul>
<li>Accuracy: A measure of the model’s overall correctness in predicting sentiments.</li>
<li>Precision: The ratio of true positive predictions to the total positive predictions.</li>
<li>Recall: The ratio of true positive predictions to the total actual positives in the data.</li>
<li>F1-score: A metric that combines precision and recall for a balanced evaluation. The F1 score reaches its best value at 1 (perfect precision and recall) and its worst at 0 (either precision or recall is 0).</li>
</ul></li>
<li>The classification report provides detailed information on precision, recall, and F1-score for both positive and negative sentiments, along with support values indicating the number of instances for each class.</li>
</ul></li>
</ul>
<p>Below you can see the code used for the model as well as the summary report output and visualizations.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the necessary libraries and load the dataset</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, classification_report</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset containing Spotify reviews and their ratings</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/clean_data/reviews/spotify_reviews_sentiment.csv'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Train-test split</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[<span class="st">'processed_review'</span>]  <span class="co"># Text data</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'sentiment'</span>]  <span class="co"># Binary sentiment label</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature extraction</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Use TF-IDF vectorization to convert text data into numerical features</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> vectorizer.fit_transform(X_train)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> vectorizer.transform(X_test)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a Naive Bayes classifier</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train a Multinomial Naive Bayes classifier</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MultinomialNB()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the trained model to make predictions on the test set</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy and generate a classification report</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(report)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.8477149119246692
              precision    recall  f1-score   support

    negative       0.82      0.91      0.86      6321
    positive       0.89      0.79      0.83      5998

    accuracy                           0.85     12319
   macro avg       0.85      0.85      0.85     12319
weighted avg       0.85      0.85      0.85     12319
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Class labels for binary classification</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>class_labels <span class="op">=</span> [<span class="st">'Positive'</span>, <span class="st">'Negative'</span>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))  <span class="co"># Adjust the figure size</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Define labels for the confusion matrix cells</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>group_names <span class="op">=</span> [<span class="st">'True Pos'</span>, <span class="st">'False Neg'</span>, <span class="st">'False Pos'</span>, <span class="st">'True Neg'</span>]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>group_counts <span class="op">=</span> [<span class="st">"</span><span class="sc">{0:0.0f}</span><span class="st">"</span>.<span class="bu">format</span>(value) <span class="cf">for</span> value <span class="kw">in</span> conf_matrix.flatten()]</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate percentages relative to true instances (actual positives and actual negatives)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>tp, fn, fp, tn <span class="op">=</span> conf_matrix.ravel()</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>total_positives <span class="op">=</span> tp <span class="op">+</span> fn</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>total_negatives <span class="op">=</span> fp <span class="op">+</span> tn</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>tp_percentage <span class="op">=</span> tp <span class="op">/</span> total_positives</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>fn_percentage <span class="op">=</span> fn <span class="op">/</span> total_positives</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>fp_percentage <span class="op">=</span> fp <span class="op">/</span> total_negatives</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>tn_percentage <span class="op">=</span> tn <span class="op">/</span> total_negatives</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>group_percentages <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>tp_percentage<span class="sc">:.2%}</span><span class="ss">"</span>, <span class="ss">f"</span><span class="sc">{</span>fn_percentage<span class="sc">:.2%}</span><span class="ss">"</span>, <span class="ss">f"</span><span class="sc">{</span>fp_percentage<span class="sc">:.2%}</span><span class="ss">"</span>, <span class="ss">f"</span><span class="sc">{</span>tn_percentage<span class="sc">:.2%}</span><span class="ss">"</span>]</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>v1<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>v2<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>v3<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> v1, v2, v3 <span class="kw">in</span> <span class="bu">zip</span>(group_names, group_counts, group_percentages)]</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.asarray(labels).reshape(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a heatmap and annotate the cells with labels</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span>labels, fmt<span class="op">=</span><span class="st">''</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, xticklabels<span class="op">=</span>class_labels, yticklabels<span class="op">=</span>class_labels)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Actual'</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix'</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="naive_bayes_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the sentiment distribution</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>sentiment_distribution <span class="op">=</span> df[<span class="st">'sentiment'</span>].value_counts()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percentages</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>percentages <span class="op">=</span> (sentiment_distribution <span class="op">/</span> sentiment_distribution.<span class="bu">sum</span>()) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the bar plot</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sentiment_distribution.plot(kind<span class="op">=</span><span class="st">'bar'</span>, color<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'green'</span>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Sentiment'</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distribution of Sentiments'</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the bars with percentages</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, count <span class="kw">in</span> <span class="bu">enumerate</span>(sentiment_distribution):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    ax.text(i, count, <span class="ss">f'</span><span class="sc">{</span>count<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>percentages[i]<span class="sc">:.2f}</span><span class="ss">%)'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="naive_bayes_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning curve function</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_learning_curve(estimator, title, X, y, ylim<span class="op">=</span><span class="va">None</span>, cv<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>                        n_jobs<span class="op">=</span><span class="dv">1</span>, train_sizes<span class="op">=</span>np.linspace(<span class="fl">.1</span>, <span class="fl">1.0</span>, <span class="dv">5</span>)):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ylim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        plt.ylim(<span class="op">*</span>ylim)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Training examples"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Accuracy"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        estimator, X, y, cv<span class="op">=</span>cv, n_jobs<span class="op">=</span>n_jobs, train_sizes<span class="op">=</span>train_sizes, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    train_scores_mean <span class="op">=</span> np.mean(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    train_scores_std <span class="op">=</span> np.std(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    test_scores_mean <span class="op">=</span> np.mean(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    test_scores_std <span class="op">=</span> np.std(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(train_sizes, train_scores_mean <span class="op">-</span> train_scores_std,</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>                     train_scores_mean <span class="op">+</span> train_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">"r"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(train_sizes, test_scores_mean <span class="op">-</span> test_scores_std,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>                     test_scores_mean <span class="op">+</span> test_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">"g"</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    plt.plot(train_sizes, train_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"r"</span>,</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="st">"Training score"</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    plt.plot(train_sizes, test_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"g"</span>,</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>             label<span class="op">=</span><span class="st">"Cross-validation score"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> plt</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a learning curve</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>plot_learning_curve(clf, <span class="st">'Naive Bayes Learning Curve'</span>, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="naive_bayes_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="model-results-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="model-results-interpretation">Model Results Interpretation</h3>
<section id="classification-report-output-analysis" class="level4">
<h4 class="anchored" data-anchor-id="classification-report-output-analysis"><strong>Classification Report Output Analysis</strong></h4>
<p>Let’s analyze the classification report output from above:</p>
<ul>
<li><p><strong>Accuracy</strong>: The accuracy of the model is approximately 84.77%, which indicates that the model correctly predicted the sentiment for about 84.77% of the reviews in the test dataset. This is a measure of overall correctness.</p></li>
<li><p><strong>Precision</strong>:</p>
<ul>
<li>For the ‘negative’ class, the precision is 82%. This means that when the model predicts a review as ‘negative,’ it is correct 82% of the time.</li>
<li>For the ‘positive’ class, the precision is 89%. This indicates that when the model predicts a review as ‘positive,’ it is correct 89% of the time.</li>
</ul></li>
<li><p><strong>Recall</strong>:</p>
<ul>
<li>For the ‘negative’ class, the recall is 91%. This means that the model correctly identifies 91% of the actual ‘negative’ reviews.</li>
<li>For the ‘positive’ class, the recall is 79%. The model captures 79% of the actual ‘positive’ reviews.</li>
</ul></li>
<li><p><strong>F1-Score</strong>:</p>
<ul>
<li>The F1-score is a balance between precision and recall. For the ‘negative’ class, it is 0.86, indicating a good balance between precision and recall.</li>
<li>For the ‘positive’ class, the F1-score is 0.83, also indicating a good balance.</li>
</ul></li>
<li><p><strong>Support</strong>:</p>
<ul>
<li>The ‘support’ column shows the number of instances for each class in the test dataset. There are 6,321 instances of ‘negative’ reviews and 5,998 instances of ‘positive’ reviews.</li>
</ul></li>
<li><p><strong>Macro Avg</strong>:</p>
<ul>
<li>The macro average F1-score is 0.85, which is the average of the F1-scores for both classes. It provides an overall evaluation of the model’s performance, giving equal weight to both classes.</li>
</ul></li>
<li><p><strong>Weighted Avg</strong>:</p>
<ul>
<li>The weighted average F1-score is also 0.85. It is similar to the macro average but takes into account the class imbalance in the dataset. It is weighted based on the number of instances in each class.</li>
</ul></li>
</ul>
<p><strong>Analysis</strong>:</p>
<ul>
<li>The accuracy of 84.77% suggests that the model is performing reasonably well in classifying sentiment.</li>
<li>Precision and recall values indicate that the model is better at predicting ‘positive’ sentiment than ‘negative’ sentiment.</li>
<li>The F1-scores are relatively balanced for both classes, which is a good sign.</li>
<li>The macro and weighted average F1-scores are also around 0.85, indicating consistent performance across classes and accounting for class imbalances.</li>
</ul>
<p>In summary, the model appears to perform well in predicting sentiments, but there might be room for improvement in correctly identifying ‘negative’ sentiments. The F1-scores suggest a reasonable balance between precision and recall. The performance metrics are well-balanced and provide a comprehensive view of the model’s effectiveness in classifying sentiment in the dataset.</p>
</section>
<section id="confusion-matrix-analysis" class="level4">
<h4 class="anchored" data-anchor-id="confusion-matrix-analysis"><strong>Confusion Matrix Analysis</strong></h4>
<p>Let’s analyze the confusion matrix from above:</p>
<p>The confusion matrix for the binary classification task is as follows:</p>
<ul>
<li>True Positives (TP): 5725</li>
<li>False Negatives (FN): 596</li>
<li>False Positives (FP): 1280</li>
<li>True Negatives (TN): 4718</li>
</ul>
<p>And the percentages, relative to the actual instances, are:</p>
<ul>
<li>True Positive Rate (Recall for ‘Positive’ class): 90.61%
<ul>
<li>This indicates that out of all the actual positive cases, the model correctly predicted approximately 90.61% of them as positive.</li>
</ul></li>
<li>False Negative Rate: 9.39%
<ul>
<li>This represents the cases where the model incorrectly predicted negative when the actual class was positive.</li>
</ul></li>
<li>False Positive Rate: 21.34%
<ul>
<li>This shows the cases where the model incorrectly predicted positive when the actual class was negative.</li>
</ul></li>
<li>True Negative Rate (Specificity for ‘Negative’ class): 78.66%
<ul>
<li>This indicates that out of all the actual negative cases, the model correctly predicted approximately 78.66% of them as negative.</li>
</ul></li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li>The model’s ability to correctly predict positive cases (True Positives) is quite high at 90.61%, indicating that it’s effective at identifying positive reviews.</li>
<li>There is a relatively low rate of False Negatives (9.39%), meaning that the model misses only a small portion of positive reviews.</li>
<li>The model’s ability to correctly predict negative cases (True Negatives) is also good at 78.66%, showing it’s effective at identifying negative reviews.</li>
<li>The False Positive Rate is 21.34%, indicating that there is a notable number of false alarms where the model predicts a review as positive when it’s actually negative.</li>
</ul>
<p>Overall, the model demonstrates a reasonably good performance, especially in terms of correctly identifying positive and negative cases. However, there is room for improvement in reducing false positive predictions. It’s important to consider the specific goals of the classification task when interpreting these results.</p>
</section>
<section id="sentiment-distribution-plot-analysis" class="level4">
<h4 class="anchored" data-anchor-id="sentiment-distribution-plot-analysis"><strong>Sentiment Distribution Plot Analysis</strong></h4>
<p>The provided chart above displays the distribution of sentiments in the dataset, and it’s evident that the data is relatively well balanced between negative and positive sentiments.</p>
<p>Here’s an analysis of the chart:</p>
<ul>
<li><p><strong>Total Data Points</strong>: The chart represents a total of 61,594 data points, which is the sum of negative (31,657) and positive (29,937) sentiments.</p></li>
<li><p><strong>Balanced Distribution</strong>: The dataset is quite balanced between negative and positive sentiments, with approximately 51.40% of the reviews classified as negative and 48.6% as positive. This balance is important for training machine learning models as it prevents biases that could arise from an imbalanced dataset.</p></li>
<li><p><strong>Visual Representation</strong>: The chart provides a clear visual representation of the sentiment distribution, making it easy to understand the relative proportion of positive and negative reviews.</p></li>
</ul>
<p>In summary, this analysis highlights that the dataset is well balanced between positive and negative sentiments, which is favorable for training and evaluating machine learning models for sentiment analysis.</p>
</section>
<section id="learning-curve-analysis" class="level4">
<h4 class="anchored" data-anchor-id="learning-curve-analysis"><strong>Learning Curve Analysis</strong></h4>
<p>Let’s analyze the learning curve from above:</p>
<ul>
<li><p><strong>Cross-Validation Accuracy Line</strong>: The accuracy of the model on the cross-validation data is represented by the green line. It starts at approximately 0.83 when the number of training examples is minimal and gradually increases as the training set size grows. This indicates that the model benefits from more data, as it’s able to generalize better with larger training sets. At around 40,000 training examples, the cross-validation accuracy reaches approximately 0.85.</p></li>
<li><p><strong>Training Accuracy Line</strong>: The red line represents the accuracy on the training data. It starts high, around 0.88, but as the number of training examples increases, it slightly decreases to around 0.87. This decrease in training accuracy as more examples are added indicates that the model might be suffering from overfitting. Overfitting occurs when a model becomes too specialized on the training data and doesn’t generalize well to unseen data.</p></li>
</ul>
<p>Here’s the key takeaway:</p>
<ul>
<li><p>Increasing the size of the training dataset benefits the model’s performance on unseen data (cross-validation accuracy). This suggests that the model could benefit from even more training data if available.</p></li>
<li><p>The small decline in training accuracy as more data is added may indicate some degree of overfitting. Regularization techniques or model complexity reduction may help address this issue.</p></li>
</ul>
<p>In summary, the learning curve provides valuable insights into how the model’s performance changes with more training examples, highlighting the trade-off between model bias and variance. The model seems to be improving with more data but might require additional adjustments to mitigate overfitting.</p>
</section>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>In summary, the model demonstrates a reasonable level of accuracy and balanced performance across different metrics. While it excels at identifying ‘positive’ sentiment, there is room for improvement in correctly identifying ‘negative’ sentiment. The sentiment distribution plot confirms a balanced dataset, and the learning curve highlights the potential benefits of additional training data while addressing overfitting concerns. Overall, the model shows promise in sentiment classification, but ongoing refinement may further enhance its performance and generalization capabilities.</p>
</section>
</section>
<section id="edm-subgenres" class="level2">
<h2 class="anchored" data-anchor-id="edm-subgenres">EDM Subgenres</h2>
<section id="feature-selection-for-record-data" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-for-record-data">Feature Selection for Record Data</h3>
<p><strong>Objective:</strong> The primary objective of the Feature Selection component in this project is to identify and choose the most relevant and informative features (variables or attributes) from the dataset, for the given task. Effective feature selection can improve the model’s performance, reduce overfitting, and enhance the interpretability of the results.</p>
<p>Using the EDM subgenres dataset, the columns ‘danceability’, ‘energy’, ‘loudness’, ‘speechiness’, ‘instrumentalness’, ‘liveness’, ‘valence’, and ‘tempo’ are the input features and ‘genre’ is my target feature. I plan to use a Gaussian Naive Bayes classification model to classify the inputted features as one of the 5 edm subgenres.</p>
</section>
<section id="naive-bayes-classification-model-1" class="level3">
<h3 class="anchored" data-anchor-id="naive-bayes-classification-model-1">Naive Bayes Classification Model</h3>
<p>In Python, after importing my dataset, I completed the following steps:</p>
<ul>
<li>Train-Test Split:
<ul>
<li>I divided the dataset into two parts: a training set, which constitutes 80% of the data, and a test set, comprising the remaining 20%. This division allows us to train the model on one part and evaluate its performance on the other.</li>
</ul></li>
<li>Feature Selection:
<ul>
<li>For this EDM subgenre prediction model, I selected specific audio features as input, including ‘danceability,’ ‘energy,’ ‘loudness_linear,’ ‘speechiness,’ ‘acousticness,’ ‘instrumentalness,’ ‘liveness,’ ‘valence,’ and ‘tempo.’ These features will be used to make predictions about the subgenres of EDM music.</li>
</ul></li>
<li>Training a Gaussian Naive Bayes Classifier:
<ul>
<li>I employed a Gaussian Naive Bayes classifier, which is well-suited for handling continuous, numeric features. This model assumes that the features are normally distributed (Gaussian) and learns to predict EDM subgenres based on the provided input features.</li>
</ul></li>
<li>Making Predictions:
<ul>
<li>Using the trained Gaussian Naive Bayes model, I made predictions on the test data. These predictions assign specific EDM subgenre labels to the test data based on the model’s learned patterns.</li>
</ul></li>
<li>Model Evaluation:
<ul>
<li>To assess the model’s performance, I compared the predicted subgenre labels with the true subgenre labels present in the test data. This evaluation step allows us to determine how well the model can classify EDM subgenres.</li>
</ul></li>
<li>Calculating Accuracy and Generating a Classification Report:
<ul>
<li>I calculated the model’s accuracy, which provides an overall measure of its correctness in predicting EDM subgenres. Additionally, I generated a classification report, which includes detailed metrics such as precision, recall, and F1-score for each subgenre class. The report provides valuable insights into the model’s performance, including its ability to distinguish between different EDM subgenres.</li>
</ul></li>
</ul>
<p>Below you can see the code used for the model as well as the summary report output and visualizations.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose your input features</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'danceability'</span>, <span class="st">'energy'</span>, <span class="st">'loudness_linear'</span>, <span class="st">'speechiness'</span>, <span class="st">'instrumentalness'</span>, <span class="st">'liveness'</span>, <span class="st">'valence'</span>, <span class="st">'tempo'</span>]</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into training and testing sets</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[features]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'genre_encoded'</span>]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a Gaussian Naive Bayes classifier</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>gnb_classifier <span class="op">=</span> GaussianNB()</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>gnb_classifier.fit(X_train, y_train)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test data</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> gnb_classifier.predict(X_test)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the classifier</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> gnb_classifier.score(X_test, y_test)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the classification report</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> classification_report(y_test, y_pred, target_names<span class="op">=</span>label_encoder.classes_)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(report)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>               precision    recall  f1-score   support

drum and bass       0.70      0.56      0.62       779
      dubstep       0.62      0.59      0.61       733
   tech house       0.60      0.63      0.62       767
       techno       0.61      0.66      0.63       763
       trance       0.53      0.59      0.56       744

     accuracy                           0.61      3786
    macro avg       0.61      0.61      0.61      3786
 weighted avg       0.61      0.61      0.61      3786

Accuracy: 0.6075013206550449</code></pre>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test data</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> gnb_classifier.predict(X_test)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the confusion matrix</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>confusion <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate percentages for each cell</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>confusion_percentages <span class="op">=</span> confusion.astype(<span class="st">'float'</span>) <span class="op">/</span> confusion.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)[:, np.newaxis] <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Define class labels for the confusion matrix</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>class_labels <span class="op">=</span> label_encoder.classes_</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a heatmap of the confusion matrix with percentages</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>sns.heatmap(confusion_percentages, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'.2f'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, xticklabels<span class="op">=</span>class_labels, yticklabels<span class="op">=</span>class_labels)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Actual'</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix (with Percentages)'</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="naive_bayes_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the training sizes you want to include in the learning curve</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(GaussianNB(), X, y, train_sizes<span class="op">=</span>np.linspace(<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="dv">5</span>), cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean and standard deviation of training and test scores</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>train_scores_mean <span class="op">=</span> np.mean(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>train_scores_std <span class="op">=</span> np.std(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>test_scores_mean <span class="op">=</span> np.mean(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>test_scores_std <span class="op">=</span> np.std(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the learning curve plot</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, train_scores_mean, label<span class="op">=</span><span class="st">'Training Accuracy'</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.fill_between(train_sizes, train_scores_mean <span class="op">-</span> train_scores_std, train_scores_mean <span class="op">+</span> train_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, test_scores_mean, label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>plt.fill_between(train_sizes, test_scores_mean <span class="op">-</span> test_scores_std, test_scores_mean <span class="op">+</span> test_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Training Examples'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Learning Curve for Gaussian Naive Bayes'</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="naive_bayes_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> permutation_importance</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the Gaussian Naive Bayes classifier</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>gnb_classifier.fit(X_train, y_train)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate permutation importances</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>perm_importance <span class="op">=</span> permutation_importance(gnb_classifier, X_test, y_test, n_repeats<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature importances</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> perm_importance.importances_mean</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a bar chart to visualize feature importances</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>plt.barh(features, importances, color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Permutation Feature Importance'</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Importance'</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature'</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="naive_bayes_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="model-results-interpretation-1" class="level3">
<h3 class="anchored" data-anchor-id="model-results-interpretation-1">Model Results Interpretation</h3>
<section id="classification-report-output-analysis-1" class="level4">
<h4 class="anchored" data-anchor-id="classification-report-output-analysis-1"><strong>Classification Report Output Analysis</strong></h4>
<p>Let’s analyze the classification report output from above:</p>
<ul>
<li><p><strong>Classes</strong>: The report is based on a classification task with five classes: “drum and bass,” “dubstep,” “tech house,” “techno,” and “trance.”</p></li>
<li><p><strong>Precision</strong>: Precision measures how many of the predicted positive instances were actually correct. Here’s the precision analysis for each class:</p>
<ul>
<li>For “drum and bass,” precision is 0.70, meaning that 70% of the instances predicted as “drum and bass” were correct.</li>
<li>For “dubstep,” precision is 0.62, indicating that 62% of the instances predicted as “dubstep” were correct.</li>
<li>For “tech house,” precision is 0.60, indicating that 60% of the instances predicted as “tech house” were correct.</li>
<li>For “techno,” precision is 0.61, meaning that 61% of the instances predicted as “techno” were correct.</li>
<li>For “trance,” precision is 0.53, indicating that 53% of the instances predicted as “trance” were correct.</li>
</ul></li>
<li><p><strong>Recall</strong>: Recall measures how many of the actual positive instances were correctly predicted. Here’s the recall analysis for each class:</p>
<ul>
<li>For “drum and bass,” recall is 0.56, meaning that 56% of the actual “drum and bass” instances were correctly predicted.</li>
<li>For “dubstep,” recall is 0.59, indicating that 59% of the actual “dubstep” instances were correctly predicted.</li>
<li>For “tech house,” recall is 0.63, indicating that 63% of the actual “tech house” instances were correctly predicted.</li>
<li>For “techno,” recall is 0.66, meaning that 66% of the actual “techno” instances were correctly predicted.</li>
<li>For “trance,” recall is 0.59, indicating that 59% of the actual “trance” instances were correctly predicted.</li>
</ul></li>
<li><p><strong>F1-Score</strong>: The F1-score is the harmonic mean of precision and recall. It provides a balance between the two metrics. Here’s the F1-score analysis for each class:</p>
<ul>
<li>For “drum and bass,” the F1-score is 0.62, indicating a balanced performance between precision and recall.</li>
<li>For “dubstep,” the F1-score is 0.61, also showing a balance between precision and recall.</li>
<li>For “tech house,” the F1-score is 0.62, indicating a balanced performance.</li>
<li>For “techno,” the F1-score is 0.63, showing a good balance between precision and recall.</li>
<li>For “trance,” the F1-score is 0.56, indicating a slightly lower balance.</li>
</ul></li>
<li><p><strong>Support</strong>: Support represents the number of instances in each class.</p></li>
<li><p><strong>Accuracy</strong>: The overall accuracy of the model is 0.6075, indicating that the model correctly classifies 60.75% of all instances.</p></li>
<li><p><strong>Macro Average</strong>: The macro average provides the average precision, recall, and F1-score across all classes. In this case, the macro average is approximately 0.61.</p></li>
<li><p><strong>Weighted Average</strong>: The weighted average considers class imbalance and provides an average that gives more weight to classes with more instances. In this case, the weighted average is approximately 0.61.</p></li>
</ul>
<p>In summary, the classification report shows that the model has varying performance across different classes. While it achieves a reasonably balanced F1-score for most classes, there might be room for improvement in terms of precision and recall, particularly for the “trance” class. Additionally, the overall accuracy is around 60.75%, indicating that the model’s performance is decent, but there may be potential for improvement.</p>
</section>
<section id="confusion-matrix-analysis-1" class="level4">
<h4 class="anchored" data-anchor-id="confusion-matrix-analysis-1"><strong>Confusion Matrix Analysis</strong></h4>
<p>Let’s analyze the confusion matrix from above:</p>
<p>The confusion matrix with percentages provides insight into the model’s performance for each class. Each row represents the actual instances for a specific class, and each column represents the predicted class. The values in each row are the percentages of actual instances that were predicted to belong to each class.</p>
<ul>
<li><strong>Drum and Bass (Actuals)</strong>:
<ul>
<li>56.23% of actual “drum and bass” instances were correctly predicted as “drum and bass.”</li>
<li>23.62% of actual “drum and bass” instances were incorrectly predicted as “dubstep.”</li>
<li>8.22% were incorrectly predicted as “tech house.”</li>
<li>3.21% were incorrectly predicted as “techno.”</li>
<li>8.73% were incorrectly predicted as “trance.”</li>
</ul></li>
<li><strong>Dubstep (Actuals)</strong>:
<ul>
<li>59.35% of actual “dubstep” instances were correctly predicted as “dubstep.”</li>
<li>17.74% were incorrectly predicted as “drum and bass.”</li>
<li>6.55% were incorrectly predicted as “tech house.”</li>
<li>3.14% were incorrectly predicted as “techno.”</li>
<li>13.23% were incorrectly predicted as “trance.”</li>
</ul></li>
<li><strong>Tech House (Actuals)</strong>:
<ul>
<li>63.36% of actual “tech house” instances were correctly predicted as “tech house.”</li>
<li>2.22%were incorrectly predicted as “drum and bass.”</li>
<li>2.35% were incorrectly predicted as “dubstep.”</li>
<li>18.25% were incorrectly predicted as “techno.”</li>
<li>13.82% were incorrectly predicted as “trance.”</li>
</ul></li>
<li><strong>Techno (Actuals)</strong>:
<ul>
<li>66.19% of actual “techno” instances were correctly predicted as “techno.”</li>
<li>2.49% were incorrectly predicted as “drum and bass.”<br>
</li>
<li>1.18% were incorrectly predicted as “dubstep.”</li>
<li>15.73% were incorrectly predicted as “tech house.”</li>
<li>14.42% were incorrectly predicted as “trance.”</li>
</ul></li>
<li><strong>Trance (Actuals)</strong>:
<ul>
<li>58.6% of actual “trance” instances were correctly predicted as “trance.”</li>
<li>2.96% were incorrectly predicted as “drum and bass.”<br>
</li>
<li>7.80% were incorrectly predicted as “dubstep.”</li>
<li>12.1% were incorrectly predicted as “tech house.”</li>
<li>18.55% were incorrectly predicted as “techno.”</li>
</ul></li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li>The diagonal values represent the true positives, i.e., the percentages of correctly predicted instances for each class.</li>
<li>The off-diagonal values represent various types of misclassifications.</li>
<li>The model appears to have varying performance across different genres. While some genres like “dubstep” and “techno” are well-predicted, others like “drum and bass” and “trance” have room for improvement.</li>
<li>The model has the most significant challenges in distinguishing between “tech house” and “techno,” where misclassifications are relatively high. This makes sense because tech house combines stylistic features of techno with house.</li>
<li>The analysis suggests that fine-tuning the model, considering feature importance, or collecting more genre-specific data could lead to improved genre classification accuracy.</li>
</ul>
</section>
<section id="learning-curve-analysis-1" class="level4">
<h4 class="anchored" data-anchor-id="learning-curve-analysis-1"><strong>Learning Curve Analysis</strong></h4>
<p>Let’s analyze the above learning curve:</p>
<ul>
<li><strong>Training Accuracy (Blue Line)</strong>:
<ul>
<li>Initially, with a relatively small training dataset size of around 2,000 samples, the model achieves a perfect training accuracy of 1. This means the model can perfectly fit the smaller dataset, as it essentially memorizes the data.</li>
<li>As the training dataset size increases beyond 2,000 samples, the training accuracy starts to decrease gradually. This decrease in training accuracy suggests that the model might be finding it more challenging to fit the larger dataset.</li>
<li>The training accuracy eventually stabilizes at around 0.6 (60%) when the training dataset reaches approximately 14,000 samples.</li>
<li>The decrease in training accuracy indicates that the model is not overfitting the training data, and it starts to generalize better as the dataset size increases.</li>
</ul></li>
<li><strong>Validation Accuracy (Orange Line)</strong>:
<ul>
<li>Initially, with a small training dataset size of around 2,000 samples, the validation accuracy is notably lower, at around 0.2 (20%). This lower accuracy indicates that the model’s performance on unseen data is not as strong when trained on a small dataset.</li>
<li>As the training dataset size increases from 2,000 to around 14,000 samples, the validation accuracy steadily improves. It rises to approximately 0.6 (60%) at the larger dataset size.</li>
<li>The increasing validation accuracy suggests that the model’s ability to generalize to new, unseen data improves as more training data becomes available.</li>
</ul></li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li>The initial low training accuracy and validation accuracy with a small dataset size (around 2,000 samples) suggest that the model may struggle to generalize effectively from limited training data.</li>
<li>The convergence of the training and validation accuracy lines at around 14,000 training samples is a positive sign. It indicates that the model is neither underfitting nor overfitting but achieving a good balance between bias and variance.</li>
<li>The model appears to benefit from the increased dataset size, allowing it to generalize more effectively and improve its accuracy on unseen data.</li>
<li>The learning curve suggests that collecting more training data could lead to further improvements in the model’s performance.</li>
<li>Overall, the learning curve illustrates the importance of dataset size in achieving better generalization and model performance.</li>
</ul>
</section>
<section id="permutation-feature-importance-analysis" class="level4">
<h4 class="anchored" data-anchor-id="permutation-feature-importance-analysis"><strong>Permutation Feature Importance Analysis</strong></h4>
<section id="how-does-it-work" class="level5">
<h5 class="anchored" data-anchor-id="how-does-it-work">How does it work?</h5>
<p>“Permutation Feature Importance” is used to assess the importance of each feature in a machine learning model, in this case, a Gaussian Naive Bayes classifier.</p>
<ul>
<li><strong>Permutation Importance Calculation:</strong> The permutation_importance function is used to calculate the importance of each feature. It does this by taking the following steps for each feature:
<ol type="a">
<li>It temporarily shuffles (permutes) the values of a single feature in the test data (X_test). This means it disrupts the relationship between the feature and the target variable (genre in this case).</li>
<li>Then, it measures the impact of this permutation on the model’s performance. If the feature was important, shuffling its values should significantly decrease the model’s accuracy or other performance metric.</li>
<li>This process is repeated multiple times (in this case, 30 times) for each feature, and the average impact on model performance is calculated.</li>
</ol></li>
<li><strong>Feature Importance Values:</strong> After running these permutations, you get a measure of how much each feature contributes to the model’s predictive performance. Features that, when permuted, have a large negative impact on the model’s performance are considered more important.</li>
<li><strong>Bar Chart Visualization:</strong> Finally, the script creates a horizontal bar chart to visualize the feature importances. Each feature is shown on the y-axis, and its importance (the impact of permuting it) is shown on the x-axis. Features with higher importance will have higher values on the x-axis.</li>
</ul>
</section>
<section id="analysis-of-chart" class="level5">
<h5 class="anchored" data-anchor-id="analysis-of-chart"><strong>Analysis of Chart</strong></h5>
<p>Let’s analyze the bar chart from above:</p>
<ol type="1">
<li><p><strong>Tempo</strong>: Tempo stands out as the most important feature. When the tempo is permuted (randomly shuffled), it has a significant negative impact on the model’s performance. This suggests that the tempo plays a crucial role in the model’s ability to classify music genres accurately.</p></li>
<li><p><strong>Instrumentalness</strong>: Following tempo, instrumentalness is the second most important feature. This means that the proportion of instrumental music in a track significantly influences the model’s predictions. High instrumentalness might be a strong indicator for specific music genres.</p></li>
<li><p><strong>Danceability</strong>: Danceability is the third most important feature. It indicates that the level of danceability, or how suitable a track is for dancing, plays a notable role in classifying genres correctly.</p></li>
<li><p><strong>Loudness</strong>: Loudness is the fourth most important feature. It suggests that the volume or loudness of a track has a significant impact on the model’s predictions. It might be that certain genres tend to be louder than others.</p></li>
</ol>
<p>This analysis aligns with the idea that musical attributes related to tempo, instrumentalness, danceability, and loudness are strong indicators for genre classification. It provides valuable insights into the characteristics of music that contribute the most to the model’s accuracy.</p>
<p>These findings could be useful for several purposes, such as understanding the model’s decision-making process, potentially identifying which musical attributes are distinctive for each genre, and guiding future data collection or feature engineering efforts.</p>
</section>
</section>
</section>
<section id="conclusion-1" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-1">Conclusion</h3>
<p>In summary, the genre classification model shows potential, but it benefits from refinement, particularly for genres with higher misclassification rates. Additionally, the learning curve highlights the importance of dataset size for improving model generalization. Further data collection and model fine-tuning may contribute to more accurate genre predictions.</p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>