{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Classification Decision Tree\"\n",
    "format: html\n",
    "execute:\n",
    "    echo: true\n",
    "code-fold: true\n",
    "link-external-newwindow: true\n",
    "bibliography: reference.bib\n",
    "website:\n",
    "  back-to-top-navigation: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "**Decision Tree:**\n",
    "\n",
    "Decision trees are a powerful tool in machine learning that mimics the way humans make decisions. Imagine you have a complex problem, and you're trying to solve it by breaking it down into a series of simpler decisions. A decision tree does just that. It starts with a big question at the top (the root) and then branches out into a series of smaller questions based on the answers at each step. These questions are based on the features of your data, and the goal is to ultimately reach a decision or prediction at the leaves of the tree. Each decision is made by evaluating the importance of different features in the data, helping the model to learn patterns and make accurate predictions.\n",
    "\n",
    "**Random Forest:**\n",
    "\n",
    "Random Forest takes the idea of decision trees a step further by creating a \"forest\" or a collection of decision trees. Each tree is trained on a different subset of the data and makes its own set of predictions. Instead of relying on just one tree, Random Forest combines the predictions from all the trees to make a more robust and accurate final prediction. The \"random\" part comes from the fact that each tree is trained on a random subset of features, adding an element of diversity. This helps prevent overfitting, where the model becomes too tailored to the training data and performs poorly on new, unseen data. By leveraging the collective intelligence of multiple trees, Random Forest provides a more reliable and generalizable solution.\n",
    "\n",
    "**XGBoost:**\n",
    "\n",
    "XGBoost, or Extreme Gradient Boosting, is a sophisticated algorithm that excels in predictive modeling. At its core, XGBoost is an ensemble method like Random Forest, but it uses a different strategy. Instead of training all the trees independently, XGBoost builds them sequentially, learning from the mistakes of the previous ones. It's like a team of experts playing a game, with each expert focusing on the aspects where others fell short. XGBoost is particularly good at handling complex relationships in the data, capturing patterns that might be difficult for other algorithms. It also incorporates a regularization term to control the complexity of the model, preventing it from becoming too intricate and overfitting the training data. Overall, XGBoost is a powerful and flexible tool for making accurate predictions in a wide range of applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "In the below code, I take the EDM subgenre dataset and look at the class distribution of the target labels. As shown in the [data exploration](data_exploration.ipynb) tab of the website, I had intentionally balanced the classes in order to have an evenly distributed dataset; each genre has 3786 tracks. Having evenly distributed class labels in a dataset, often referred to as class balance or class distribution, can offer several advantages in the context of machine learning:\n",
    "\n",
    "1. **Improved Model Performance:**\n",
    "   - **Preventing Bias:** Models trained on imbalanced datasets may develop a bias towards the majority class, leading to poorer performance on minority classes. A balanced distribution helps ensure that the model learns to recognize patterns from all classes equally, leading to a more accurate and fair predictive model.\n",
    "\n",
    "2. **Better Generalization:**\n",
    "   - **Enhanced Generalization:** Models trained on imbalanced data might perform well on the training set but struggle to generalize to new, unseen data. A balanced dataset helps in creating models that are more likely to generalize well, as they are exposed to a representative distribution of examples from each class during training.\n",
    "\n",
    "3. **More Robust Evaluation:**\n",
    "   - **Accurate Evaluation Metrics:** Imbalanced datasets can mislead the evaluation of a model. For example, accuracy may seem high, but it might be due to the model predicting the majority class most of the time. Balanced datasets provide a more reliable evaluation, allowing for the use of metrics like precision, recall, and F1 score that consider the performance across all classes.\n",
    "\n",
    "4. **Stable Training:**\n",
    "   - **Stable Convergence:** Training models on imbalanced data can lead to instability in the learning process. An evenly distributed dataset often contributes to a more stable convergence during training, preventing the model from getting stuck in local minima and improving convergence speed.\n",
    "\n",
    "5. **Reduced Sensitivity to Data Changes:**\n",
    "   - **Robustness to Data Drift:** In real-world applications, the distribution of data may change over time. Models trained on balanced datasets tend to be more robust to such changes, as they have learned patterns from all classes rather than being overly specialized in the majority class.\n",
    "\n",
    "6. **Enhanced Feature Importance:**\n",
    "   - **Accurate Feature Importance:** In imbalanced datasets, models may assign exaggerated importance to features correlated with the majority class. Balanced datasets allow models to assign importance more accurately, reflecting the actual contribution of features to the prediction of each class.\n",
    "\n",
    "While having balanced class labels is often beneficial, it's essential to note that real-world data is not always perfectly balanced. In cases of significant class imbalance, techniques such as oversampling the minority class, undersampling the majority class, or using specialized algorithms designed for imbalanced data may be employed to address these challenges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "trance           3786\n",
       "tech house       3786\n",
       "techno           3786\n",
       "drum and bass    3786\n",
       "dubstep          3786\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.genre.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model for Comparison\n",
    "\n",
    "The baseline comparison using a random classifier serves as a critical benchmark in evaluating the performance of more sophisticated machine learning models. The purpose of the random classifier is to establish a baseline level of performance that any meaningful model should surpass. This baseline is particularly important when dealing with classification tasks, as it helps assess whether the chosen model is learning patterns from the data or is merely performing at a chance level. By comparing accuracy, precision, recall, and F1-score metrics from the random classifier against those of a trained model, one can gauge the model's effectiveness in capturing relevant patterns and making informed predictions. This baseline comparison is instrumental in validating the significance of the machine learning model's contributions to the task at hand, guiding researchers and practitioners in the development and selection of effective classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MULTI-CLASS:  BALANCED LOAD\n",
      "-----RANDOM CLASSIFIER-----\n",
      "count of prediction: dict_values([2062, 2012, 1931, 2044, 1951])\n",
      "probability of prediction: [0.2062 0.2012 0.1931 0.2044 0.1951]\n",
      "accuracy: 0.2017\n",
      "precision: 0.20162185181207337\n",
      "recall: 0.20156229528691672\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def generate_balanced_data(num_labels, N=10000):\n",
    "    # Generates N random labels with balanced load across all labels.\n",
    "    weights = [1/num_labels] * num_labels\n",
    "    labels = list(range(num_labels))\n",
    "    y = random.choices(labels, weights=weights, k=N)\n",
    "    return y\n",
    "\n",
    "# TEST\n",
    "num_labels = 5\n",
    "y = generate_balanced_data(num_labels, 10000)\n",
    "\n",
    "def random_classifier(y_data):\n",
    "    # Implements a random classifier that predicts labels randomly.\n",
    "    max_label = np.max(y_data)\n",
    "    \n",
    "    # Generate random predictions within the range of labels.\n",
    "    y_pred = [int(np.floor((max_label + 1) * np.random.uniform(0, 1))) for _ in range(len(y_data))]\n",
    "    \n",
    "    # Print information about the random classifier's predictions.\n",
    "    print(\"-----RANDOM CLASSIFIER-----\")\n",
    "    print(\"count of prediction:\", Counter(y_pred).values())\n",
    "    print(\"probability of prediction:\", np.fromiter(Counter(y_pred).values(), dtype=float) / len(y_data))\n",
    "    print(\"accuracy:\", accuracy_score(y_data, y_pred))\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_data, y_pred, average='macro')\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "\n",
    "# Testing the random classifier with balanced labels\n",
    "print(\"\\nMULTI-CLASS:  BALANCED LOAD\")\n",
    "y = generate_balanced_data(num_labels, 10000)\n",
    "random_classifier(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results\n",
    "\n",
    "After modeling the decision tree, the below are the results:\n",
    "* Accuracy: 69.18%\n",
    "    * The accuracy is the ratio of correctly predicted instances to the total instances. In this case, approximately 69.18% of the test set instances were predicted correctly by the Decision Tree Classifier.\n",
    "* Precision: 69.19%\n",
    "    * Precision is a measure of the accuracy of the positive predictions. In this context, it indicates that around 69.19% of the instances predicted as positive by the model were actually positive.\n",
    "* Recall: 69.17%\n",
    "    * Recall (or sensitivity) measures the ability of the model to capture all the relevant instances. A recall of 69.17% means that the model identified around 69.17% of the actual positive instances in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----DECISION TREE CLASSIFIER-----\n",
      "Accuracy: 0.6917591125198098\n",
      "Precision: 0.6919464788765407\n",
      "Recall: 0.691736509738276\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'genre' is the target column\n",
    "target_column = 'genre'\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.select_dtypes(include='number')\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"-----DECISION TREE CLASSIFIER-----\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"717pt\" height=\"433pt\"\n",
       " viewBox=\"0.00 0.00 717.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-429 713,-429 713,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M601,-425C601,-425 373,-425 373,-425 367,-425 361,-419 361,-413 361,-413 361,-354 361,-354 361,-348 367,-342 373,-342 373,-342 601,-342 601,-342 607,-342 613,-348 613,-354 613,-354 613,-413 613,-413 613,-419 607,-425 601,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"435.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">tempo ≤ 166.054</text>\n",
       "<text text-anchor=\"start\" x=\"459\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.8</text>\n",
       "<text text-anchor=\"start\" x=\"434.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15144</text>\n",
       "<text text-anchor=\"start\" x=\"369\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [3007, 3053, 3019, 3023, 3042]</text>\n",
       "<text text-anchor=\"start\" x=\"439.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = dubstep</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M479,-306C479,-306 251,-306 251,-306 245,-306 239,-300 239,-294 239,-294 239,-235 239,-235 239,-229 245,-223 251,-223 251,-223 479,-223 479,-223 485,-223 491,-229 491,-235 491,-235 491,-294 491,-294 491,-300 485,-306 479,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"313.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">tempo ≤ 139.443</text>\n",
       "<text text-anchor=\"start\" x=\"329.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.788</text>\n",
       "<text text-anchor=\"start\" x=\"312.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13174</text>\n",
       "<text text-anchor=\"start\" x=\"247\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1331, 2827, 2991, 3005, 3020]</text>\n",
       "<text text-anchor=\"start\" x=\"322.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = trance</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M444.67,-341.91C435.02,-332.65 424.68,-322.73 414.75,-313.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"416.89,-310.42 407.25,-306.02 412.05,-315.47 416.89,-310.42\"/>\n",
       "<text text-anchor=\"middle\" x=\"407.78\" y=\"-327.32\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#e9965a\" stroke=\"black\" d=\"M697,-298.5C697,-298.5 521,-298.5 521,-298.5 515,-298.5 509,-292.5 509,-286.5 509,-286.5 509,-242.5 509,-242.5 509,-236.5 515,-230.5 521,-230.5 521,-230.5 697,-230.5 697,-230.5 703,-230.5 709,-236.5 709,-242.5 709,-242.5 709,-286.5 709,-286.5 709,-292.5 703,-298.5 697,-298.5\"/>\n",
       "<text text-anchor=\"start\" x=\"573.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.263</text>\n",
       "<text text-anchor=\"start\" x=\"560.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1970</text>\n",
       "<text text-anchor=\"start\" x=\"517\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1676, 226, 28, 18, 22]</text>\n",
       "<text text-anchor=\"start\" x=\"540.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = drum and bass</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>0&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M529.33,-341.91C541.52,-330.21 554.82,-317.46 567,-305.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"569.62,-308.11 574.42,-298.67 564.78,-303.06 569.62,-308.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"573.89\" y=\"-319.96\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#fcfffe\" stroke=\"black\" d=\"M353,-187C353,-187 125,-187 125,-187 119,-187 113,-181 113,-175 113,-175 113,-116 113,-116 113,-110 119,-104 125,-104 125,-104 353,-104 353,-104 359,-104 365,-110 365,-116 365,-116 365,-175 365,-175 365,-181 359,-187 353,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"187.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">tempo ≤ 127.502</text>\n",
       "<text text-anchor=\"start\" x=\"207\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.77</text>\n",
       "<text text-anchor=\"start\" x=\"186.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10907</text>\n",
       "<text text-anchor=\"start\" x=\"121\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1072, 1221, 2949, 2836, 2829]</text>\n",
       "<text text-anchor=\"start\" x=\"182.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = tech house</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.28,-222.91C311.22,-213.56 300.43,-203.54 290.08,-193.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"292.35,-191.26 282.64,-187.02 287.58,-196.39 292.35,-191.26\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#a6ee7a\" stroke=\"black\" d=\"M586.5,-179.5C586.5,-179.5 395.5,-179.5 395.5,-179.5 389.5,-179.5 383.5,-173.5 383.5,-167.5 383.5,-167.5 383.5,-123.5 383.5,-123.5 383.5,-117.5 389.5,-111.5 395.5,-111.5 395.5,-111.5 586.5,-111.5 586.5,-111.5 592.5,-111.5 598.5,-117.5 598.5,-123.5 598.5,-123.5 598.5,-167.5 598.5,-167.5 598.5,-173.5 592.5,-179.5 586.5,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"455.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.472</text>\n",
       "<text text-anchor=\"start\" x=\"442.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2267</text>\n",
       "<text text-anchor=\"start\" x=\"391.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [259, 1606, 42, 169, 191]</text>\n",
       "<text text-anchor=\"start\" x=\"443.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = dubstep</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M408.72,-222.91C421.31,-211.21 435.05,-198.46 447.62,-186.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"450.34,-189.04 455.28,-179.67 445.57,-183.91 450.34,-189.04\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#d2f9f2\" stroke=\"black\" d=\"M218,-68C218,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 218,0 218,0 224,0 230,-6 230,-12 230,-12 230,-56 230,-56 230,-62 224,-68 218,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"79.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.735</text>\n",
       "<text text-anchor=\"start\" x=\"66.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6121</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [763, 873, 2498, 1432, 555]</text>\n",
       "<text text-anchor=\"start\" x=\"58.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = tech house</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.83,-103.73C182.19,-94.33 170.88,-84.35 160.28,-74.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.52,-72.3 152.71,-68.3 157.89,-77.54 162.52,-72.3\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#f8ccef\" stroke=\"black\" d=\"M466,-68C466,-68 260,-68 260,-68 254,-68 248,-62 248,-56 248,-56 248,-12 248,-12 248,-6 254,0 260,0 260,0 466,0 466,0 472,0 478,-6 478,-12 478,-12 478,-56 478,-56 478,-62 472,-68 466,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.67</text>\n",
       "<text text-anchor=\"start\" x=\"314.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4786</text>\n",
       "<text text-anchor=\"start\" x=\"256\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [309, 348, 451, 1404, 2274]</text>\n",
       "<text text-anchor=\"start\" x=\"320.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = trance</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M285.17,-103.73C295.81,-94.33 307.12,-84.35 317.72,-74.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"320.11,-77.54 325.29,-68.3 315.48,-72.3 320.11,-77.54\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7fddf10a1640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import graphviz\n",
    "def visualize_tree(dtc_object):\n",
    "  dot_data = sklearn.tree.export_graphviz(dtc_object, out_file=None,\n",
    "                      feature_names=X_train.columns,\n",
    "                      class_names=list(genre_map.keys()),\n",
    "                      filled=True, rounded=True,\n",
    "                      special_characters=True)\n",
    "  graph = graphviz.Source(dot_data)\n",
    "  graph.render(outfile=\"test.svg\")\n",
    "  return graph\n",
    "visualize_tree(dtc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results\n",
    "After performing hyper-parameter tuning on our random forest algorithm, the results very slightly improved:\n",
    "* Accuracy: 77.87%\n",
    "    * The accuracy is the ratio of correctly predicted instances to the total instances. In this case, approximately 77.87% of the test set instances were predicted correctly by the Decision Tree Classifier.\n",
    "* Precision: 77.87%\n",
    "    * Precision is a measure of the accuracy of the positive predictions. In this context, it indicates that around 77.87% of the instances predicted as positive by the model were actually positive.\n",
    "* Recall: 77.82%\n",
    "    * Recall (or sensitivity) measures the ability of the model to capture all the relevant instances. A recall of 77.82% means that the model identified around 77.82% of the actual positive instances in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----RANDOM FOREST CLASSIFIER-----\n",
      "Accuracy: 0.7778658214474379\n",
      "Precision: 0.7787186900592407\n",
      "Recall: 0.7781547761093727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "target_column = 'genre'\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.select_dtypes(include='number')\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"-----RANDOM FOREST CLASSIFIER-----\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Accuracy: 0.7786582144743793\n",
      "Precision: 0.7794206959651867\n",
      "Recall: 0.7788913235366173\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "target_column = 'genre'\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.select_dtypes(include='number')\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters and their possible values for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use Grid Search with cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train a Random Forest Classifier with the best hyperparameters\n",
    "best_rf_classifier = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results\n",
    "After performing hyper-parameter tuning on our XGBoost algorithm, the results very slightly improved:\n",
    "* Accuracy: 78.79%\n",
    "    * The accuracy is the ratio of correctly predicted instances to the total instances. In this case, approximately 78.79% of the test set instances were predicted correctly by the Decision Tree Classifier.\n",
    "* Precision: 78.96%\n",
    "    * Precision is a measure of the accuracy of the positive predictions. In this context, it indicates that around 78.96% of the instances predicted as positive by the model were actually positive.\n",
    "* Recall: 78.82%\n",
    "    * Recall (or sensitivity) measures the ability of the model to capture all the relevant instances. A recall of 78.82% means that the model identified around 78.82% of the actual positive instances in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----XGBOOST CLASSIFIER-----\n",
      "Accuracy: 0.7823560486001057\n",
      "Precision: 0.7838098236528813\n",
      "Recall: 0.7824599382154643\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "target_column = 'genre'\n",
    "\n",
    "# Create a label encoder and fit it to the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_genre'] = label_encoder.fit_transform(df[target_column])\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "# Filter for only numeric columns in the feature matrix\n",
    "X = df.select_dtypes(include=['number']).drop('encoded_genre', axis=1)\n",
    "y = df['encoded_genre']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Decode the predicted labels back to the original class labels\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"-----XGBOOST CLASSIFIER-----\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----XGBOOST CLASSIFIER-----\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "Accuracy: 0.7879027997886952\n",
      "Precision: 0.7895878189560148\n",
      "Recall: 0.7880435869170253\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming 'genre' is the target column\n",
    "target_column = 'genre'\n",
    "\n",
    "# Create a label encoder and fit it to the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_genre'] = label_encoder.fit_transform(df[target_column])\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.select_dtypes(include=['number']).drop('encoded_genre', axis=1)\n",
    "y = df['encoded_genre']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Define a parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(xgb_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best parameters to train the final model\n",
    "final_model = xgb.XGBClassifier(**best_params, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Decode the predicted labels back to the original class labels\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"-----XGBOOST CLASSIFIER-----\")\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusions & Summary\n",
    "\n",
    "#### Decision Tree Classifier:\n",
    "- **Accuracy:** 69.18%\n",
    "- **Precision:** 69.19%\n",
    "- **Recall:** 69.17%\n",
    "\n",
    "**Conclusion:**\n",
    "The Decision Tree Classifier, while providing a baseline, might face challenges in capturing the intricate relationships inherent in EDM subgenre classification. It may not be sufficient for discerning the nuanced patterns associated with different EDM subgenres.\n",
    "\n",
    "#### Random Forest Classifier:\n",
    "  - **Accuracy:** 77.87%\n",
    "  - **Precision:** 77.94%\n",
    "  - **Recall:** 77.89%\n",
    "\n",
    "**Conclusion:**\n",
    "The Random Forest Classifier, with or without tuning, offers an improvement over the Decision Tree, making it more suitable for capturing the diverse patterns and characteristics associated with different EDM subgenres. It can handle multiple features simultaneously and provides a solid foundation for EDM subgenre classification.\n",
    "\n",
    "#### XGBoost Classifier:\n",
    "- **Accuracy:** 78.79%\n",
    "- **Precision:** 78.96%\n",
    "- **Recall:** 78.80%\n",
    "\n",
    "**Conclusion:**\n",
    "XGBoost, known for its superior performance in various contexts, including structured data, demonstrates competitive accuracy in predicting EDM subgenres. Its ability to handle complex relationships and adapt to sequential learning makes it a strong candidate for EDM subgenre classification tasks.\n",
    "\n",
    "#### Overall Recommendation:\n",
    "- **Decision Tree:** While interpretable, it may struggle to capture the nuances of EDM subgenres. Consider it as a starting point or for simple cases.\n",
    "\n",
    "- **Random Forest:** A robust choice, especially after hyperparameter tuning, providing a good balance between interpretability and predictive power for EDM subgenre classification.\n",
    "\n",
    "- **XGBoost:** Offers high accuracy and is well-suited for complex patterns in EDM subgenres. Consider it for tasks where precision and recall are crucial.\n",
    "\n",
    "In our exploration of predicting EDM subgenres using machine learning models like Decision Trees, Random Forests, and XGBoost, we uncovered some interesting insights. Imagine these models as detectives trying to understand the distinct characteristics that define different types of electronic dance music. What we found was that the Random Forest and XGBoost detectives were particularly skilled at deciphering the intricate patterns that distinguish one EDM subgenre from another, outperforming the simpler Decision Tree detective.\n",
    "\n",
    "This discovery is important because it opens up new possibilities for music enthusiasts and industry professionals alike. By leveraging these advanced models, we can more accurately categorize and identify EDM subgenres, offering a deeper understanding of the unique elements that make each genre special. Imagine having a virtual music expert that can not only recognize your favorite subgenres but also introduce you to new and exciting ones based on subtle musical features.\n",
    "\n",
    "However, our journey wasn't without its challenges. The Decision Tree detective, while straightforward, struggled to grasp the complexity of EDM subgenres. Yet, with the Random Forest and XGBoost detectives, armed with their ability to collaborate and adapt, we were able to overcome these limitations and create a more nuanced picture of the EDM landscape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
